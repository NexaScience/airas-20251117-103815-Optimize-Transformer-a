{
  "research_topic": "Optimize Transformer training by incorporating biological perspectives",
  "queries": [
    "bio-inspired transformer",
    "Hebbian learning transformer",
    "spiking neural transformer",
    "biologically plausible attention",
    "synaptic plasticity transformer"
  ],
  "research_study_list": [
    {
      "title": "Analogous to Evolutionary Algorithm: Designing a Unified Sequence Model",
      "abstract": "Inspired by biological evolution, we explain the rationality of Vision Transformer by analogy with the proven practical Evolutionary Algorithm (EA) and derive that both of them have consistent mathematical representation. Analogous to the dynamic local population in EA, we improve the existing transformer structure and propose a more efficient EAT model, and design task-related heads to deal with different tasks more flexibly. Moreover, we introduce the spatial-filling curve into the current vision transformer to sequence image data into a uniform sequential format. Thus we can design a unified EAT framework to address multi-modal tasks, separating the network architecture from the data format adaptation. Our approach achieves state-of-the-art results on the ImageNet classification task compared with recent vision transformer works while having smaller parameters and greater throughput. We further conduct multi-modal tasks to demonstrate the superiority of the unified EAT, e.g., Text-Based Image Retrieval, and our approach improves the rank-1 by +3.7 points over the baseline on the CSS dataset.",
      "meta_data": {
        "arxiv_id": "2105.15089v3",
        "authors": [
          "Jiangning Zhang",
          "Chao Xu",
          "Jian Li",
          "Wenzhou Chen",
          "Yabiao Wang",
          "Ying Tai",
          "Shuo Chen",
          "Chengjie Wang",
          "Feiyue Huang",
          "Yong Liu"
        ],
        "published_date": "2021-05-31T16:20:03Z",
        "pdf_url": "https://arxiv.org/pdf/2105.15089v3.pdf"
      }
    },
    {
      "title": "Spikformer: When Spiking Neural Network Meets Transformer ",
      "abstract": "We consider two biologically plausible structures, the Spiking Neural Network (SNN) and the self-attention mechanism. The former offers an energy-efficient and event-driven paradigm for deep learning, while the latter has the ability to capture feature dependencies, enabling Transformer to achieve good performance. It is intuitively promising to explore the marriage between them. In this paper, we consider leveraging both self-attention capability and biological properties of SNNs, and propose a novel Spiking Self Attention (SSA) as well as a powerful framework, named Spiking Transformer (Spikformer). The SSA mechanism in Spikformer models the sparse visual feature by using spike-form Query, Key, and Value without softmax. Since its computation is sparse and avoids multiplication, SSA is efficient and has low computational energy consumption. It is shown that Spikformer with SSA can outperform the state-of-the-art SNNs-like frameworks in image classification on both neuromorphic and static datasets. Spikformer (66.3M parameters) with comparable size to SEW-ResNet-152 (60.2M,69.26%) can achieve 74.81% top1 accuracy on ImageNet using 4 time steps, which is the state-of-the-art in directly trained SNNs models.",
      "meta_data": {
        "arxiv_id": "2209.15425v2",
        "authors": [
          "Zhaokun Zhou",
          "Yuesheng Zhu",
          "Chao He",
          "Yaowei Wang",
          "Shuicheng Yan",
          "Yonghong Tian",
          "Li Yuan"
        ],
        "published_date": "2022-09-29T14:16:49Z",
        "pdf_url": "https://arxiv.org/pdf/2209.15425v2.pdf"
      }
    },
    {
      "title": "BIOT: Biosignal Transformer for Cross-data Learning in the Wild",
      "abstract": "Biological signals, such as electroencephalograms (EEG), play a crucial role in numerous clinical applications, exhibiting diverse data formats and quality profiles. Current deep learning models for biosignals are typically specialized for specific datasets and clinical settings, limiting their broader applicability. Motivated by the success of large language models in text processing, we explore the development of foundational models that are trained from multiple data sources and can be fine-tuned on different downstream biosignal tasks.\n  To overcome the unique challenges associated with biosignals of various formats, such as mismatched channels, variable sample lengths, and prevalent missing values, we propose a Biosignal Transformer (\\method). The proposed \\method model can enable cross-data learning with mismatched channels, variable lengths, and missing values by tokenizing diverse biosignals into unified \"biosignal sentences\". Specifically, we tokenize each channel into fixed-length segments containing local signal features, flattening them to form consistent \"sentences\". Channel embeddings and {\\em relative} position embeddings are added to preserve spatio-temporal features.\n  The \\method model is versatile and applicable to various biosignal learning settings across different datasets, including joint pre-training for larger models. Comprehensive evaluations on EEG, electrocardiogram (ECG), and human activity sensory signals demonstrate that \\method outperforms robust baselines in common settings and facilitates learning across multiple datasets with different formats. Use CHB-MIT seizure detection task as an example, our vanilla \\method model shows 3\\% improvement over baselines in balanced accuracy, and the pre-trained \\method models (optimized from other data sources) can further bring up to 4\\% improvements.",
      "meta_data": {
        "arxiv_id": "2305.10351v1",
        "authors": [
          "Chaoqi Yang",
          "M. Brandon Westover",
          "Jimeng Sun"
        ],
        "published_date": "2023-05-10T19:26:58Z",
        "pdf_url": "https://arxiv.org/pdf/2305.10351v1.pdf"
      }
    },
    {
      "title": "Redesigning the Transformer Architecture with Insights from Multi-particle Dynamical Systems",
      "abstract": "The Transformer and its variants have been proven to be efficient sequence learners in many different domains. Despite their staggering success, a critical issue has been the enormous number of parameters that must be trained (ranging from $10^7$ to $10^{11}$) along with the quadratic complexity of dot-product attention. In this work, we investigate the problem of approximating the two central components of the Transformer -- multi-head self-attention and point-wise feed-forward transformation, with reduced parameter space and computational complexity. We build upon recent developments in analyzing deep neural networks as numerical solvers of ordinary differential equations. Taking advantage of an analogy between Transformer stages and the evolution of a dynamical system of multiple interacting particles, we formulate a temporal evolution scheme, TransEvolve, to bypass costly dot-product attention over multiple stacked layers. We perform exhaustive experiments with TransEvolve on well-known encoder-decoder as well as encoder-only tasks. We observe that the degree of approximation (or inversely, the degree of parameter reduction) has different effects on the performance, depending on the task. While in the encoder-decoder regime, TransEvolve delivers performances comparable to the original Transformer, in encoder-only tasks it consistently outperforms Transformer along with several subsequent variants.",
      "meta_data": {
        "arxiv_id": "2109.15142v3",
        "authors": [
          "Subhabrata Dutta",
          "Tanya Gautam",
          "Soumen Chakrabarti",
          "Tanmoy Chakraborty"
        ],
        "published_date": "2021-09-30T14:01:06Z",
        "pdf_url": "https://arxiv.org/pdf/2109.15142v3.pdf"
      }
    },
    {
      "title": "Redesigning the Transformer Architecture with Insights from Multi-particle Dynamical Systems",
      "abstract": "The Transformer and its variants have been proven to be efficient sequence learners in many different domains. Despite their staggering success, a critical issue has been the enormous number of parameters that must be trained (ranging from $10^7$ to $10^{11}$) along with the quadratic complexity of dot-product attention. In this work, we investigate the problem of approximating the two central components of the Transformer -- multi-head self-attention and point-wise feed-forward transformation, with reduced parameter space and computational complexity. We build upon recent developments in analyzing deep neural networks as numerical solvers of ordinary differential equations. Taking advantage of an analogy between Transformer stages and the evolution of a dynamical system of multiple interacting particles, we formulate a temporal evolution scheme, TransEvolve, to bypass costly dot-product attention over multiple stacked layers. We perform exhaustive experiments with TransEvolve on well-known encoder-decoder as well as encoder-only tasks. We observe that the degree of approximation (or inversely, the degree of parameter reduction) has different effects on the performance, depending on the task. While in the encoder-decoder regime, TransEvolve delivers performances comparable to the original Transformer, in encoder-only tasks it consistently outperforms Transformer along with several subsequent variants.",
      "meta_data": {
        "arxiv_id": "2109.15142v3",
        "authors": [
          "Subhabrata Dutta",
          "Tanya Gautam",
          "Soumen Chakrabarti",
          "Tanmoy Chakraborty"
        ],
        "published_date": "2021-09-30T14:01:06Z",
        "pdf_url": "https://arxiv.org/pdf/2109.15142v3.pdf"
      }
    },
    {
      "title": "Hebbian Deep Learning Without Feedback",
      "abstract": "Recent approximations to backpropagation (BP) have mitigated many of BP's computational inefficiencies and incompatibilities with biology, but important limitations still remain. Moreover, the approximations significantly decrease accuracy in benchmarks, suggesting that an entirely different approach may be more fruitful. Here, grounded on recent theory for Hebbian learning in soft winner-take-all networks, we present multilayer SoftHebb, i.e. an algorithm that trains deep neural networks, without any feedback, target, or error signals. As a result, it achieves efficiency by avoiding weight transport, non-local plasticity, time-locking of layer updates, iterative equilibria, and (self-) supervisory or other feedback signals -- which were necessary in other approaches. Its increased efficiency and biological compatibility do not trade off accuracy compared to state-of-the-art bio-plausible learning, but rather improve it. With up to five hidden layers and an added linear classifier, accuracies on MNIST, CIFAR-10, STL-10, and ImageNet, respectively reach 99.4%, 80.3%, 76.2%, and 27.3%. In conclusion, SoftHebb shows with a radically different approach from BP that Deep Learning over few layers may be plausible in the brain and increases the accuracy of bio-plausible machine learning. Code is available at https://github.com/NeuromorphicComputing/SoftHebb.",
      "meta_data": {
        "arxiv_id": "2209.11883v2",
        "authors": [
          "Adrien Journé",
          "Hector Garcia Rodriguez",
          "Qinghai Guo",
          "Timoleon Moraitis"
        ],
        "published_date": "2022-09-23T23:12:59Z",
        "venue": "The Eleventh International Conference on Learning Representations (2023) Retrieved from https://openreview.net/forum?id=8gd4M-_Rj1",
        "pdf_url": "https://arxiv.org/pdf/2209.11883v2.pdf"
      }
    },
    {
      "title": "Hebbian Deep Learning Without Feedback",
      "abstract": "Recent approximations to backpropagation (BP) have mitigated many of BP's computational inefficiencies and incompatibilities with biology, but important limitations still remain. Moreover, the approximations significantly decrease accuracy in benchmarks, suggesting that an entirely different approach may be more fruitful. Here, grounded on recent theory for Hebbian learning in soft winner-take-all networks, we present multilayer SoftHebb, i.e. an algorithm that trains deep neural networks, without any feedback, target, or error signals. As a result, it achieves efficiency by avoiding weight transport, non-local plasticity, time-locking of layer updates, iterative equilibria, and (self-) supervisory or other feedback signals -- which were necessary in other approaches. Its increased efficiency and biological compatibility do not trade off accuracy compared to state-of-the-art bio-plausible learning, but rather improve it. With up to five hidden layers and an added linear classifier, accuracies on MNIST, CIFAR-10, STL-10, and ImageNet, respectively reach 99.4%, 80.3%, 76.2%, and 27.3%. In conclusion, SoftHebb shows with a radically different approach from BP that Deep Learning over few layers may be plausible in the brain and increases the accuracy of bio-plausible machine learning. Code is available at https://github.com/NeuromorphicComputing/SoftHebb.",
      "meta_data": {
        "arxiv_id": "2209.11883v2",
        "authors": [
          "Adrien Journé",
          "Hector Garcia Rodriguez",
          "Qinghai Guo",
          "Timoleon Moraitis"
        ],
        "published_date": "2022-09-23T23:12:59Z",
        "venue": "The Eleventh International Conference on Learning Representations (2023) Retrieved from https://openreview.net/forum?id=8gd4M-_Rj1",
        "pdf_url": "https://arxiv.org/pdf/2209.11883v2.pdf"
      }
    },
    {
      "title": "Unsupervised 3D Object Learning through Neuron Activity aware Plasticity",
      "abstract": "We present an unsupervised deep learning model for 3D object classification. Conventional Hebbian learning, a well-known unsupervised model, suffers from loss of local features leading to reduced performance for tasks with complex geometric objects. We present a deep network with a novel Neuron Activity Aware (NeAW) Hebbian learning rule that dynamically switches the neurons to be governed by Hebbian learning or anti-Hebbian learning, depending on its activity. We analytically show that NeAW Hebbian learning relieves the bias in neuron activity, allowing more neurons to attend to the representation of the 3D objects. Empirical results show that the NeAW Hebbian learning outperforms other variants of Hebbian learning and shows higher accuracy over fully supervised models when training data is limited.",
      "meta_data": {
        "arxiv_id": "2302.11622v1",
        "authors": [
          "Beomseok Kang",
          "Biswadeep Chakraborty",
          "Saibal Mukhopadhyay"
        ],
        "published_date": "2023-02-22T19:57:12Z",
        "pdf_url": "https://arxiv.org/pdf/2302.11622v1.pdf"
      }
    },
    {
      "title": "Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks",
      "abstract": "Neuromorphic computing with spiking neural networks is promising for energy-efficient artificial intelligence (AI) applications. However, different from humans who continually learn different tasks in a lifetime, neural network models suffer from catastrophic forgetting. How could neuronal operations solve this problem is an important question for AI and neuroscience. Many previous studies draw inspiration from observed neuroscience phenomena and propose episodic replay or synaptic metaplasticity, but they are not guaranteed to explicitly preserve knowledge for neuron populations. Other works focus on machine learning methods with more mathematical grounding, e.g., orthogonal projection on high dimensional spaces, but there is no neural correspondence for neuromorphic computing. In this work, we develop a new method with neuronal operations based on lateral connections and Hebbian learning, which can protect knowledge by projecting activity traces of neurons into an orthogonal subspace so that synaptic weight update will not interfere with old tasks. We show that Hebbian and anti-Hebbian learning on recurrent lateral connections can effectively extract the principal subspace of neural activities and enable orthogonal projection. This provides new insights into how neural circuits and Hebbian learning can help continual learning, and also how the concept of orthogonal projection can be realized in neuronal systems. Our method is also flexible to utilize arbitrary training methods based on presynaptic activities/traces. Experiments show that our method consistently solves forgetting for spiking neural networks with nearly zero forgetting under various supervised training methods with different error propagation approaches, and outperforms previous approaches under various settings. Our method can pave a solid path for building continual neuromorphic computing systems.",
      "meta_data": {
        "arxiv_id": "2402.11984v1",
        "authors": [
          "Mingqing Xiao",
          "Qingyan Meng",
          "Zongpeng Zhang",
          "Di He",
          "Zhouchen Lin"
        ],
        "published_date": "2024-02-19T09:29:37Z",
        "pdf_url": "https://arxiv.org/pdf/2402.11984v1.pdf"
      }
    },
    {
      "title": "Outlier-Efficient Hopfield Layers for Large Transformer-Based Models",
      "abstract": "We introduce an Outlier-Efficient Modern Hopfield Model (termed $\\mathrm{OutEffHop}$) and use it to address the outlier inefficiency problem of {training} gigantic transformer-based models. Our main contribution is a novel associative memory model facilitating \\textit{outlier-efficient} associative memory retrievals. Interestingly, this memory model manifests a model-based interpretation of an outlier-efficient attention mechanism (${\\rm Softmax}_1$): it is an approximation of the memory retrieval process of $\\mathrm{OutEffHop}$. Methodologically, this allows us to introduce novel outlier-efficient Hopfield layers as powerful alternatives to traditional attention mechanisms, with superior post-quantization performance. Theoretically, the Outlier-Efficient Modern Hopfield Model retains and improves the desirable properties of standard modern Hopfield models, including fixed point convergence and exponential storage capacity. Empirically, we demonstrate the efficacy of the proposed model across large-scale transformer-based and Hopfield-based models (including BERT, OPT, ViT, and STanHop-Net), benchmarking against state-of-the-art methods like $\\mathtt{Clipped\\_Softmax}$ and $\\mathtt{Gated\\_Attention}$. Notably, $\\mathrm{OutEffHop}$ achieves an average reduction of 22+\\% in average kurtosis and 26+\\% in the maximum infinity norm of model outputs across four models. Code is available at \\href{https://github.com/MAGICS-LAB/OutEffHop}{GitHub}; models are on \\href{https://huggingface.co/collections/magicslabnu/outeffhop-6610fcede8d2cda23009a98f}{Hugging Face Hub}; future updates are on \\href{https://arxiv.org/abs/2404.03828}{arXiv}.",
      "meta_data": {
        "arxiv_id": "2404.03828v2",
        "authors": [
          "Jerry Yao-Chieh Hu",
          "Pei-Hsuan Chang",
          "Robin Luo",
          "Hong-Yu Chen",
          "Weijian Li",
          "Wei-Po Wang",
          "Han Liu"
        ],
        "published_date": "2024-04-04T23:08:43Z",
        "pdf_url": "https://arxiv.org/pdf/2404.03828v2.pdf"
      }
    },
    {
      "title": "Spikformer: When Spiking Neural Network Meets Transformer ",
      "abstract": "We consider two biologically plausible structures, the Spiking Neural Network (SNN) and the self-attention mechanism. The former offers an energy-efficient and event-driven paradigm for deep learning, while the latter has the ability to capture feature dependencies, enabling Transformer to achieve good performance. It is intuitively promising to explore the marriage between them. In this paper, we consider leveraging both self-attention capability and biological properties of SNNs, and propose a novel Spiking Self Attention (SSA) as well as a powerful framework, named Spiking Transformer (Spikformer). The SSA mechanism in Spikformer models the sparse visual feature by using spike-form Query, Key, and Value without softmax. Since its computation is sparse and avoids multiplication, SSA is efficient and has low computational energy consumption. It is shown that Spikformer with SSA can outperform the state-of-the-art SNNs-like frameworks in image classification on both neuromorphic and static datasets. Spikformer (66.3M parameters) with comparable size to SEW-ResNet-152 (60.2M,69.26%) can achieve 74.81% top1 accuracy on ImageNet using 4 time steps, which is the state-of-the-art in directly trained SNNs models.",
      "meta_data": {
        "arxiv_id": "2209.15425v2",
        "authors": [
          "Zhaokun Zhou",
          "Yuesheng Zhu",
          "Chao He",
          "Yaowei Wang",
          "Shuicheng Yan",
          "Yonghong Tian",
          "Li Yuan"
        ],
        "published_date": "2022-09-29T14:16:49Z",
        "pdf_url": "https://arxiv.org/pdf/2209.15425v2.pdf"
      }
    },
    {
      "title": "SpikedAttention: Training-Free and Fully Spike-Driven Transformer-to-SNN Conversion with Winner-Oriented Spike Shift for Softmax Operation"
    },
    {
      "title": "SpikeZIP-TF: Conversion is All You Need for Transformer-based SNN",
      "abstract": "Spiking neural network (SNN) has attracted great attention due to its characteristic of high efficiency and accuracy. Currently, the ANN-to-SNN conversion methods can obtain ANN on-par accuracy SNN with ultra-low latency (8 time-steps) in CNN structure on computer vision (CV) tasks. However, as Transformer-based networks have achieved prevailing precision on both CV and natural language processing (NLP), the Transformer-based SNNs are still encounting the lower accuracy w.r.t the ANN counterparts. In this work, we introduce a novel ANN-to-SNN conversion method called SpikeZIP-TF, where ANN and SNN are exactly equivalent, thus incurring no accuracy degradation. SpikeZIP-TF achieves 83.82% accuracy on CV dataset (ImageNet) and 93.79% accuracy on NLP dataset (SST-2), which are higher than SOTA Transformer-based SNNs. The code is available in GitHub: https://github.com/Intelligent-Computing-Research-Group/SpikeZIP_transformer",
      "meta_data": {
        "arxiv_id": "2406.03470v1",
        "authors": [
          "Kang You",
          "Zekai Xu",
          "Chen Nie",
          "Zhijie Deng",
          "Qinghai Guo",
          "Xiang Wang",
          "Zhezhi He"
        ],
        "published_date": "2024-06-05T17:24:07Z",
        "venue": "International Conference on Machine Learning 2024",
        "pdf_url": "https://arxiv.org/pdf/2406.03470v1.pdf"
      }
    },
    {
      "title": "Spike-driven Transformer",
      "abstract": "Spiking Neural Networks (SNNs) provide an energy-efficient deep learning option due to their unique spike-based event-driven (i.e., spike-driven) paradigm. In this paper, we incorporate the spike-driven paradigm into Transformer by the proposed Spike-driven Transformer with four unique properties: 1) Event-driven, no calculation is triggered when the input of Transformer is zero; 2) Binary spike communication, all matrix multiplications associated with the spike matrix can be transformed into sparse additions; 3) Self-attention with linear complexity at both token and channel dimensions; 4) The operations between spike-form Query, Key, and Value are mask and addition. Together, there are only sparse addition operations in the Spike-driven Transformer. To this end, we design a novel Spike-Driven Self-Attention (SDSA), which exploits only mask and addition operations without any multiplication, and thus having up to $87.2\\times$ lower computation energy than vanilla self-attention. Especially in SDSA, the matrix multiplication between Query, Key, and Value is designed as the mask operation. In addition, we rearrange all residual connections in the vanilla Transformer before the activation functions to ensure that all neurons transmit binary spike signals. It is shown that the Spike-driven Transformer can achieve 77.1\\% top-1 accuracy on ImageNet-1K, which is the state-of-the-art result in the SNN field. The source code is available at https://github.com/BICLab/Spike-Driven-Transformer.",
      "meta_data": {
        "arxiv_id": "2307.01694v1",
        "authors": [
          "Man Yao",
          "Jiakui Hu",
          "Zhaokun Zhou",
          "Li Yuan",
          "Yonghong Tian",
          "Bo Xu",
          "Guoqi Li"
        ],
        "published_date": "2023-07-04T13:00:18Z",
        "pdf_url": "https://arxiv.org/pdf/2307.01694v1.pdf"
      }
    },
    {
      "title": "Spatio-Temporal Approximation: A Training-Free SNN Conversion for Transformers"
    },
    {
      "title": "Attention as Implicit Structural Inference"
    },
    {
      "title": "Neural encoding with visual attention",
      "abstract": "Visual perception is critically influenced by the focus of attention. Due to limited resources, it is well known that neural representations are biased in favor of attended locations. Using concurrent eye-tracking and functional Magnetic Resonance Imaging (fMRI) recordings from a large cohort of human subjects watching movies, we first demonstrate that leveraging gaze information, in the form of attentional masking, can significantly improve brain response prediction accuracy in a neural encoding model. Next, we propose a novel approach to neural encoding by including a trainable soft-attention module. Using our new approach, we demonstrate that it is possible to learn visual attention policies by end-to-end learning merely on fMRI response data, and without relying on any eye-tracking. Interestingly, we find that attention locations estimated by the model on independent data agree well with the corresponding eye fixation patterns, despite no explicit supervision to do so. Together, these findings suggest that attention modules can be instrumental in neural encoding models of visual stimuli.",
      "meta_data": {
        "arxiv_id": "2010.00516v1",
        "authors": [
          "Meenakshi Khosla",
          "Gia H. Ngo",
          "Keith Jamison",
          "Amy Kuceyeski",
          "Mert R. Sabuncu"
        ],
        "published_date": "2020-10-01T16:04:21Z",
        "pdf_url": "https://arxiv.org/pdf/2010.00516v1.pdf"
      }
    },
    {
      "title": "Neural encoding with visual attention",
      "abstract": "Visual perception is critically influenced by the focus of attention. Due to limited resources, it is well known that neural representations are biased in favor of attended locations. Using concurrent eye-tracking and functional Magnetic Resonance Imaging (fMRI) recordings from a large cohort of human subjects watching movies, we first demonstrate that leveraging gaze information, in the form of attentional masking, can significantly improve brain response prediction accuracy in a neural encoding model. Next, we propose a novel approach to neural encoding by including a trainable soft-attention module. Using our new approach, we demonstrate that it is possible to learn visual attention policies by end-to-end learning merely on fMRI response data, and without relying on any eye-tracking. Interestingly, we find that attention locations estimated by the model on independent data agree well with the corresponding eye fixation patterns, despite no explicit supervision to do so. Together, these findings suggest that attention modules can be instrumental in neural encoding models of visual stimuli.",
      "meta_data": {
        "arxiv_id": "2010.00516v1",
        "authors": [
          "Meenakshi Khosla",
          "Gia H. Ngo",
          "Keith Jamison",
          "Amy Kuceyeski",
          "Mert R. Sabuncu"
        ],
        "published_date": "2020-10-01T16:04:21Z",
        "pdf_url": "https://arxiv.org/pdf/2010.00516v1.pdf"
      }
    },
    {
      "title": "Attention Approximates Sparse Distributed Memory",
      "abstract": "While Attention has come to be an important mechanism in deep learning, there remains limited intuition for why it works so well. Here, we show that Transformer Attention can be closely related under certain data conditions to Kanerva's Sparse Distributed Memory (SDM), a biologically plausible associative memory model. We confirm that these conditions are satisfied in pre-trained GPT2 Transformer models. We discuss the implications of the Attention-SDM map and provide new computational and biological interpretations of Attention.",
      "meta_data": {
        "arxiv_id": "2111.05498v2",
        "authors": [
          "Trenton Bricken",
          "Cengiz Pehlevan"
        ],
        "published_date": "2021-11-10T02:36:32Z",
        "venue": "35th Conference on Neural Information Processing Systems (NeurIPS 2021)",
        "pdf_url": "https://arxiv.org/pdf/2111.05498v2.pdf"
      }
    },
    {
      "title": "Biologically Inspired Learning Model for Instructed Vision",
      "abstract": "As part of understanding how the brain learns, ongoing work seeks to combine biological knowledge and current artificial intelligence (AI) modeling in an attempt to find an efficient biologically plausible learning scheme. Current models of biologically plausible learning often use a cortical-like combination of bottom-up (BU) and top-down (TD) processing, where the TD part carries feedback signals used for learning. However, in the visual cortex, the TD pathway plays a second major role of visual attention, by guiding the visual process to locations and tasks of interest. A biological model should therefore combine the two tasks, and learn to guide the visual process. We introduce a model that uses a cortical-like combination of BU and TD processing that naturally integrates the two major functions of the TD stream. The integrated model is obtained by an appropriate connectivity pattern between the BU and TD streams, a novel processing cycle that uses the TD part twice, and the use of 'Counter-Hebb' learning that operates across the streams. We show that the 'Counter-Hebb' mechanism can provide an exact backpropagation synaptic modification. We further demonstrate the model's ability to guide the visual stream to perform a task of interest, achieving competitive performance compared with AI models on standard multi-task learning benchmarks. The successful combination of learning and visual guidance could provide a new view on combining BU and TD processing in human vision, and suggests possible directions for both biologically plausible models and artificial instructed models, such as vision-language models (VLMs).",
      "meta_data": {
        "arxiv_id": "2306.02415v3",
        "authors": [
          "Roy Abel",
          "Shimon Ullman"
        ],
        "published_date": "2023-06-04T17:38:06Z",
        "pdf_url": "https://arxiv.org/pdf/2306.02415v3.pdf"
      }
    },
    {
      "title": "Short-Term Plasticity Neurons Learning to Learn and Forget",
      "abstract": "Short-term plasticity (STP) is a mechanism that stores decaying memories in synapses of the cerebral cortex. In computing practice, STP has been used, but mostly in the niche of spiking neurons, even though theory predicts that it is the optimal solution to certain dynamic tasks. Here we present a new type of recurrent neural unit, the STP Neuron (STPN), which indeed turns out strikingly powerful. Its key mechanism is that synapses have a state, propagated through time by a self-recurrent connection-within-the-synapse. This formulation enables training the plasticity with backpropagation through time, resulting in a form of learning to learn and forget in the short term. The STPN outperforms all tested alternatives, i.e. RNNs, LSTMs, other models with fast weights, and differentiable plasticity. We confirm this in both supervised and reinforcement learning (RL), and in tasks such as Associative Retrieval, Maze Exploration, Atari video games, and MuJoCo robotics. Moreover, we calculate that, in neuromorphic or biological circuits, the STPN minimizes energy consumption across models, as it depresses individual synapses dynamically. Based on these, biological STP may have been a strong evolutionary attractor that maximizes both efficiency and computational power. The STPN now brings these neuromorphic advantages also to a broad spectrum of machine learning practice. Code is available at https://github.com/NeuromorphicComputing/stpn",
      "meta_data": {
        "arxiv_id": "2206.14048v1",
        "authors": [
          "Hector Garcia Rodriguez",
          "Qinghai Guo",
          "Timoleon Moraitis"
        ],
        "published_date": "2022-06-28T14:47:56Z",
        "venue": "Proceedings of the 39th International Conference on Machine Learning, 162:18704-18722 (2022)",
        "pdf_url": "https://arxiv.org/pdf/2206.14048v1.pdf"
      }
    },
    {
      "title": "Short-Term Plasticity Neurons Learning to Learn and Forget",
      "abstract": "Short-term plasticity (STP) is a mechanism that stores decaying memories in synapses of the cerebral cortex. In computing practice, STP has been used, but mostly in the niche of spiking neurons, even though theory predicts that it is the optimal solution to certain dynamic tasks. Here we present a new type of recurrent neural unit, the STP Neuron (STPN), which indeed turns out strikingly powerful. Its key mechanism is that synapses have a state, propagated through time by a self-recurrent connection-within-the-synapse. This formulation enables training the plasticity with backpropagation through time, resulting in a form of learning to learn and forget in the short term. The STPN outperforms all tested alternatives, i.e. RNNs, LSTMs, other models with fast weights, and differentiable plasticity. We confirm this in both supervised and reinforcement learning (RL), and in tasks such as Associative Retrieval, Maze Exploration, Atari video games, and MuJoCo robotics. Moreover, we calculate that, in neuromorphic or biological circuits, the STPN minimizes energy consumption across models, as it depresses individual synapses dynamically. Based on these, biological STP may have been a strong evolutionary attractor that maximizes both efficiency and computational power. The STPN now brings these neuromorphic advantages also to a broad spectrum of machine learning practice. Code is available at https://github.com/NeuromorphicComputing/stpn",
      "meta_data": {
        "arxiv_id": "2206.14048v1",
        "authors": [
          "Hector Garcia Rodriguez",
          "Qinghai Guo",
          "Timoleon Moraitis"
        ],
        "published_date": "2022-06-28T14:47:56Z",
        "venue": "Proceedings of the 39th International Conference on Machine Learning, 162:18704-18722 (2022)",
        "pdf_url": "https://arxiv.org/pdf/2206.14048v1.pdf"
      }
    },
    {
      "title": "Spatio-Temporal Approximation: A Training-Free SNN Conversion for Transformers"
    },
    {
      "title": "Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data",
      "abstract": "State-of-the-art systems neuroscience experiments yield large-scale multimodal data, and these data sets require new tools for analysis. Inspired by the success of large pretrained models in vision and language domains, we reframe the analysis of large-scale, cellular-resolution neuronal spiking data into an autoregressive spatiotemporal generation problem. Neuroformer is a multimodal, multitask generative pretrained transformer (GPT) model that is specifically designed to handle the intricacies of data in systems neuroscience. It scales linearly with feature size, can process an arbitrary number of modalities, and is adaptable to downstream tasks, such as predicting behavior. We first trained Neuroformer on simulated datasets, and found that it both accurately predicted simulated neuronal circuit activity, and also intrinsically inferred the underlying neural circuit connectivity, including direction. When pretrained to decode neural responses, the model predicted the behavior of a mouse with only few-shot fine-tuning, suggesting that the model begins learning how to do so directly from the neural representations themselves, without any explicit supervision. We used an ablation study to show that joint training on neuronal responses and behavior boosted performance, highlighting the model's ability to associate behavioral and neural representations in an unsupervised manner. These findings show that Neuroformer can analyze neural datasets and their emergent properties, informing the development of models and hypotheses associated with the brain.",
      "meta_data": {
        "arxiv_id": "2311.00136v4",
        "authors": [
          "Antonis Antoniades",
          "Yiyi Yu",
          "Joseph Canzano",
          "William Wang",
          "Spencer LaVere Smith"
        ],
        "published_date": "2023-10-31T20:17:32Z",
        "pdf_url": "https://arxiv.org/pdf/2311.00136v4.pdf"
      }
    },
    {
      "title": "Spikformer: When Spiking Neural Network Meets Transformer "
    }
  ],
  "evaluated_hypothesis_history": [
    {
      "hypothesis": {
        "open_problems": "1. Transformers often develop very large internal activation magnitudes in some layers while others remain under-utilised, leading to unstable training and sub-optimal generalisation.\n2. Biological neurons exhibit homeostatic synaptic scaling: average firing rates are kept in a narrow range by up- or down-regulating all incoming weights. This stabilises activity without changing relative information content.\n3. Current Transformer training does not explicitly enforce such layer-wise homeostasis; adding it only requires a tiny change to the loss function.",
        "method": "Homeostatic Synaptic Scaling Regularisation (HSSR)\n• For every Transformer layer l compute the mini-batch mean pre-activation magnitude  A_l  (e.g. mean(|h_l|)).\n• Add a quadratic penalty that encourages A_l to stay close to a biologically plausible target τ (τ≈1.0).\n  L_total = L_task  +  λ · Σ_l (A_l − τ)^2\n• λ is a small constant (e.g. 1e-3). No other changes to optimiser, architecture, or training loop are needed.\nTheoretical motivation: Keeping layer-wise activity in a narrow band mimics biological homeostasis, avoids saturation/exploding activations, promotes balanced gradient flow, and implicitly regularises the model.",
        "experimental_setup": "Model: 2-layer Transformer encoder (tiny) with 2 attention heads (~1 M parameters).\nDataset: WikiText-2 word-level language modelling.\nTraining: 40 epochs with Adam (lr=1e-3, warm-up 2k steps).\nBaselines: (a) Standard training, (b) Standard + conventional L2 weight decay (0.01).\nProposed: Standard + HSSR (λ=1e-3, τ=1.0).\nEvaluation: validation and test perplexity every epoch.",
        "primary_metric": "perplexity",
        "experimental_code": "import torch, torch.nn as nn\n\nclass HSSR(nn.Module):\n    def __init__(self, model, tau=1.0, lam=1e-3):\n        super().__init__()\n        self.layers = [m for m in model.modules() if isinstance(m, nn.TransformerEncoderLayer)]\n        self.tau, self.lam = tau, lam\n    def forward(self):\n        loss = 0.0\n        for layer in self.layers:\n            # hook retrieves pre-activations stored during forward pass\n            act = layer.last_hidden_state   # shape (seq,batch,dim)\n            A = act.abs().mean()\n            loss = loss + (A - self.tau) ** 2\n        return self.lam * loss\n\n# in training loop\nout = model(src)            # usual forward\nloss_task = criterion(out, tgt)\nloss_hssr = hssr()           # extra regulariser\nloss = loss_task + loss_hssr\nloss.backward()\noptimizer.step()",
        "expected_result": "On WikiText-2 test set after 40 epochs:\n• Baseline perplexity ≈ 72\n• +L2 weight decay perplexity ≈ 70\n• +HSSR perplexity ≈ 66 (≈8% relative reduction)\nTraining is more stable: gradient norm variance reduced by ~15%.",
        "expected_conclusion": "Adding a biologically inspired homeostatic term is trivial to implement yet yields noticeably lower perplexity and stabler training. The improvement arises because balanced layer activations prevent saturation and maintain informative gradients, mirroring how biological neurons regulate firing rates. Thus, minimal biological insight can translate into practical gains for Transformer optimisation."
      },
      "evaluation": {
        "novelty_reason": "The idea of directly regularising the average activation magnitude is related to existing techniques such as (1) activation-regularisation and temporal activation-regularisation used for RNN language models (e.g., AWD-LSTM), (2) squared-L2 penalties on hidden states sometimes called \"Activation L2\" in Transformer implementations, and (3) implicit control of activation statistics through BatchNorm or LayerNorm.  However, those methods are motivated from over-fitting or optimisation convenience, not from the biological concept of homeostatic synaptic scaling, and they are usually applied to specific layers or tokens rather than enforced uniformly across all Transformer layers with a global target τ.  The proposed HSSR frames the constraint explicitly as a biologically-inspired layer-wise homeostasis term, applies it to every Transformer layer regardless of position, and requires only one additional scalar per layer, which to the best of my knowledge has not been systematically studied in Transformer training literature.  Therefore the hypothesis is moderately novel: it repackages a familiar mathematical tool but introduces a new biological motivation and a uniform, very simple implementation that has not yet been evaluated for Transformers.",
        "novelty_score": 6,
        "significance_reason": "If the hypothesised ~8 % perplexity drop and 15 % reduction in gradient-norm variance generalise to larger models and datasets, the method would offer a virtually free improvement to Transformer training—no architectural change, negligible computational overhead, and only one hyper-parameter.  This makes it attractive for both academic research (better understanding of how activation homeostasis affects optimisation and generalisation) and for industry-scale training where stability is critical and training budgets are huge.  In addition, showing that a concrete principle from neurobiology yields measurable gains strengthens the interdisciplinary dialogue between neuroscience and machine learning, a topic of rising academic interest.  Nonetheless, the current evidence is limited to a tiny model on WikiText-2; impact on state-of-the-art, multi-billion-parameter Transformers and other modalities is untested.  Hence the potential significance is promising but not yet proven.",
        "significance_score": 7
      }
    },
    {
      "hypothesis": {
        "open_problems": "1. Transformer training is notoriously sensitive to the scale of internal activations; when a few layers or heads run \"hot\" while others remain \"cold\", gradient flow becomes erratic, convergence slows, and generalisation suffers.\n2. Existing fixes (LayerNorm, activation-L2, careful learning-rate schedules) require hand-tuned hyper-parameters that are re-optimised for every model size, task, and hardware setting – an expensive practice that inflates the carbon footprint of research and industry deployments.\n3. In the brain, firing-rate homeostasis is achieved through an *integral* feedback mechanism: neurons accumulate an error signal (difference between actual and target rate) and use it to multiplicatively scale all incoming synapses, thereby restoring activity without external hyper-parameter tuning.\n4. No current Transformer optimisation scheme replicates this integral control; prior \"activation regularisers\" use a *fixed* penalty weight, meaning the strength of the correction is frozen throughout training and must be guessed by the user.\n5. The open question: Can we translate biological integral homeostatic control into a zero-tuning regulariser that autonomously keeps every Transformer layer in its optimal activation range, thereby improving both performance and sustainability?",
        "method": "Homeostatic Integral Control Regularisation (HICR)\n-----------------------------------------------------\nLet h_l \\in R^{B\\times T\\times d} be the pre-LayerNorm activations of Transformer layer l in a mini-batch of size B, sequence length T.\n1. Per-layer activation statistic:  A_l = mean(|h_l|).\n2. Target firing rate (constant): τ (default 1.0).\n3. Per-layer *homeostatic drive* λ_l  (a persistent scalar, one per layer) initialised to 0 and **not** updated by back-prop.\n4. Extra loss term:   L_homeo = Σ_l  λ_l · (A_l – τ).\n   (linear, not quadratic – mirrors Lagrange multipliers in constrained optimisation.)\n5. After every optimiser step, update each λ_l with an **integral control rule**\n      λ_l ← λ_l + η · (A_l.detach() – τ)\n   where η≈10^{-2} is a small learning rate.  This is gradient ascent on the dual variable and drives the *constraint* A_l=τ in expectation.\n6. No additional hyper-parameters beyond η (shared by all layers) and the biologically motivated τ.  Crucially, *λ_l learns itself*: if a layer is persistently over-active λ_l becomes positive and suppresses it; if under-active λ_l becomes negative and boosts it.\n   The mechanism is therefore plug-and-play, optimiser-agnostic, and scale-free.",
        "experimental_setup": "Language Modelling:\n  • Dataset: WikiText-2.\n  • Model: 2-layer Transformer encoder (≈1 M params).\n  • Training: Adam, lr=1e-3, 40 epochs, identical schedule for all conditions.\nVision:\n  • Dataset: CIFAR-10.\n  • Model: ViT-Tiny (4 heads, 192-dim, 12 layers).\n  • Training: AdamW, lr=3e-4, 100 epochs, cosine decay.\nBaselines:\n  a) Standard training\n  b) Quadratic homeostasis (HSSR from prior hypothesis, λ=1e-3)\nProposed:\n  c) HICR (η=0.01, τ=1.0)\nMetrics collected:\n  • WikiText-2: validation/test perplexity\n  • CIFAR-10: top-1 accuracy\n  • Training stability: variance of gradient norms, incidence of NaNs, wall-clock time until convergence\n  • Hyper-parameter cost: GPU-hours spent on λ/η grid-search (expected to be 0 for HICR)",
        "primary_metric": "(i) WikiText-2 test perplexity; (ii) CIFAR-10 top-1 accuracy.  Secondary: gradient-norm variance.",
        "experimental_code": "class HICR(torch.nn.Module):\n    def __init__(self, model, tau=1.0, eta=1e-2):\n        super().__init__()\n        self.tau, self.eta = tau, eta\n        # pick Transformer layers\n        self.layers = [m for m in model.modules()\n                       if isinstance(m, torch.nn.TransformerEncoderLayer)]\n        # one persistent lambda per layer, registered as buffer\n        for idx, _ in enumerate(self.layers):\n            self.register_buffer(f\"lambda_{idx}\", torch.zeros(1))\n\n    def penalty(self):\n        loss = 0.0\n        for idx, layer in enumerate(self.layers):\n            A = layer.cached_act.abs().mean()  # hook stores activations\n            lam = getattr(self, f\"lambda_{idx}\")\n            loss = loss + lam * (A - self.tau)\n        return loss\n\n    @torch.no_grad()\n    def update_lambdas(self):\n        for idx, layer in enumerate(self.layers):\n            A = layer.cached_act.abs().mean()\n            lam = getattr(self, f\"lambda_{idx}\")\n            lam += self.eta * (A - self.tau)\n\n# --- training loop snippet ---\nout = model(src)\nloss_task = criterion(out, tgt)\nloss_homeo = hicr.penalty()\nloss = loss_task + loss_homeo\nloss.backward()\noptimizer.step()\nhicr.update_lambdas()\noptimizer.zero_grad()",
        "expected_result": "WikiText-2 (40 epochs):\n  • Baseline perplexity ≈ 72\n  • Quadratic HSSR ≈ 66\n  • HICR ≈ 64  (≈11 % relative drop vs baseline)\n  • Gradient-norm variance down by ~20 %\nCIFAR-10 (ViT-Tiny, 100 epochs):\n  • Baseline accuracy 84.5 %\n  • HSSR 85.1 %\n  • HICR 86.0 % (+1.5 pp) with identical training budget\nHyper-parameter search cost:\n  • Baseline+HSSR: 6 grid points × 2 seeds = 12 runs\n  • HICR: zero additional runs (η kept fixed)",
        "expected_conclusion": "Replacing a fixed quadratic activation penalty with biologically inspired *integral* homeostatic control yields consistent gains in language and vision Transformers, while completely eliminating the need to tune a regularisation coefficient.  By continuously adjusting a per-layer drive variable, HICR keeps neural activity in a healthy regime, leading to smoother gradients, faster convergence, and better generalisation.  The work demonstrates that a direct translation of synaptic-scaling feedback control from neuroscience can reduce computational waste and carbon emissions associated with hyper-parameter sweeps, thus offering both academic insight and tangible social benefit."
      },
      "evaluation": {
        "novelty_reason": "1. Existing stabilisation techniques for Transformers (LayerNorm, BatchNorm variants, Fixup/ScaleNorm, ReZero, activation-L2 penalties, adaptive gradient clipping) all rely on coefficients that are fixed throughout training and must be selected by grid search. 2. Prior attempts to mimic biological homeostasis in ANNs generally use static quadratic penalties (e.g. synaptic-scaling regularisers, Sparsity penalties with a fixed beta) rather than a dual-ascent/integral controller that updates its own strength online. 3. The proposed Homeostatic Integral Control Regularisation (HICR) introduces an explicit dual variable λ_l per layer that is updated with gradient-free integral feedback—this mirrors the integral control found in neural firing-rate homeostasis and is not present in mainstream optimisation literature for Transformers. 4. To the best of current knowledge no published Transformer work combines: (i) an activation-level constraint, (ii) a dual-ascent rule external to back-prop, and (iii) zero additional tuned hyper-parameters, making the method conceptually and algorithmically distinct from both fixed penalties and adaptive normalisation layers.",
        "novelty_score": 8,
        "significance_reason": "Academic: The hypothesis provides a biologically grounded control-theoretic lens on Transformer optimisation, potentially opening a new line of work that unifies constrained optimisation (dual ascent), neuroscience (integral homeostatic scaling), and deep-learning practice. If validated, it would enrich our theoretical understanding of why balanced activations matter for gradient flow and generalisation. Societal/Practical: Hyper-parameter sweeps for large Transformers consume thousands of GPU-hours and substantial energy. A plug-and-play mechanism that eliminates regularisation tuning could cut both cost and carbon footprint while simultaneously delivering modest accuracy/perplexity gains. Because Transformers dominate NLP and are rapidly spreading to vision, audio and reinforcement learning, even single-digit-percent improvements and reduced tuning effort translate into large absolute resource savings.",
        "significance_score": 8
      }
    },
    {
      "hypothesis": {
        "open_problems": "1. LayerNorm is the single most-frequent operation inside modern Transformers (two calls per block). At training time it stabilises gradients, yet at inference it contributes 3–8 % of total FLOPs, extra memory reads/writes, and prevents efficient operator fusion.\n2. Existing LayerNorm-free recipes (Fixup, ScaleNorm, ReZero, etc.) achieve partial success but still demand hand-tuned initialisations, tailored learning-rate schedules, or fragile depth-dependent scalings that break when model size, data modality, or hardware changes.\n3. In biological cortex, long-term stability of firing rates is realised through integral homeostatic control: each neuron integrates the deviation between its recent activity and a target set-point and scales all incoming synapses multiplicatively. This mechanism is automatic, local, and does not rely on any external “normalisation layer.”\n4. The open question: can we replace LayerNorm entirely with an online, biologically inspired integral controller that (a) keeps every Transformer block in its optimal activation range, (b) requires **zero** manual re-tuning across depths, widths and tasks, and (c) reduces both training instability and inference cost?",
        "method": "LayerNorm-Free Integral Homeostatic Control (LIHC)\n--------------------------------------------------\nLet h_l ∈ R^{B×T×d} be the pre-activation tensor that would normally be fed into a LayerNorm inside block l.\n1. Per-block statistic   A_l = mean(|h_l|)  (mean over batch, sequence, features).\n2. Persistent *homeostatic drive* λ_l  (scalar, one per block) initialised to 0 and **not** updated by back-prop.\n3. Replace LayerNorm with a simple multiplicative gate  g_l  applied to the block output:\n      ĥ_l = g_l · h_l , with   g_l = exp(−λ_l).\n   (Exponent ensures positivity and keeps gradients well-behaved.)\n4. Extra loss term (dual objective)\n      L_homeo = Σ_l λ_l · (A_l − τ)  (τ≈1.0 fixed).\n5. After every optimiser step perform **integral update** (gradient-free):\n      λ_l ← λ_l + η · (A_l.detach() − τ)  (η≈10⁻² shared by all blocks).\n   This is dual-ascent on the constrained problem “∀l: A_l = τ.”\n6. No LayerNorm, no special weight initialisation, and only two global hyper-parameters (τ, η) that stay constant across models and datasets.\n7. Optional per-head variant: maintain λ_{l,h} to equalise attention heads when desired (swap Σ_l for Σ_{l,h}).",
        "experimental_setup": "1. Language Modelling: WikiText-2 (small) + OpenWebText (large).   Models: 2-layer Tiny-Transformer (1 M params) and GPT-Small (124 M).\n2. Vision: CIFAR-10 + ImageNet-1k.   Models: ViT-Tiny (12 layers) and ViT-Base (86 M).\n3. Speech: LibriSpeech 100 h with Conformer encoder.   Model: Conformer-Small (46 M).\nConditions:\n  a) Baseline: Standard LayerNorm Transformer.\n  b) Fixup initialisation (no norms).\n  c) LIHC (ours, no norms, τ=1, η=0.01).\nAll other hyper-parameters use community defaults; **no extra tuning** for LIHC.\nMetrics:\n  • Task performance – perplexity, top-1 accuracy, word-error-rate.\n  • Training stability – gradient-norm variance, NaN incidence.\n  • Inference cost – FLOPs, latency, SRAM bandwidth measured on NVIDIA A100.\n  • Carbon footprint – Watt-hours per training run and per 1 M inference tokens.\n  • Hyper-parameter search cost – total GPU-hours spent tuning.",
        "primary_metric": "Task-specific metric (perplexity / accuracy / WER) under identical training budgets, plus percentage reduction in inference FLOPs relative to LayerNorm baseline.",
        "experimental_code": "# PyTorch pseudo-code for one Transformer block without LayerNorm\nclass LIHCBlock(nn.Module):\n    def __init__(self, ... , tau=1.0, eta=1e-2):\n        super().__init__()\n        self.attn = MultiHeadAttention(...)\n        self.ff   = FeedForward(...)\n        self.tau, self.eta = tau, eta\n        self.register_buffer('lambda_h', torch.zeros(1))  # persistent\n    def forward(self, x):\n        # usual residual path but NO LayerNorm\n        h = x + self.attn(x)\n        h = h + self.ff(h)\n        A = h.detach().abs().mean()\n        g = torch.exp(-self.lambda_h)         # gating factor\n        h = g * h                             # apply homeostatic scaling\n        # cache A for later lambda update\n        self.cached_A = A\n        return h\n    @torch.no_grad()\n    def update_lambda(self):\n        self.lambda_h += self.eta * (self.cached_A - self.tau)\n\n# after each optimiser step\nfor blk in model.blocks:\n    blk.update_lambda()",
        "expected_result": "WikiText-2 Tiny-Transformer:\n  • Baseline (LayerNorm)   perplexity ≈ 72\n  • Fixup                  perplexity ≈ 75 (diverges occasionally)\n  • LIHC                   perplexity ≈ 66 (−8 %, stable)\nGPT-Small on OpenWebText (300 K steps):\n  • Baseline ppl 34.5 ; LIHC ppl 33.1 ; training 7 % faster (no LN ops).\nViT-Base on ImageNet-1k (300 epochs):\n  • Baseline top-1 83.0 %; Fixup 81.7 %; LIHC 83.4 %.\nInference: removing LayerNorm saves ≈6 % FLOPs and 9 % SRAM bandwidth; end-to-end latency −5 % on A100.\nAcross all runs LIHC required zero hyper-parameter trials beyond the default τ,η.",
        "expected_conclusion": "A biologically inspired integral homeostatic controller can completely substitute LayerNorm in Transformers, yielding equal or better accuracy, smoother optimisation, and measurable reductions in both training and inference cost. By eliminating the need for norm layers and hyper-parameter sweeps, LIHC lowers the carbon footprint of large-scale model development and daily inference, illustrating that neuroscience principles of firing-rate homeostasis can translate into concrete efficiency gains for mainstream machine-learning systems."
      },
      "evaluation": {
        "novelty_reason": "Replacing LayerNorm with a biologically-inspired, gradient-free integral controller is not present in prior Transformer literature. Existing “norm-free” approaches (Fixup, ReZero, DeepNorm, ScaleNorm, μParam, etc.) rely on static initialisation rules or depth-dependent learnt scalars that stay frozen during a forward pass; none maintain a persistent state updated after every optimisation step to enforce a homeostatic set-point. Likewise, neuroscience-inspired synaptic-scaling work has been limited to small recurrent nets and has not been applied to the very high-dimensional, residual-heavy Transformer architecture. LIHC’s use of (i) a single scalar λ per block updated by dual-ascent, (ii) removal of all feature-wise normalisation statistics, and (iii) independence from model depth/width constitutes a new algorithmic family that cannot be reproduced by re-combining existing methods.",
        "novelty_score": 8,
        "significance_reason": "LayerNorm accounts for 3–8 % of inference FLOPs and memory traffic in production-scale Transformers. Eliminating it while matching or slightly surpassing accuracy directly lowers latency, power, and cost for language, vision, and speech models that serve billions of requests daily. Academically, the work tightens the link between control-theoretic formulations of biological homeostasis and optimisation stability in deep networks, opening a new line of inquiry into continuous, local learning rules inside large models. If validated, LIHC would become a drop-in replacement that simplifies hyper-parameter tuning (no depth-specific scalings) and reduces training crashes, benefiting both researchers and industry practitioners and contributing to greener AI.",
        "significance_score": 8
      }
    },
    {
      "hypothesis": {
        "open_problems": "1. LayerNorm is indispensable for stable optimisation of deep Transformers but costs 3–8 % of inference FLOPs and hampers kernel fusion.\n2. Prior ‘norm-free’ techniques (Fixup, ReZero, μParam, DeepNorm, ScaleNorm, LIHC) either demand hand-tuned depth-dependent constants or retain a per-token multiplicative gate that still executes at inference time.\n3. In cortex, firing-rate homeostasis is realised not by a runtime gate but by slowly scaling **synaptic efficacies** (weights) through integral feedback, so that the neuron’s operating point is restored with *zero* per-spike overhead.\n4. The open question: Can we translate integral synaptic scaling into Transformer training so that (a) LayerNorm is removed, (b) all extra computations vanish after training, and (c) no manual re-tuning is required across depths, widths, tasks or hardware?",
        "method": "Fused Integral Synaptic Normalisation (FISN)\n------------------------------------------------\nNotation: for block l let h_l be its post-residual activations and W_l^k each parameter matrix inside the block (Q,K,V,FFN-up,FFN-down).\n\n1. Statistic   A_l = mean(|h_l|)   (averaged over batch, tokens, features).\n2. Persistent dual variable (homeostatic drive)   λ_l   initialised to 0; **not** updated by back-prop.\n3. Dual loss term added to the task loss:\n     L_homeo = Σ_l λ_l · (A_l − τ)              (τ≈1.0 fixed).\n4. After every optimiser step perform two gradient-free updates:\n   a) Integral ascent on λ_l\n        λ_l ← λ_l + η (A_l.detach() − τ)        (η≈10⁻²).\n   b) Weight fusion (compiles the gate away)\n        for every matrix W_l^k:   W_l^k ← e^{−η_w λ_l} · W_l^k     (η_w=1 by default).\n   Because all incoming weights are rescaled, the next forward pass automatically applies the corrective gain; no runtime multiply, no extra tensor.\n5. Optional hierarchical variant: keep λ_{l,h} per attention head; rescale each head’s Q,K,V separately.\n6. No LayerNorm, no special initialisation, two global hyper-parameters (τ,η) that remain constant for any model or modality.",
        "experimental_setup": "Benchmarks\n1. Language: WikiText-103 (medium) & The Pile (large) with 6-layer (44 M) and 24-layer (350 M) GPT-style models.\n2. Vision: CIFAR-100 & ImageNet-1k with ViT-Small (22 M) and ViT-Base (86 M).\n3. Speech: LibriSpeech 100 h with Conformer-Small (46 M).\n\nConditions\nA) LayerNorm baseline.\nB) LIHC (activation gate, stays in graph).\nC) FISN (ours, weights fused, no runtime gate).\nD) Fixup (no norms, tuned as in original paper).\n\nTraining uses identical optimiser schedules; FISN employs fixed τ=1, η=0.01, η_w=1 everywhere.  No other hyper-parameter search is allowed for FISN.\n\nMeasurements\n• Task metrics: perplexity / top-1 accuracy / word-error-rate.\n• Training stability: gradient-norm variance, NaN count.\n• Inference efficiency on NVIDIA A100: total FLOPs, SRAM bandwidth, end-to-end latency for 128-token batch.\n• Carbon footprint: watt-hours per training run + per 1 M inference tokens.\n• Memory footprint of inference graph.",
        "primary_metric": "Task performance under equal training budgets plus percentage reduction in inference FLOPs/latency relative to LayerNorm baseline.",
        "experimental_code": "# simplified PyTorch sketch for one Transformer block without LayerNorm\nclass FISNBlock(nn.Module):\n    def __init__(self, dim, heads, tau=1., eta=1e-2):\n        super().__init__()\n        self.attn  = MultiHeadAttention(dim, heads)\n        self.ff    = FeedForward(dim)\n        self.tau, self.eta = tau, eta\n        self.register_buffer('lambda_h', torch.zeros(1))\n    def forward(self, x):\n        h = x + self.attn(x)\n        h = h + self.ff(h)\n        self.A = h.detach().abs().mean()  # cached for update\n        return h                           # no runtime gate\n    @torch.no_grad()\n    def fuse_weights(self):\n        # integral dual-ascent\n        self.lambda_h += self.eta * (self.A - self.tau)\n        g = torch.exp(-self.lambda_h)      # positive scale\n        # fuse into all parameter matrices of the block\n        for mod in [self.attn.q_proj, self.attn.k_proj, self.attn.v_proj,\n                    self.attn.o_proj, self.ff.w1, self.ff.w2]:\n            mod.weight.mul_(g)",
        "expected_result": "WikiText-103, 6-layer GPT (44 M):\n LayerNorm   ppl 32.8   (baseline)\n Fixup       ppl 34.3   (diverges 2/5 runs)\n LIHC        ppl 32.2   (-1.8 %)\n FISN        ppl 31.6   (-3.7 %), training 6 % faster than LN.\n\nImageNet ViT-Base (86 M, 300 ep):\n LayerNorm   top-1 83.0 %\n LIHC        83.4 %\n FISN        83.6 %, identical hyper-params.\n\nInference on A100, ViT-Base (224×224):\n LayerNorm   17.4 GFLOPs, 5.8 ms latency\n LIHC        16.4 GFLOPs, 5.5 ms (gate multiply remains)\n FISN        16.2 GFLOPs, 5.4 ms (no gate, no LN)  →  -7 % FLOPs, ‑7 % latency.\n\nAcross all experiments FISN required zero hyper-parameter sweeps and showed 20-25 % lower gradient-norm variance than the baseline.",
        "expected_conclusion": "By migrating biological integral homeostatic control from the activation domain to the weight domain, FISN removes the need for both LayerNorm *and* any residual runtime gate, yielding a truly zero-overhead inference path.  The single feedback variable per block self-regulates activation magnitude across depths, obviating manual tuning and improving convergence.  Empirically FISN matches or surpasses LayerNorm accuracy while cutting inference FLOPs and latency by ~7 %, thereby lowering operational cost and carbon footprint.  The results demonstrate that synaptic-scaling principles from neuroscience can be compiled into weight matrices of modern Transformers, offering a new efficiency frontier for large-scale ML systems."
      },
      "evaluation": {
        "novelty_reason": "The proposal introduces a control-theoretic, biologically inspired mechanism—integral synaptic scaling—implemented as an auxiliary dual variable that is (1) updated outside back-prop by gradient-free integral ascent and (2) immediately fused into the block’s weights so that no multiplicative gate or learned scale remains in the inference graph.  Prior “norm-free” or “light-norm” works (Fixup, ReZero, µParam, DeepNorm, ScaleNorm, LIHC) either depend on depth-specific hand-tuned constants, keep a per-token scale parameter that is still evaluated at inference, or both; none remove online normalisation while also eliminating all runtime arithmetic.  Existing biological-inspired studies mainly mimic neuronal firing dynamics or Hebbian plasticity, not slow homeostatic weight scaling translated into Transformer optimisation.  The explicit dual-ascent formulation that treats activation magnitude as a constraint and compiles the Lagrange multiplier into weights appears unprecedented in Transformer literature, giving the hypothesis clear novelty.",
        "novelty_score": 8,
        "significance_reason": "LayerNorm accounts for 3–8 % of inference FLOPs and impedes kernel fusion in production systems; eliminating it without sacrificing accuracy directly lowers latency, energy use, and carbon footprint for the rapidly growing volume of Transformer inference.  If the same two global hyper-parameters work across depths, widths, and modalities, practitioners can save extensive tuning time and cost.  Academically, the work offers a fresh bridge between neuroscience (homeostatic synaptic scaling) and deep learning optimisation, potentially inspiring further biologically grounded control schemes.  Societally, a 7 % efficiency gain at the scale of large-language-model deployment translates into substantial energy savings and CO₂ reduction.  Hence the hypothesis bears high practical and scientific importance.",
        "significance_score": 8
      }
    },
    {
      "hypothesis": {
        "open_problems": "1. Removing LayerNorm with weight-fusion homeostatic control (FISN) eliminates runtime overhead but still requires (a) computing full-batch activation statistics every step and (b) leaves the over-parameterised weights that were continually down-scaled, wasting FLOPs and energy at inference.\n2. Existing pruning pipelines (magnitude or movement pruning) need an extra fine-tuning stage and hyper-parameter sweeps; they are orthogonal to normalisation methods.\n3. Biology solves both stability and metabolic efficiency jointly: after long-term depression, persistently weak synapses are eliminated (synaptic pruning) while the remaining ones are re-scaled to keep firing rates homeostatic.\n4. Open question: can we design a single integral-control rule that (i) stabilises Transformer training without LayerNorm, (ii) identifies chronically under-utilised weights on the fly, and (iii) compiles both normalisation and sparsity into the final static graph—without any extra retraining or tuning?",
        "method": "Homeostatic Integral Scale-AND-Prune (HISP)\n------------------------------------------------\nNotation: For block l let h_l be its post-residual activations, and W_l^k each weight matrix (Q, K, V, FFN-up, FFN-down).\n\n1. Streaming statistic  A_l  = EMA_β(|h_l|)   (|·| averaged over 128 randomly sampled tokens); β≈0.95.\n2. Persistent dual variable  λ_l  (one scalar per block) initialised to 0, **not** in back-prop.\n3. Dual loss added to task loss:   L_homeo = Σ_l λ_l·(A_l−τ)   with τ≈1.\n4. After every optimiser step perform two gradient-free updates:\n   a) Integral ascent   λ_l ← λ_l + η(A_l.detach()−τ)       (η≈10⁻²)\n   b) Weight fusion    W_l^k ← e^{−η_w λ_l} · W_l^k        (η_w=1)\n5. Online pruning flag: maintain an EMA of λ̂_l = EMA_0.99(λ_l).  If λ̂_l > θ (θ≈0.8) this block has been down-scaled for many iterations; mask (set to zero) all weights in W_l^k whose absolute value < γ·median(|W_l^k|) with γ=0.25.  Masking is permanent and executed immediately after step 4b; pruned weights no longer receive gradients.\n6. Optional fine granularity: apply steps 1–5 per attention head (λ_{l,h}) to yield head-wise sparsity.\n7. Hyper-parameters (τ,η,β,θ,γ) are global constants reused for every depth, width, modality.",
        "experimental_setup": "Benchmarks & models (same as FISN to ensure comparability)\n• Language: WikiText-103 (44 M, 350 M GPT).\n• Vision: ImageNet-1k with ViT-Small/ViT-Base.\n• Speech: LibriSpeech 100 h with Conformer-Small.\n\nConditions\nA) LayerNorm baseline.\nB) FISN (norm-free, dense).\nC) HISP (ours, norm-free + online pruning).\n\nAll methods share identical optimiser, schedule, and training budget; HISP uses fixed hyper-parameters listed above.  No separate pruning or fine-tuning phase.\n\nMeasurements\n• Task metric (ppl/accuracy/WER).\n• Training stability (grad-norm variance, NaNs).\n• Final sparsity (% non-zero weights) and structured sparsity (% pruned heads).\n• Inference efficiency on A100: GFLOPs, latency, energy.\n• Total GPU-hours including any pruning or tuning (should be equal across B & C).",
        "primary_metric": "Task performance under equal budget plus percentage reduction in inference GFLOPs after pruning.",
        "experimental_code": "# simplified PyTorch core of HISP for one block\nclass HISPBlock(nn.Module):\n    def __init__(self, dim, heads, tau=1., eta=1e-2, beta=0.95,\n                 prune_th=0.8, prune_gamma=0.25):\n        super().__init__()\n        self.attn = MultiHeadAttention(dim, heads)\n        self.ff   = FeedForward(dim)\n        self.tau, self.eta, self.beta = tau, eta, beta\n        self.pth, self.pgamma = prune_th, prune_gamma\n        self.register_buffer('lambda_h', torch.zeros(1))\n        self.register_buffer('lambda_ema', torch.zeros(1))\n        self.register_buffer('A', torch.zeros(1))\n    def forward(self, x):\n        h = x + self.attn(x)\n        h = h + self.ff(h)\n        # update streaming statistic with 128 random tokens\n        sample = h.flatten(0,1)      # (B*T, d)\n        idx = torch.randint(0, sample.size(0), (128,), device=h.device)\n        a_now = sample[idx].abs().mean()\n        self.A.mul_(self.beta).add_(a_now*(1-self.beta))\n        return h     # scaling already fused into weights\n    @torch.no_grad()\n    def fuse_and_prune(self):\n        # integral ascent and weight fusion\n        self.lambda_h += self.eta * (self.A - self.tau)\n        g = torch.exp(-self.lambda_h)\n        for mod in [self.attn.q_proj, self.attn.k_proj, self.attn.v_proj,\n                    self.attn.o_proj, self.ff.w1, self.ff.w2]:\n            mod.weight.mul_(g)\n        # update long-term EMA of lambda\n        self.lambda_ema.mul_(0.99).add_(0.01*self.lambda_h)\n        # automatic pruning if chronically over-active\n        if self.lambda_ema > self.pth:\n            for mod in [self.attn.q_proj, self.attn.k_proj, self.attn.v_proj,\n                        self.attn.o_proj, self.ff.w1, self.ff.w2]:\n                thr = self.pgamma * mod.weight.abs().median()\n                mask = mod.weight.abs() < thr\n                mod.weight[mask] = 0.\n                mod.weight.requires_grad = False",
        "expected_result": "ImageNet ViT-Base (86 M):\n LayerNorm   83.0 % top-1, 17.4 GFLOPs\n FISN        83.6 %, 16.2 GFLOPs (-7 %)\n HISP        83.5 %, 12.8 GFLOPs (-26 % vs LN, ‑21 % vs FISN); 25 % of attention heads pruned, overall 32 % weight sparsity.\n\nWikiText-103 GPT-350 M:\n LayerNorm   ppl 29.2, 69 GFLOPs/128 tok\n FISN        ppl 28.7, 64 GFLOPs\n HISP        ppl 28.9, 48 GFLOPs (-30 %), identical training stability\n\nAcross tasks HISP keeps accuracy within ±0.2 pp or perplexity within +0.3 relative to FISN while delivering 20–30 % additional inference savings.  No re-training or hyper-parameter search required.",
        "expected_conclusion": "A single biologically inspired integral controller can simultaneously normalise activations and conduct synaptic pruning inside Transformers.  By fusing its actions into weights during training, HISP removes LayerNorm, avoids runtime gates, and automatically excises redundant parameters, yielding up to 30 % extra FLOP and energy savings over prior norm-free methods with negligible accuracy loss.  The study shows that cortical principles of homeostatic scaling and structural plasticity translate into practical, self-contained algorithms for greener and faster large-scale AI systems."
      },
      "evaluation": {
        "novelty_reason": "The proposal unifies two research threads that have so far been treated separately: (1) norm-free / scale-equilibrated Transformer training (e.g., FISN, FixNorm, ScaleNorm) and (2) weight/structural pruning (magnitude, movement, SNIP-it, GMP), which is usually applied after pre-training with an additional fine-tuning budget. HISP is novel in that a single biologically-motivated integral controller simultaneously (a) keeps activations in a target range, (b) fuses the normalisation gain directly into the weights so that no run-time operation is left, and (c) drives an online, once-for-all pruning rule that requires zero extra optimisation steps or hyper-parameter sweeps. No existing paper combines homeostatic scaling with on-the-fly, inference-time-static sparsification inside the same optimisation loop for Transformers. Furthermore, the use of a scalar dual variable per block/head that is excluded from back-prop and updated with gradient-free integrator ascent is uncommon in deep-learning practice and directly mirrors cortical synaptic scaling models, adding conceptual novelty.",
        "novelty_score": 8,
        "significance_reason": "If validated, HISP would permit practitioners to train norm-free, sparsity-aware Transformers that are 20–30 % cheaper at inference without any extra fine-tuning cost, immediately lowering energy consumption and enabling deployment on resource-constrained hardware. Academically, it bridges neuroscience (homeostatic plasticity and synaptic pruning) with optimisation theory (integral control, dual ascent) and deep-learning engineering (normalisation and pruning), providing a concrete demonstration that biologically plausible control rules can out-perform hand-engineered pipelines. Because compute and energy are the dominant bottlenecks for scaling large models, a method that jointly improves training stability and inference efficiency while simplifying the pipeline has high relevance to both researchers and industry. The ability to generalise across language, vision and speech models with fixed hyper-parameters further increases its practical impact.",
        "significance_score": 9
      }
    },
    {
      "hypothesis": {
        "open_problems": "1. Current norm-free integral controllers such as FISN/HISP stabilise activations and fuse the scaling gain into the weights, but their optimisation objective is agnostic to the *energetic* cost of the model.  In consequence they may prune weights that contribute little to the loss yet leave expensive dense sub-tensors untouched, limiting real-world efficiency gains.\n2. Magnitude-based on-the-fly pruning produces unstructured sparsity that most inference accelerators cannot exploit.  Hardware-friendly patterns (head-, block-, or row-level sparsity) require separate structured-pruning passes and extra tuning.\n3. Biology addresses both problems through multi-timescale plasticity: fast homeostatic scaling keeps firing rates in range, while slow *structural plasticity* removes metabolically costly synapses until neural activity meets an energy budget.\n4. Open research question: can we endow Transformers with a *dual-objective* integral controller that simultaneously enforces (i) activation homeostasis *and* (ii) a target energy/FLOP budget, yielding hardware-aware structured sparsity with no post-hoc fine-tuning?",
        "method": "Multiscale Integral Plasticity for Energy-aware Transformers (MIPET)\n---------------------------------------------------------------------\nNotation: for block l let h_l be post-residual activations, W_l^k each weight matrix (Q,K,V,FFN-up,FFN-down), and C_l the theoretical dense FLOPs of the block.\n\nDual variables (no back-prop):\n  λ_l  – fast homeostatic drive (activation target τ≈1).\n  μ_l  – slow metabolic drive (energy target ρ, e.g. 50% of dense FLOPs).\n\nStep-wise procedure (executed every optimiser step):\n1. Streaming statistics (over 128 random tokens)\n      A_l  ←  EMA_β(|h_l|)                     # activation magnitude\n      F_l  ←  Σ_k nnz(W_l^k) / |W_l^k|         # current density per matrix\n      E_l  =  F_l · C_l                        # estimated FLOPs this block\n2. Dual loss added to task loss\n      L_dual = Σ_l [ λ_l·(A_l−τ)  +  μ_l·(E_l−ρ·C_l) ]\n3. Optimiser step on total loss.\n4. Gradient-free dual updates (β≈0.95, η_fast≈10⁻², η_slow≈10⁻³)\n      λ_l ← λ_l + η_fast · (A_l.detach()−τ)\n      μ_l ← μ_l + η_slow · (E_l.detach()−ρ·C_l)\n5. Weight fusion (as in FISN)\n      scale = exp(−λ_l)\n      for W_l^k:  W_l^k ← scale · W_l^k\n6. Structured pruning triggered by sustained positive μ̂_l = EMA_0.99(μ_l):\n      if μ̂_l > θ (θ≈0.5) then\n          a) prune entire attention heads whose total |W| < γ·median(|W|_heads)\n          b) prune FFN rows whose ℓ₂ norm < γ·median(‖row‖)\n      Masked structures are permanently zeroed and excluded from further gradient updates, instantly reducing E_l.\n7. Regrowth safety net: if μ̂_l becomes < −θ for 500 steps, randomly resurrect 10% of previously removed heads/rows to avoid capacity collapse (mirrors synaptic regrowth).  Regrown parameters are re-initialised with small Gaussian noise.\n\nGlobal hyper-parameters (τ,ρ,β,η_fast,η_slow,θ,γ) are fixed once and reused for all depths, widths, and modalities.",
        "experimental_setup": "Datasets & Models (all match prior FISN benchmarks for comparability)\n• Language: WikiText-103 (44 M GPT) and C4-subset (350 M GPT).\n• Vision: ImageNet-1k with ViT-Base (86 M).\n• Speech: LibriSpeech 100 h with Conformer-Small (46 M).\n\nConditions\nA) LayerNorm baseline.\nB) FISN (norm-free, dense).\nC) HISP (norm-free, unstructured sparsity).\nD) MIPET (ours, norm-free + structured sparsity + energy constraint).\n\nAll methods use identical optimiser and training budget; MIPET employs a *single* energy target ρ = 0.6 (i.e. aim for 40 % FLOP reduction) and default hyper-parameters above—no per-model tuning.\n\nMetrics\n• Task: perplexity / top-1 accuracy / WER.\n• Training stability: gradient-norm variance, NaNs.\n• Energy efficiency: actual inference GFLOPs, latency, and joules measured with NVIDIA Nsight on A100.\n• Sparsity pattern: % pruned heads, % pruned FFN rows, overall non-zero count.\n• Total GPU-hours including any pruning or tuning.",
        "primary_metric": "Task performance under equal training budget *and* percentage reduction in real measured inference energy (joules) relative to LayerNorm baseline.",
        "experimental_code": "# core PyTorch fragment for one Transformer block\nclass MIPETBlock(nn.Module):\n    def __init__(self, dim, heads, C_flops, tau=1., rho=0.6,\n                 eta_fast=1e-2, eta_slow=1e-3, beta=0.95,\n                 th=0.5, gamma=0.25):\n        super().__init__()\n        self.attn = MultiHeadAttention(dim, heads)\n        self.ff   = FeedForward(dim)\n        self.tau, self.rho = tau, rho\n        self.eta_f, self.eta_s = eta_fast, eta_slow\n        self.beta, self.th, self.gamma = beta, th, gamma\n        self.C = C_flops\n        self.register_buffer('lam', torch.zeros(1))\n        self.register_buffer('mu',  torch.zeros(1))\n        self.register_buffer('A',   torch.zeros(1))\n        self.register_buffer('E',   torch.tensor(self.C))\n    def forward(self, x):\n        h = x + self.attn(x)\n        h = h + self.ff(h)\n        # streaming activation statistic\n        sample = h.flatten(0,1)\n        idx = torch.randint(0, sample.size(0), (128,), device=h.device)\n        a_now = sample[idx].abs().mean()\n        self.A.mul_(self.beta).add_(a_now*(1-self.beta))\n        return h  # scaling already fused into weights\n    def dual_loss(self):\n        # density estimate\n        nz = 0\n        tot = 0\n        for mod in [self.attn.q_proj, self.attn.k_proj, self.attn.v_proj,\n                    self.attn.o_proj, self.ff.w1, self.ff.w2]:\n            nz += mod.weight.count_nonzero()\n            tot += mod.weight.numel()\n        density = nz / tot\n        self.E = density * self.C\n        return self.lam*(self.A - self.tau) + self.mu*(self.E - self.rho*self.C)\n    @torch.no_grad()\n    def fuse_prune_update(self):\n        # integral updates\n        self.lam += self.eta_f * (self.A - self.tau)\n        self.mu  += self.eta_s * (self.E - self.rho*self.C)\n        # fuse scaling into weights\n        g = torch.exp(-self.lam)\n        for mod in [self.attn.q_proj, self.attn.k_proj, self.attn.v_proj,\n                    self.attn.o_proj, self.ff.w1, self.ff.w2]:\n            mod.weight.mul_(g)\n        # structured pruning when sustained high mu\n        if self.mu > self.th:\n            # prune complete attention heads\n            head_scores = self.attn.q_proj.weight.view(self.attn.h, -1).abs().mean(dim=1)\n            head_mask = head_scores < self.gamma * head_scores.median()\n            for name in ['q_proj','k_proj','v_proj','o_proj']:\n                w = getattr(self.attn, name).weight.view(self.attn.h, -1)\n                w[head_mask] = 0.\n                getattr(self.attn, name).weight.requires_grad = False\n            # prune FFN rows\n            row_scores = self.ff.w1.weight.norm(dim=1)\n            row_mask = row_scores < self.gamma * row_scores.median()\n            self.ff.w1.weight[row_mask] = 0.\n            self.ff.w2.weight[:,row_mask] = 0.\n            self.ff.w1.weight.requires_grad = False\n            self.ff.w2.weight.requires_grad = False",
        "expected_result": "ImageNet ViT-Base:\n LayerNorm   83.0 % top-1, 17.4 GFLOPs, 30 J / image batch\n FISN        83.6 %, 16.2 GFLOPs, 28 J\n HISP        83.5 %, 12.8 GFLOPs, 23 J\n MIPET       83.4 %, 10.1 GFLOPs (-42 % vs LN), 18 J (-40 %) ; 38 % heads and 45 % FFN rows pruned.\n\nWikiText-103 GPT-350 M:\n LayerNorm   ppl 29.2, 69 GFLOPs/128 tok, 26 J\n FISN        ppl 28.7, 64 GFLOPs, 24 J\n MIPET       ppl 29.0, 41 GFLOPs (-41 %), 16 J (-38 %) ; training equally stable (no extra NaNs).\n\nAcross all tasks MIPET hits the predefined energy budget (ρ=0.6) within ±3 %, retains accuracy within ±0.2 pp or +0.3 ppl of dense norm-free baselines, and needs no additional fine-tuning.",
        "expected_conclusion": "A two-timescale, biologically inspired integral controller can turn Transformers into self-optimising systems that respect both neuronal (activation) and metabolic (energy) homeostasis.  By jointly regulating activation magnitude and FLOP budget, MIPET removes LayerNorm, fuses gains into weights, and yields hardware-friendly structured sparsity without any post-training phase or hyper-parameter search.  Empirical results on language, vision, and speech show up to 40 % real energy savings at equal accuracy, demonstrating a practical path toward greener large-scale AI driven by principles of cortical plasticity."
      },
      "evaluation": {
        "novelty_reason": "The proposal unifies two research lines that have so far remained separate: (i) norm-free integral controllers (e.g. FISN, HISP) that stabilise activations and (ii) structured-sparsity or lottery-ticket style pruning/regrowth methods that target efficiency.  No prior work endows a transformer with a dual set of biologically-motivated fast/slow Lagrange multipliers that (a) keeps activations in a homeostatic range and (b) simultaneously meters the block-level FLOP budget during training.  The slow μ_l variable drives head- and row-level pruning on-the-fly, eliminating the need for an extra pruning-and-fine-tune stage that all existing structured-sparsity papers (e.g. movement-pruning, head-pruning, block-pruning, RigL) still require.  Moreover, the controller is gradient-free and fuses the gain into weights at every step, a mechanism borrowed from cortical structural plasticity that is absent from previous ML sparsifiers.  Finally, using a *single global energy hyper-parameter* that transfers across modalities is new; existing methods either hand-tune sparsity ratios per model or ignore hardware-realistic energy metrics altogether.",
        "novelty_score": 8,
        "significance_reason": "Academically, the hypothesis offers a principled optimisation framework that treats energy/FLOPs as a first-class constraint and marries it with biologically plausible dual-timescale plasticity.  This could open a new family of norm-free, resource-aware training algorithms and stimulate cross-disciplinary work between neuroscience and efficient ML.  Practically, the expected 40 % reduction in real inference energy at equal accuracy and without post-hoc tuning directly addresses the mounting environmental and economic costs of deploying large transformer models.  Producing hardware-friendly structured sparsity in a single training run simplifies deployment on existing accelerators, increasing the likelihood of industry adoption and large-scale societal energy savings.  Because the method is model-agnostic and uses fixed hyper-parameters, it scales to diverse tasks, further amplifying impact.",
        "significance_score": 8
      }
    }
  ],
  "research_session": {
    "hypothesis": {
      "open_problems": "1. Current norm-free integral controllers such as FISN/HISP stabilise activations and fuse the scaling gain into the weights, but their optimisation objective is agnostic to the *energetic* cost of the model.  In consequence they may prune weights that contribute little to the loss yet leave expensive dense sub-tensors untouched, limiting real-world efficiency gains.\n2. Magnitude-based on-the-fly pruning produces unstructured sparsity that most inference accelerators cannot exploit.  Hardware-friendly patterns (head-, block-, or row-level sparsity) require separate structured-pruning passes and extra tuning.\n3. Biology addresses both problems through multi-timescale plasticity: fast homeostatic scaling keeps firing rates in range, while slow *structural plasticity* removes metabolically costly synapses until neural activity meets an energy budget.\n4. Open research question: can we endow Transformers with a *dual-objective* integral controller that simultaneously enforces (i) activation homeostasis *and* (ii) a target energy/FLOP budget, yielding hardware-aware structured sparsity with no post-hoc fine-tuning?",
      "method": "Multiscale Integral Plasticity for Energy-aware Transformers (MIPET)\n---------------------------------------------------------------------\nNotation: for block l let h_l be post-residual activations, W_l^k each weight matrix (Q,K,V,FFN-up,FFN-down), and C_l the theoretical dense FLOPs of the block.\n\nDual variables (no back-prop):\n  λ_l  – fast homeostatic drive (activation target τ≈1).\n  μ_l  – slow metabolic drive (energy target ρ, e.g. 50% of dense FLOPs).\n\nStep-wise procedure (executed every optimiser step):\n1. Streaming statistics (over 128 random tokens)\n      A_l  ←  EMA_β(|h_l|)                     # activation magnitude\n      F_l  ←  Σ_k nnz(W_l^k) / |W_l^k|         # current density per matrix\n      E_l  =  F_l · C_l                        # estimated FLOPs this block\n2. Dual loss added to task loss\n      L_dual = Σ_l [ λ_l·(A_l−τ)  +  μ_l·(E_l−ρ·C_l) ]\n3. Optimiser step on total loss.\n4. Gradient-free dual updates (β≈0.95, η_fast≈10⁻², η_slow≈10⁻³)\n      λ_l ← λ_l + η_fast · (A_l.detach()−τ)\n      μ_l ← μ_l + η_slow · (E_l.detach()−ρ·C_l)\n5. Weight fusion (as in FISN)\n      scale = exp(−λ_l)\n      for W_l^k:  W_l^k ← scale · W_l^k\n6. Structured pruning triggered by sustained positive μ̂_l = EMA_0.99(μ_l):\n      if μ̂_l > θ (θ≈0.5) then\n          a) prune entire attention heads whose total |W| < γ·median(|W|_heads)\n          b) prune FFN rows whose ℓ₂ norm < γ·median(‖row‖)\n      Masked structures are permanently zeroed and excluded from further gradient updates, instantly reducing E_l.\n7. Regrowth safety net: if μ̂_l becomes < −θ for 500 steps, randomly resurrect 10% of previously removed heads/rows to avoid capacity collapse (mirrors synaptic regrowth).  Regrown parameters are re-initialised with small Gaussian noise.\n\nGlobal hyper-parameters (τ,ρ,β,η_fast,η_slow,θ,γ) are fixed once and reused for all depths, widths, and modalities.",
      "experimental_setup": "Datasets & Models (all match prior FISN benchmarks for comparability)\n• Language: WikiText-103 (44 M GPT) and C4-subset (350 M GPT).\n• Vision: ImageNet-1k with ViT-Base (86 M).\n• Speech: LibriSpeech 100 h with Conformer-Small (46 M).\n\nConditions\nA) LayerNorm baseline.\nB) FISN (norm-free, dense).\nC) HISP (norm-free, unstructured sparsity).\nD) MIPET (ours, norm-free + structured sparsity + energy constraint).\n\nAll methods use identical optimiser and training budget; MIPET employs a *single* energy target ρ = 0.6 (i.e. aim for 40 % FLOP reduction) and default hyper-parameters above—no per-model tuning.\n\nMetrics\n• Task: perplexity / top-1 accuracy / WER.\n• Training stability: gradient-norm variance, NaNs.\n• Energy efficiency: actual inference GFLOPs, latency, and joules measured with NVIDIA Nsight on A100.\n• Sparsity pattern: % pruned heads, % pruned FFN rows, overall non-zero count.\n• Total GPU-hours including any pruning or tuning.",
      "primary_metric": "Task performance under equal training budget *and* percentage reduction in real measured inference energy (joules) relative to LayerNorm baseline.",
      "experimental_code": "# core PyTorch fragment for one Transformer block\nclass MIPETBlock(nn.Module):\n    def __init__(self, dim, heads, C_flops, tau=1., rho=0.6,\n                 eta_fast=1e-2, eta_slow=1e-3, beta=0.95,\n                 th=0.5, gamma=0.25):\n        super().__init__()\n        self.attn = MultiHeadAttention(dim, heads)\n        self.ff   = FeedForward(dim)\n        self.tau, self.rho = tau, rho\n        self.eta_f, self.eta_s = eta_fast, eta_slow\n        self.beta, self.th, self.gamma = beta, th, gamma\n        self.C = C_flops\n        self.register_buffer('lam', torch.zeros(1))\n        self.register_buffer('mu',  torch.zeros(1))\n        self.register_buffer('A',   torch.zeros(1))\n        self.register_buffer('E',   torch.tensor(self.C))\n    def forward(self, x):\n        h = x + self.attn(x)\n        h = h + self.ff(h)\n        # streaming activation statistic\n        sample = h.flatten(0,1)\n        idx = torch.randint(0, sample.size(0), (128,), device=h.device)\n        a_now = sample[idx].abs().mean()\n        self.A.mul_(self.beta).add_(a_now*(1-self.beta))\n        return h  # scaling already fused into weights\n    def dual_loss(self):\n        # density estimate\n        nz = 0\n        tot = 0\n        for mod in [self.attn.q_proj, self.attn.k_proj, self.attn.v_proj,\n                    self.attn.o_proj, self.ff.w1, self.ff.w2]:\n            nz += mod.weight.count_nonzero()\n            tot += mod.weight.numel()\n        density = nz / tot\n        self.E = density * self.C\n        return self.lam*(self.A - self.tau) + self.mu*(self.E - self.rho*self.C)\n    @torch.no_grad()\n    def fuse_prune_update(self):\n        # integral updates\n        self.lam += self.eta_f * (self.A - self.tau)\n        self.mu  += self.eta_s * (self.E - self.rho*self.C)\n        # fuse scaling into weights\n        g = torch.exp(-self.lam)\n        for mod in [self.attn.q_proj, self.attn.k_proj, self.attn.v_proj,\n                    self.attn.o_proj, self.ff.w1, self.ff.w2]:\n            mod.weight.mul_(g)\n        # structured pruning when sustained high mu\n        if self.mu > self.th:\n            # prune complete attention heads\n            head_scores = self.attn.q_proj.weight.view(self.attn.h, -1).abs().mean(dim=1)\n            head_mask = head_scores < self.gamma * head_scores.median()\n            for name in ['q_proj','k_proj','v_proj','o_proj']:\n                w = getattr(self.attn, name).weight.view(self.attn.h, -1)\n                w[head_mask] = 0.\n                getattr(self.attn, name).weight.requires_grad = False\n            # prune FFN rows\n            row_scores = self.ff.w1.weight.norm(dim=1)\n            row_mask = row_scores < self.gamma * row_scores.median()\n            self.ff.w1.weight[row_mask] = 0.\n            self.ff.w2.weight[:,row_mask] = 0.\n            self.ff.w1.weight.requires_grad = False\n            self.ff.w2.weight.requires_grad = False",
      "expected_result": "ImageNet ViT-Base:\n LayerNorm   83.0 % top-1, 17.4 GFLOPs, 30 J / image batch\n FISN        83.6 %, 16.2 GFLOPs, 28 J\n HISP        83.5 %, 12.8 GFLOPs, 23 J\n MIPET       83.4 %, 10.1 GFLOPs (-42 % vs LN), 18 J (-40 %) ; 38 % heads and 45 % FFN rows pruned.\n\nWikiText-103 GPT-350 M:\n LayerNorm   ppl 29.2, 69 GFLOPs/128 tok, 26 J\n FISN        ppl 28.7, 64 GFLOPs, 24 J\n MIPET       ppl 29.0, 41 GFLOPs (-41 %), 16 J (-38 %) ; training equally stable (no extra NaNs).\n\nAcross all tasks MIPET hits the predefined energy budget (ρ=0.6) within ±3 %, retains accuracy within ±0.2 pp or +0.3 ppl of dense norm-free baselines, and needs no additional fine-tuning.",
      "expected_conclusion": "A two-timescale, biologically inspired integral controller can turn Transformers into self-optimising systems that respect both neuronal (activation) and metabolic (energy) homeostasis.  By jointly regulating activation magnitude and FLOP budget, MIPET removes LayerNorm, fuses gains into weights, and yields hardware-friendly structured sparsity without any post-training phase or hyper-parameter search.  Empirical results on language, vision, and speech show up to 40 % real energy savings at equal accuracy, demonstrating a practical path toward greener large-scale AI driven by principles of cortical plasticity."
    },
    "iterations": [
      {
        "iteration_id": 1,
        "method": "Multiscale Integral Plasticity for Energy-aware Transformers (MIPET)\n---------------------------------------------------------------------\nNotation: for block l let h_l be post-residual activations, W_l^k each weight matrix (Q,K,V,FFN-up,FFN-down), and C_l the theoretical dense FLOPs of the block.\n\nDual variables (no back-prop):\n  λ_l  – fast homeostatic drive (activation target τ≈1).\n  μ_l  – slow metabolic drive (energy target ρ, e.g. 50% of dense FLOPs).\n\nStep-wise procedure (executed every optimiser step):\n1. Streaming statistics (over 128 random tokens)\n      A_l  ←  EMA_β(|h_l|)                     # activation magnitude\n      F_l  ←  Σ_k nnz(W_l^k) / |W_l^k|         # current density per matrix\n      E_l  =  F_l · C_l                        # estimated FLOPs this block\n2. Dual loss added to task loss\n      L_dual = Σ_l [ λ_l·(A_l−τ)  +  μ_l·(E_l−ρ·C_l) ]\n3. Optimiser step on total loss.\n4. Gradient-free dual updates (β≈0.95, η_fast≈10⁻², η_slow≈10⁻³)\n      λ_l ← λ_l + η_fast · (A_l.detach()−τ)\n      μ_l ← μ_l + η_slow · (E_l.detach()−ρ·C_l)\n5. Weight fusion (as in FISN)\n      scale = exp(−λ_l)\n      for W_l^k:  W_l^k ← scale · W_l^k\n6. Structured pruning triggered by sustained positive μ̂_l = EMA_0.99(μ_l):\n      if μ̂_l > θ (θ≈0.5) then\n          a) prune entire attention heads whose total |W| < γ·median(|W|_heads)\n          b) prune FFN rows whose ℓ₂ norm < γ·median(‖row‖)\n      Masked structures are permanently zeroed and excluded from further gradient updates, instantly reducing E_l.\n7. Regrowth safety net: if μ̂_l becomes < −θ for 500 steps, randomly resurrect 10% of previously removed heads/rows to avoid capacity collapse (mirrors synaptic regrowth).  Regrown parameters are re-initialised with small Gaussian noise.\n\nGlobal hyper-parameters (τ,ρ,β,η_fast,η_slow,θ,γ) are fixed once and reused for all depths, widths, and modalities.",
        "experimental_design": {
          "experiment_summary": "This experiment evaluates the newly-proposed Multiscale Integral Plasticity for Energy-aware Transformers (MIPET) on an image-classification task.  A standard ViT-Base (86 M parameters) is trained from scratch on ImageNet-1K under four conditions: (A) conventional LayerNorm, (B) FISN (norm-free dense), (C) HISP (norm-free unstructured sparsity) and (D) our MIPET controller.  All configurations share the same training budget (300 epochs, AdamW, cosine LR decay, RandAugment, etc.).\n\nMIPET augments each Transformer block with two integral control signals: a fast homeostatic drive (λ) that keeps post-residual activation magnitude close to τ≈1, and a slow metabolic drive (μ) that keeps per-block FLOPs near a global target ρ·C (ρ=0.6).  At every optimiser step the block computes streaming activation and density statistics, adds a dual loss to the task loss, updates dual variables, fuses the λ gain directly into the weights, and—when μ stays positive—executes structured head- and row-level pruning.  A regrowth protocol reactivates 10 % of previously pruned structures if the energy budget is over-satisfied for 500 steps, preventing capacity collapse without manual tuning.\n\nWe measure task performance (top-1 accuracy), training stability, sparsity pattern, theoretical and real inference FLOPs, latency, and wall-power energy with NVIDIA Nsight on A100 GPUs.  The primary figure of merit is accuracy at equal training cost together with the percentage reduction in measured inference energy relative to the LayerNorm baseline.\n\nA small hyper-parameter sweep (learning-rate, weight-decay, pruning aggressiveness γ, and energy target ρ) is performed with random search (20 trials) on a held-out 50-class ImageNet subset.  The best setting is retrained on the full dataset for the final comparison.",
          "evaluation_metrics": [
            "Top-1 Accuracy",
            "Gradient-Norm Variance",
            "NaN Count",
            "Inference GFLOPs",
            "Inference Latency",
            "Inference Energy (Joules)",
            "Task performance under equal training budget and percentage reduction in real measured inference energy (joules) relative to LayerNorm baseline",
            "Percentage Pruned Heads",
            "Percentage Pruned FFN Rows",
            "Total Non-Zero Parameter Count",
            "GPU Training Hours",
            "Task performance under equal training budget *and* percentage reduction in real measured inference energy (joules) relative to LayerNorm baseline."
          ],
          "proposed_method": "Multiscale Integral Plasticity for Energy-aware Transformers (MIPET)\n1. Every Transformer block l maintains two scalar dual variables: λ_l (fast) and μ_l (slow).\n2. Streaming statistics are updated over 128 randomly sampled activations per forward pass:\n   • A_l ← EMA_β(|h_l|) (β=0.95)\n   • F_l ← Σ_k nnz(W_l^k)/|W_l^k| (current weight density)\n   • E_l = F_l·C_l (estimated FLOPs)\n3. A dual loss L_dual = Σ_l [λ_l(A_l−τ)+μ_l(E_l−ρC_l)] is added to the task loss and back-propagated; λ_l and μ_l themselves do NOT receive gradients.\n4. After the optimiser step, λ and μ are updated by integral control:\n      λ_l ← λ_l + η_fast(A_l−τ)   (η_fast=1e-2)\n      μ_l ← μ_l + η_slow(E_l−ρC_l) (η_slow=1e-3)\n5. All block weights are rescaled in-place by exp(−λ_l), fusing the homeostatic gain (no LayerNorm required).\n6. If the long-term average μ̂_l (EMA 0.99) exceeds θ=0.5, the block prunes:\n      a. Whole attention heads with total |W| below γ·median(|W|_heads) (γ=0.25).\n      b. FFN rows whose ℓ₂ norm is below γ·median(row norms).\n   Masks are permanent and weights are excluded from further gradient updates.\n7. If μ̂_l ≤ −θ for 500 consecutive steps, 10 % of previously pruned heads/rows are randomly re-initialised (Gaussian σ=0.02) and unlocked, providing capacity regrowth.\n8. All global hyper-parameters (τ,ρ,β,η_fast,η_slow,θ,γ) are fixed once and reused across datasets and model sizes, demonstrating robustness and eliminating per-model tuning.",
          "comparative_methods": [
            "FISN (Feature-wise linear Integral SN)"
          ],
          "models_to_use": [
            "ViT-Base (86 M)"
          ],
          "datasets_to_use": [
            "ImageNet-1K"
          ],
          "hyperparameters_to_search": {
            "learning_rate": "1e-4-5e-4",
            "weight_decay": "0.02-0.2",
            "gamma_prune": "0.15-0.35",
            "rho_energy_target": "0.5-0.7"
          },
          "external_resources": {
            "hugging_face": {
              "models": [
                {
                  "id": "google/vit-base-patch16-224",
                  "author": "google",
                  "sha": "3f49326eb077187dfe1c2a2bb15fbd74e6ab91e3",
                  "created_at": "2022-03-02T23:29:05+00:00",
                  "last_modified": "2023-09-05T15:27:12+00:00",
                  "private": false,
                  "gated": false,
                  "disabled": false,
                  "downloads": 3104863,
                  "likes": 891,
                  "siblings": [
                    {
                      "rfilename": ".gitattributes"
                    },
                    {
                      "rfilename": "README.md"
                    },
                    {
                      "rfilename": "config.json"
                    },
                    {
                      "rfilename": "flax_model.msgpack"
                    },
                    {
                      "rfilename": "model.safetensors"
                    },
                    {
                      "rfilename": "preprocessor_config.json"
                    },
                    {
                      "rfilename": "pytorch_model.bin"
                    },
                    {
                      "rfilename": "tf_model.h5"
                    }
                  ],
                  "card_data": {
                    "license": "apache-2.0",
                    "language": [],
                    "tags": [
                      "vision",
                      "image-classification"
                    ],
                    "datasets": [
                      "imagenet-1k",
                      "imagenet-21k"
                    ],
                    "task_categories": [],
                    "size_categories": [],
                    "metrics": [],
                    "widget": [
                      {
                        "src": "https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg",
                        "example_title": "Tiger"
                      },
                      {
                        "src": "https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg",
                        "example_title": "Teapot"
                      },
                      {
                        "src": "https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg",
                        "example_title": "Palace"
                      }
                    ]
                  },
                  "tags": [
                    "transformers",
                    "pytorch",
                    "tf",
                    "jax",
                    "safetensors",
                    "vit",
                    "image-classification",
                    "vision",
                    "dataset:imagenet-1k",
                    "dataset:imagenet-21k",
                    "arxiv:2010.11929",
                    "arxiv:2006.03677",
                    "license:apache-2.0",
                    "autotrain_compatible",
                    "endpoints_compatible",
                    "region:us"
                  ],
                  "pipeline_tag": "image-classification",
                  "library_name": "transformers",
                  "readme": "---\nlicense: apache-2.0\ntags:\n- vision\n- image-classification\ndatasets:\n- imagenet-1k\n- imagenet-21k\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\n  example_title: Tiger\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg\n  example_title: Teapot\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg\n  example_title: Palace\n---\n\n# Vision Transformer (base-sized model) \n\nVision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Dosovitskiy et al. and first released in [this repository](https://github.com/google-research/vision_transformer). However, the weights were converted from the [timm repository](https://github.com/rwightman/pytorch-image-models) by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him. \n\nDisclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. Next, the model was fine-tuned on ImageNet (also referred to as ILSVRC2012), a dataset comprising 1 million images and 1,000 classes, also at resolution 224x224.\n\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\n\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.\n\n## Intended uses & limitations\n\nYou can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=google/vit) to look for\nfine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\n\n```python\nfrom transformers import ViTImageProcessor, ViTForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n```\n\nFor more code examples, we refer to the [documentation](https://huggingface.co/transformers/model_doc/vit.html#).\n\n## Training data\n\nThe ViT model was pretrained on [ImageNet-21k](http://www.image-net.org/), a dataset consisting of 14 million images and 21k classes, and fine-tuned on [ImageNet](http://www.image-net.org/challenges/LSVRC/2012/), a dataset consisting of 1 million images and 1k classes. \n\n## Training procedure\n\n### Preprocessing\n\nThe exact details of preprocessing of images during training/validation can be found [here](https://github.com/google-research/vision_transformer/blob/master/vit_jax/input_pipeline.py). \n\nImages are resized/rescaled to the same resolution (224x224) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).\n\n### Pretraining\n\nThe model was trained on TPUv3 hardware (8 cores). All model variants are trained with a batch size of 4096 and learning rate warmup of 10k steps. For ImageNet, the authors found it beneficial to additionally apply gradient clipping at global norm 1. Training resolution is 224.\n\n## Evaluation results\n\nFor evaluation results on several image classification benchmarks, we refer to tables 2 and 5 of the original paper. Note that for fine-tuning, the best results are obtained with a higher resolution (384x384). Of course, increasing the model size will result in better performance.\n\n### BibTeX entry and citation info\n\n```bibtex\n@misc{wu2020visual,\n      title={Visual Transformers: Token-based Image Representation and Processing for Computer Vision}, \n      author={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},\n      year={2020},\n      eprint={2006.03677},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n```bibtex\n@inproceedings{deng2009imagenet,\n  title={Imagenet: A large-scale hierarchical image database},\n  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\n  booktitle={2009 IEEE conference on computer vision and pattern recognition},\n  pages={248--255},\n  year={2009},\n  organization={Ieee}\n}\n```",
                  "extracted_code": "from transformers import ViTImageProcessor, ViTForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
                },
                {
                  "id": "google/vit-base-patch16-224-in21k",
                  "author": "google",
                  "sha": "b4569560a39a0f1af58e3ddaf17facf20ab919b0",
                  "created_at": "2022-03-02T23:29:05+00:00",
                  "last_modified": "2024-02-05T16:37:39+00:00",
                  "private": false,
                  "gated": false,
                  "disabled": false,
                  "downloads": 2100289,
                  "likes": 378,
                  "siblings": [
                    {
                      "rfilename": ".gitattributes"
                    },
                    {
                      "rfilename": "README.md"
                    },
                    {
                      "rfilename": "config.json"
                    },
                    {
                      "rfilename": "flax_model.msgpack"
                    },
                    {
                      "rfilename": "model.safetensors"
                    },
                    {
                      "rfilename": "preprocessor_config.json"
                    },
                    {
                      "rfilename": "pytorch_model.bin"
                    },
                    {
                      "rfilename": "tf_model.h5"
                    }
                  ],
                  "card_data": {
                    "license": "apache-2.0",
                    "language": [],
                    "tags": [
                      "vision"
                    ],
                    "datasets": [
                      "imagenet-21k"
                    ],
                    "task_categories": [],
                    "size_categories": [],
                    "metrics": [],
                    "widget": []
                  },
                  "tags": [
                    "transformers",
                    "pytorch",
                    "tf",
                    "jax",
                    "safetensors",
                    "vit",
                    "image-feature-extraction",
                    "vision",
                    "dataset:imagenet-21k",
                    "arxiv:2010.11929",
                    "arxiv:2006.03677",
                    "license:apache-2.0",
                    "region:us"
                  ],
                  "pipeline_tag": "image-feature-extraction",
                  "library_name": "transformers",
                  "readme": "---\nlicense: apache-2.0\ntags:\n- vision\ndatasets:\n- imagenet-21k\ninference: false\n---\n\n# Vision Transformer (base-sized model) \n\nVision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Dosovitskiy et al. and first released in [this repository](https://github.com/google-research/vision_transformer). However, the weights were converted from the [timm repository](https://github.com/rwightman/pytorch-image-models) by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him. \n\nDisclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. \n\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\n\nNote that this model does not provide any fine-tuned heads, as these were zero'd by Google researchers. However, the model does include the pre-trained pooler, which can be used for downstream tasks (such as image classification).\n\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.\n\n## Intended uses & limitations\n\nYou can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=google/vit) to look for\nfine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model in PyTorch:\n\n```python\nfrom transformers import ViTImageProcessor, ViTModel\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\ninputs = processor(images=image, return_tensors=\"pt\")\n\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state\n```\n\nHere is how to use this model in JAX/Flax:\n\n```python\nfrom transformers import ViTImageProcessor, FlaxViTModel\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\nmodel = FlaxViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n\ninputs = processor(images=image, return_tensors=\"np\")\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state\n```\n\n## Training data\n\nThe ViT model was pretrained on [ImageNet-21k](http://www.image-net.org/), a dataset consisting of 14 million images and 21k classes. \n\n## Training procedure\n\n### Preprocessing\n\nThe exact details of preprocessing of images during training/validation can be found [here](https://github.com/google-research/vision_transformer/blob/master/vit_jax/input_pipeline.py). \n\nImages are resized/rescaled to the same resolution (224x224) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).\n\n### Pretraining\n\nThe model was trained on TPUv3 hardware (8 cores). All model variants are trained with a batch size of 4096 and learning rate warmup of 10k steps. For ImageNet, the authors found it beneficial to additionally apply gradient clipping at global norm 1. Pre-training resolution is 224.\n\n## Evaluation results\n\nFor evaluation results on several image classification benchmarks, we refer to tables 2 and 5 of the original paper. Note that for fine-tuning, the best results are obtained with a higher resolution (384x384). Of course, increasing the model size will result in better performance.\n\n### BibTeX entry and citation info\n\n```bibtex\n@misc{wu2020visual,\n      title={Visual Transformers: Token-based Image Representation and Processing for Computer Vision}, \n      author={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},\n      year={2020},\n      eprint={2006.03677},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n```bibtex\n@inproceedings{deng2009imagenet,\n  title={Imagenet: A large-scale hierarchical image database},\n  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\n  booktitle={2009 IEEE conference on computer vision and pattern recognition},\n  pages={248--255},\n  year={2009},\n  organization={Ieee}\n}\n```",
                  "extracted_code": "from transformers import ViTImageProcessor, ViTModel\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\ninputs = processor(images=image, return_tensors=\"pt\")\n\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state\n\nfrom transformers import ViTImageProcessor, FlaxViTModel\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\nmodel = FlaxViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n\ninputs = processor(images=image, return_tensors=\"np\")\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state"
                },
                {
                  "id": "timm/vit_base_patch16_224.augreg2_in21k_ft_in1k",
                  "author": "timm",
                  "sha": "063c6c38a5d8510b2e57df480445e94b231dad2c",
                  "created_at": "2022-12-22T07:24:28+00:00",
                  "last_modified": "2025-01-20T16:09:05+00:00",
                  "private": false,
                  "gated": false,
                  "disabled": false,
                  "downloads": 511381,
                  "likes": 12,
                  "siblings": [
                    {
                      "rfilename": ".gitattributes"
                    },
                    {
                      "rfilename": "README.md"
                    },
                    {
                      "rfilename": "config.json"
                    },
                    {
                      "rfilename": "model.safetensors"
                    },
                    {
                      "rfilename": "pytorch_model.bin"
                    }
                  ],
                  "card_data": {
                    "license": "apache-2.0",
                    "language": [],
                    "library_name": "timm",
                    "tags": [
                      "image-classification",
                      "timm",
                      "transformers"
                    ],
                    "datasets": [
                      "imagenet-1k",
                      "imagenet-21k"
                    ],
                    "task_categories": [],
                    "size_categories": [],
                    "metrics": [],
                    "widget": []
                  },
                  "tags": [
                    "timm",
                    "pytorch",
                    "safetensors",
                    "image-classification",
                    "transformers",
                    "dataset:imagenet-1k",
                    "dataset:imagenet-21k",
                    "arxiv:2106.10270",
                    "arxiv:2010.11929",
                    "license:apache-2.0",
                    "region:us"
                  ],
                  "pipeline_tag": "image-classification",
                  "library_name": "timm",
                  "readme": "---\ntags:\n- image-classification\n- timm\n- transformers\nlibrary_name: timm\nlicense: apache-2.0\ndatasets:\n- imagenet-1k\n- imagenet-21k\n---\n# Model card for vit_base_patch16_224.augreg2_in21k_ft_in1k\n\nA Vision Transformer (ViT) image classification model. Trained on ImageNet-21k by paper authors and (re) fine-tuned on ImageNet-1k with additional augmentation and regularization by Ross Wightman.\n\n\n## Model Details\n- **Model Type:** Image classification / feature backbone\n- **Model Stats:**\n  - Params (M): 86.6\n  - GMACs: 16.9\n  - Activations (M): 16.5\n  - Image size: 224 x 224\n- **Papers:**\n  - How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers: https://arxiv.org/abs/2106.10270\n  - An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale: https://arxiv.org/abs/2010.11929v2\n- **Dataset:** ImageNet-1k\n- **Pretrain Dataset:** ImageNet-21k\n- **Original:** https://github.com/google-research/vision_transformer\n\n## Model Usage\n### Image Classification\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('vit_base_patch16_224.augreg2_in21k_ft_in1k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n```\n\n### Image Embeddings\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'vit_base_patch16_224.augreg2_in21k_ft_in1k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 197, 768) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor\n```\n\n## Model Comparison\nExplore the dataset and runtime metrics of this model in timm [model results](https://github.com/huggingface/pytorch-image-models/tree/main/results).\n\n## Citation\n```bibtex\n@article{steiner2021augreg,\n  title={How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers},\n  author={Steiner, Andreas and Kolesnikov, Alexander and and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},\n  journal={arXiv preprint arXiv:2106.10270},\n  year={2021}\n}\n```\n```bibtex\n@article{dosovitskiy2020vit,\n  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\n  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},\n  journal={ICLR},\n  year={2021}\n}\n```\n```bibtex\n@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}\n```",
                  "extracted_code": "from urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('vit_base_patch16_224.augreg2_in21k_ft_in1k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'vit_base_patch16_224.augreg2_in21k_ft_in1k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 197, 768) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor"
                }
              ],
              "datasets": [
                {
                  "id": "benjamin-paine/imagenet-1k-256x256",
                  "author": "benjamin-paine",
                  "sha": "1bd0400450249a7fe90c0aece37d0d03e7ea956a",
                  "created_at": "2024-09-13T13:50:22+00:00",
                  "last_modified": "2024-09-15T11:15:04+00:00",
                  "private": false,
                  "gated": false,
                  "disabled": false,
                  "downloads": 8693,
                  "likes": 12,
                  "siblings": [
                    {
                      "rfilename": ".gitattributes"
                    },
                    {
                      "rfilename": "LICENSE"
                    },
                    {
                      "rfilename": "README.md"
                    },
                    {
                      "rfilename": "data/test-00000-of-00003.parquet"
                    },
                    {
                      "rfilename": "data/test-00001-of-00003.parquet"
                    },
                    {
                      "rfilename": "data/test-00002-of-00003.parquet"
                    },
                    {
                      "rfilename": "data/train-00000-of-00040.parquet"
                    },
                    {
                      "rfilename": "data/train-00001-of-00040.parquet"
                    },
                    {
                      "rfilename": "data/train-00002-of-00040.parquet"
                    },
                    {
                      "rfilename": "data/train-00003-of-00040.parquet"
                    }
                  ],
                  "card_data": {
                    "license": [
                      "other"
                    ],
                    "language": [
                      "en"
                    ],
                    "tags": [
                      "imagenet",
                      "imagenet-1k",
                      "ilsvrc-2012"
                    ],
                    "datasets": [],
                    "task_categories": [
                      "image-classification"
                    ],
                    "size_categories": [
                      "1M<n<10M"
                    ],
                    "metrics": [],
                    "widget": []
                  },
                  "tags": [
                    "task_categories:image-classification",
                    "task_ids:multi-class-image-classification",
                    "annotations_creators:crowdsourced",
                    "language_creators:crowdsourced",
                    "multilinguality:monolingual",
                    "source_datasets:original",
                    "language:en",
                    "license:other",
                    "size_categories:1M<n<10M",
                    "format:parquet",
                    "modality:image",
                    "library:datasets",
                    "library:dask",
                    "library:mlcroissant",
                    "library:polars",
                    "arxiv:1409.0575",
                    "arxiv:1912.07726",
                    "arxiv:1811.12231",
                    "arxiv:2109.13228",
                    "region:us",
                    "imagenet",
                    "imagenet-1k",
                    "ilsvrc-2012"
                  ],
                  "readme": "---\nannotations_creators:\n- crowdsourced\nlanguage_creators:\n- crowdsourced\nlanguage:\n- en\nlicense:\n- other\nmultilinguality:\n- monolingual\nsize_categories:\n- 1M<n<10M\nsource_datasets:\n- original\ntask_categories:\n- image-classification\ntask_ids:\n- multi-class-image-classification\npaperswithcode_id: imagenet-1k-1\npretty_name: ImageNet\nlicense_details: imagenet-agreement\nextra_gated_prompt: 'By clicking on “Access repository” below, you also agree to ImageNet\n  Terms of Access:\n\n  [RESEARCHER_FULLNAME] (the \"Researcher\") has requested permission to use the ImageNet\n  database (the \"Database\") at Princeton University and Stanford University. In exchange\n  for such permission, Researcher hereby agrees to the following terms and conditions:\n\n  1. Researcher shall use the Database only for non-commercial research and educational\n  purposes.\n\n  2. Princeton University, Stanford University and Hugging Face make no representations\n  or warranties regarding the Database, including but not limited to warranties of\n  non-infringement or fitness for a particular purpose.\n\n  3. Researcher accepts full responsibility for his or her use of the Database and\n  shall defend and indemnify the ImageNet team, Princeton University, Stanford University\n  and Hugging Face, including their employees, Trustees, officers and agents, against\n  any and all claims arising from Researcher''s use of the Database, including but\n  not limited to Researcher''s use of any copies of copyrighted images that he or\n  she may create from the Database.\n\n  4. Researcher may provide research associates and colleagues with access to the\n  Database provided that they first agree to be bound by these terms and conditions.\n\n  5. Princeton University, Stanford University and Hugging Face reserve the right\n  to terminate Researcher''s access to the Database at any time.\n\n  6. If Researcher is employed by a for-profit, commercial entity, Researcher''s employer\n  shall also be bound by these terms and conditions, and Researcher hereby represents\n  that he or she is fully authorized to enter into this agreement on behalf of such\n  employer.\n\n  7. The law of the State of New Jersey shall apply to all disputes under this agreement.'\ntags:\n- imagenet\n- imagenet-1k\n- ilsvrc-2012\ndataset_info:\n  features:\n  - name: image\n    dtype: image\n  - name: label\n    dtype:\n      class_label:\n        names:\n          '0': tench, Tinca tinca\n          '1': goldfish, Carassius auratus\n          '2': great white shark, white shark, man-eater, man-eating shark, Carcharodon\n            carcharias\n          '3': tiger shark, Galeocerdo cuvieri\n          '4': hammerhead, hammerhead shark\n          '5': electric ray, crampfish, numbfish, torpedo\n          '6': stingray\n          '7': cock\n          '8': hen\n          '9': ostrich, Struthio camelus\n          '10': brambling, Fringilla montifringilla\n          '11': goldfinch, Carduelis carduelis\n          '12': house finch, linnet, Carpodacus mexicanus\n          '13': junco, snowbird\n          '14': indigo bunting, indigo finch, indigo bird, Passerina cyanea\n          '15': robin, American robin, Turdus migratorius\n          '16': bulbul\n          '17': jay\n          '18': magpie\n          '19': chickadee\n          '20': water ouzel, dipper\n          '21': kite\n          '22': bald eagle, American eagle, Haliaeetus leucocephalus\n          '23': vulture\n          '24': great grey owl, great gray owl, Strix nebulosa\n          '25': European fire salamander, Salamandra salamandra\n          '26': common newt, Triturus vulgaris\n          '27': eft\n          '28': spotted salamander, Ambystoma maculatum\n          '29': axolotl, mud puppy, Ambystoma mexicanum\n          '30': bullfrog, Rana catesbeiana\n          '31': tree frog, tree-frog\n          '32': tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui\n          '33': loggerhead, loggerhead turtle, Caretta caretta\n          '34': leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea\n          '35': mud turtle\n          '36': terrapin\n          '37': box turtle, box tortoise\n          '38': banded gecko\n          '39': common iguana, iguana, Iguana iguana\n          '40': American chameleon, anole, Anolis carolinensis\n          '41': whiptail, whiptail lizard\n          '42': agama\n          '43': frilled lizard, Chlamydosaurus kingi\n          '44': alligator lizard\n          '45': Gila monster, Heloderma suspectum\n          '46': green lizard, Lacerta viridis\n          '47': African chameleon, Chamaeleo chamaeleon\n          '48': Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus\n            komodoensis\n          '49': African crocodile, Nile crocodile, Crocodylus niloticus\n          '50': American alligator, Alligator mississipiensis\n          '51': triceratops\n          '52': thunder snake, worm snake, Carphophis amoenus\n          '53': ringneck snake, ring-necked snake, ring snake\n          '54': hognose snake, puff adder, sand viper\n          '55': green snake, grass snake\n          '56': king snake, kingsnake\n          '57': garter snake, grass snake\n          '58': water snake\n          '59': vine snake\n          '60': night snake, Hypsiglena torquata\n          '61': boa constrictor, Constrictor constrictor\n          '62': rock python, rock snake, Python sebae\n          '63': Indian cobra, Naja naja\n          '64': green mamba\n          '65': sea snake\n          '66': horned viper, cerastes, sand viper, horned asp, Cerastes cornutus\n          '67': diamondback, diamondback rattlesnake, Crotalus adamanteus\n          '68': sidewinder, horned rattlesnake, Crotalus cerastes\n          '69': trilobite\n          '70': harvestman, daddy longlegs, Phalangium opilio\n          '71': scorpion\n          '72': black and gold garden spider, Argiope aurantia\n          '73': barn spider, Araneus cavaticus\n          '74': garden spider, Aranea diademata\n          '75': black widow, Latrodectus mactans\n          '76': tarantula\n          '77': wolf spider, hunting spider\n          '78': tick\n          '79': centipede\n          '80': black grouse\n          '81': ptarmigan\n          '82': ruffed grouse, partridge, Bonasa umbellus\n          '83': prairie chicken, prairie grouse, prairie fowl\n          '84': peacock\n          '85': quail\n          '86': partridge\n          '87': African grey, African gray, Psittacus erithacus\n          '88': macaw\n          '89': sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita\n          '90': lorikeet\n          '91': coucal\n          '92': bee eater\n          '93': hornbill\n          '94': hummingbird\n          '95': jacamar\n          '96': toucan\n          '97': drake\n          '98': red-breasted merganser, Mergus serrator\n          '99': goose\n          '100': black swan, Cygnus atratus\n          '101': tusker\n          '102': echidna, spiny anteater, anteater\n          '103': platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus\n            anatinus\n          '104': wallaby, brush kangaroo\n          '105': koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus\n          '106': wombat\n          '107': jellyfish\n          '108': sea anemone, anemone\n          '109': brain coral\n          '110': flatworm, platyhelminth\n          '111': nematode, nematode worm, roundworm\n          '112': conch\n          '113': snail\n          '114': slug\n          '115': sea slug, nudibranch\n          '116': chiton, coat-of-mail shell, sea cradle, polyplacophore\n          '117': chambered nautilus, pearly nautilus, nautilus\n          '118': Dungeness crab, Cancer magister\n          '119': rock crab, Cancer irroratus\n          '120': fiddler crab\n          '121': king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes\n            camtschatica\n          '122': American lobster, Northern lobster, Maine lobster, Homarus americanus\n          '123': spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish\n          '124': crayfish, crawfish, crawdad, crawdaddy\n          '125': hermit crab\n          '126': isopod\n          '127': white stork, Ciconia ciconia\n          '128': black stork, Ciconia nigra\n          '129': spoonbill\n          '130': flamingo\n          '131': little blue heron, Egretta caerulea\n          '132': American egret, great white heron, Egretta albus\n          '133': bittern\n          '134': crane\n          '135': limpkin, Aramus pictus\n          '136': European gallinule, Porphyrio porphyrio\n          '137': American coot, marsh hen, mud hen, water hen, Fulica americana\n          '138': bustard\n          '139': ruddy turnstone, Arenaria interpres\n          '140': red-backed sandpiper, dunlin, Erolia alpina\n          '141': redshank, Tringa totanus\n          '142': dowitcher\n          '143': oystercatcher, oyster catcher\n          '144': pelican\n          '145': king penguin, Aptenodytes patagonica\n          '146': albatross, mollymawk\n          '147': grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius\n            robustus\n          '148': killer whale, killer, orca, grampus, sea wolf, Orcinus orca\n          '149': dugong, Dugong dugon\n          '150': sea lion\n          '151': Chihuahua\n          '152': Japanese spaniel\n          '153': Maltese dog, Maltese terrier, Maltese\n          '154': Pekinese, Pekingese, Peke\n          '155': Shih-Tzu\n          '156': Blenheim spaniel\n          '157': papillon\n          '158': toy terrier\n          '159': Rhodesian ridgeback\n          '160': Afghan hound, Afghan\n          '161': basset, basset hound\n          '162': beagle\n          '163': bloodhound, sleuthhound\n          '164': bluetick\n          '165': black-and-tan coonhound\n          '166': Walker hound, Walker foxhound\n          '167': English foxhound\n          '168': redbone\n          '169': borzoi, Russian wolfhound\n          '170': Irish wolfhound\n          '171': Italian greyhound\n          '172': whippet\n          '173': Ibizan hound, Ibizan Podenco\n          '174': Norwegian elkhound, elkhound\n          '175': otterhound, otter hound\n          '176': Saluki, gazelle hound\n          '177': Scottish deerhound, deerhound\n          '178': Weimaraner\n          '179': Staffordshire bullterrier, Staffordshire bull terrier\n          '180': American Staffordshire terrier, Staffordshire terrier, American pit\n            bull terrier, pit bull terrier\n          '181': Bedlington terrier\n          '182': Border terrier\n          '183': Kerry blue terrier\n          '184': Irish terrier\n          '185': Norfolk terrier\n          '186': Norwich terrier\n          '187': Yorkshire terrier\n          '188': wire-haired fox terrier\n          '189': Lakeland terrier\n          '190': Sealyham terrier, Sealyham\n          '191': Airedale, Airedale terrier\n          '192': cairn, cairn terrier\n          '193': Australian terrier\n          '194': Dandie Dinmont, Dandie Dinmont terrier\n          '195': Boston bull, Boston terrier\n          '196': miniature schnauzer\n          '197': giant schnauzer\n          '198': standard schnauzer\n          '199': Scotch terrier, Scottish terrier, Scottie\n          '200': Tibetan terrier, chrysanthemum dog\n          '201': silky terrier, Sydney silky\n          '202': soft-coated wheaten terrier\n          '203': West Highland white terrier\n          '204': Lhasa, Lhasa apso\n          '205': flat-coated retriever\n          '206': curly-coated retriever\n          '207': golden retriever\n          '208': Labrador retriever\n          '209': Chesapeake Bay retriever\n          '210': German short-haired pointer\n          '211': vizsla, Hungarian pointer\n          '212': English setter\n          '213': Irish setter, red setter\n          '214': Gordon setter\n          '215': Brittany spaniel\n          '216': clumber, clumber spaniel\n          '217': English springer, English springer spaniel\n          '218': Welsh springer spaniel\n          '219': cocker spaniel, English cocker spaniel, cocker\n          '220': Sussex spaniel\n          '221': Irish water spaniel\n          '222': kuvasz\n          '223': schipperke\n          '224': groenendael\n          '225': malinois\n          '226': briard\n          '227': kelpie\n          '228': komondor\n          '229': Old English sheepdog, bobtail\n          '230': Shetland sheepdog, Shetland sheep dog, Shetland\n          '231': collie\n          '232': Border collie\n          '233': Bouvier des Flandres, Bouviers des Flandres\n          '234': Rottweiler\n          '235': German shepherd, German shepherd dog, German police dog, alsatian\n          '236': Doberman, Doberman pinscher\n          '237': miniature pinscher\n          '238': Greater Swiss Mountain dog\n          '239': Bernese mountain dog\n          '240': Appenzeller\n          '241': EntleBucher\n          '242': boxer\n          '243': bull mastiff\n          '244': Tibetan mastiff\n          '245': French bulldog\n          '246': Great Dane\n          '247': Saint Bernard, St Bernard\n          '248': Eskimo dog, husky\n          '249': malamute, malemute, Alaskan malamute\n          '250': Siberian husky\n          '251': dalmatian, coach dog, carriage dog\n          '252': affenpinscher, monkey pinscher, monkey dog\n          '253': basenji\n          '254': pug, pug-dog\n          '255': Leonberg\n          '256': Newfoundland, Newfoundland dog\n          '257': Great Pyrenees\n          '258': Samoyed, Samoyede\n          '259': Pomeranian\n          '260': chow, chow chow\n          '261': keeshond\n          '262': Brabancon griffon\n          '263': Pembroke, Pembroke Welsh corgi\n          '264': Cardigan, Cardigan Welsh corgi\n          '265': toy poodle\n          '266': miniature poodle\n          '267': standard poodle\n          '268': Mexican hairless\n          '269': timber wolf, grey wolf, gray wolf, Canis lupus\n          '270': white wolf, Arctic wolf, Canis lupus tundrarum\n          '271': red wolf, maned wolf, Canis rufus, Canis niger\n          '272': coyote, prairie wolf, brush wolf, Canis latrans\n          '273': dingo, warrigal, warragal, Canis dingo\n          '274': dhole, Cuon alpinus\n          '275': African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus\n          '276': hyena, hyaena\n          '277': red fox, Vulpes vulpes\n          '278': kit fox, Vulpes macrotis\n          '279': Arctic fox, white fox, Alopex lagopus\n          '280': grey fox, gray fox, Urocyon cinereoargenteus\n          '281': tabby, tabby cat\n          '282': tiger cat\n          '283': Persian cat\n          '284': Siamese cat, Siamese\n          '285': Egyptian cat\n          '286': cougar, puma, catamount, mountain lion, painter, panther, Felis concolor\n          '287': lynx, catamount\n          '288': leopard, Panthera pardus\n          '289': snow leopard, ounce, Panthera uncia\n          '290': jaguar, panther, Panthera onca, Felis onca\n          '291': lion, king of beasts, Panthera leo\n          '292': tiger, Panthera tigris\n          '293': cheetah, chetah, Acinonyx jubatus\n          '294': brown bear, bruin, Ursus arctos\n          '295': American black bear, black bear, Ursus americanus, Euarctos americanus\n          '296': ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus\n          '297': sloth bear, Melursus ursinus, Ursus ursinus\n          '298': mongoose\n          '299': meerkat, mierkat\n          '300': tiger beetle\n          '301': ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle\n          '302': ground beetle, carabid beetle\n          '303': long-horned beetle, longicorn, longicorn beetle\n          '304': leaf beetle, chrysomelid\n          '305': dung beetle\n          '306': rhinoceros beetle\n          '307': weevil\n          '308': fly\n          '309': bee\n          '310': ant, emmet, pismire\n          '311': grasshopper, hopper\n          '312': cricket\n          '313': walking stick, walkingstick, stick insect\n          '314': cockroach, roach\n          '315': mantis, mantid\n          '316': cicada, cicala\n          '317': leafhopper\n          '318': lacewing, lacewing fly\n          '319': dragonfly, darning needle, devil's darning needle, sewing needle,\n            snake feeder, snake doctor, mosquito hawk, skeeter hawk\n          '320': damselfly\n          '321': admiral\n          '322': ringlet, ringlet butterfly\n          '323': monarch, monarch butterfly, milkweed butterfly, Danaus plexippus\n          '324': cabbage butterfly\n          '325': sulphur butterfly, sulfur butterfly\n          '326': lycaenid, lycaenid butterfly\n          '327': starfish, sea star\n          '328': sea urchin\n          '329': sea cucumber, holothurian\n          '330': wood rabbit, cottontail, cottontail rabbit\n          '331': hare\n          '332': Angora, Angora rabbit\n          '333': hamster\n          '334': porcupine, hedgehog\n          '335': fox squirrel, eastern fox squirrel, Sciurus niger\n          '336': marmot\n          '337': beaver\n          '338': guinea pig, Cavia cobaya\n          '339': sorrel\n          '340': zebra\n          '341': hog, pig, grunter, squealer, Sus scrofa\n          '342': wild boar, boar, Sus scrofa\n          '343': warthog\n          '344': hippopotamus, hippo, river horse, Hippopotamus amphibius\n          '345': ox\n          '346': water buffalo, water ox, Asiatic buffalo, Bubalus bubalis\n          '347': bison\n          '348': ram, tup\n          '349': bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain\n            sheep, Ovis canadensis\n          '350': ibex, Capra ibex\n          '351': hartebeest\n          '352': impala, Aepyceros melampus\n          '353': gazelle\n          '354': Arabian camel, dromedary, Camelus dromedarius\n          '355': llama\n          '356': weasel\n          '357': mink\n          '358': polecat, fitch, foulmart, foumart, Mustela putorius\n          '359': black-footed ferret, ferret, Mustela nigripes\n          '360': otter\n          '361': skunk, polecat, wood pussy\n          '362': badger\n          '363': armadillo\n          '364': three-toed sloth, ai, Bradypus tridactylus\n          '365': orangutan, orang, orangutang, Pongo pygmaeus\n          '366': gorilla, Gorilla gorilla\n          '367': chimpanzee, chimp, Pan troglodytes\n          '368': gibbon, Hylobates lar\n          '369': siamang, Hylobates syndactylus, Symphalangus syndactylus\n          '370': guenon, guenon monkey\n          '371': patas, hussar monkey, Erythrocebus patas\n          '372': baboon\n          '373': macaque\n          '374': langur\n          '375': colobus, colobus monkey\n          '376': proboscis monkey, Nasalis larvatus\n          '377': marmoset\n          '378': capuchin, ringtail, Cebus capucinus\n          '379': howler monkey, howler\n          '380': titi, titi monkey\n          '381': spider monkey, Ateles geoffroyi\n          '382': squirrel monkey, Saimiri sciureus\n          '383': Madagascar cat, ring-tailed lemur, Lemur catta\n          '384': indri, indris, Indri indri, Indri brevicaudatus\n          '385': Indian elephant, Elephas maximus\n          '386': African elephant, Loxodonta africana\n          '387': lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens\n          '388': giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca\n          '389': barracouta, snoek\n          '390': eel\n          '391': coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus\n            kisutch\n          '392': rock beauty, Holocanthus tricolor\n          '393': anemone fish\n          '394': sturgeon\n          '395': gar, garfish, garpike, billfish, Lepisosteus osseus\n          '396': lionfish\n          '397': puffer, pufferfish, blowfish, globefish\n          '398': abacus\n          '399': abaya\n          '400': academic gown, academic robe, judge's robe\n          '401': accordion, piano accordion, squeeze box\n          '402': acoustic guitar\n          '403': aircraft carrier, carrier, flattop, attack aircraft carrier\n          '404': airliner\n          '405': airship, dirigible\n          '406': altar\n          '407': ambulance\n          '408': amphibian, amphibious vehicle\n          '409': analog clock\n          '410': apiary, bee house\n          '411': apron\n          '412': ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin,\n            dustbin, trash barrel, trash bin\n          '413': assault rifle, assault gun\n          '414': backpack, back pack, knapsack, packsack, rucksack, haversack\n          '415': bakery, bakeshop, bakehouse\n          '416': balance beam, beam\n          '417': balloon\n          '418': ballpoint, ballpoint pen, ballpen, Biro\n          '419': Band Aid\n          '420': banjo\n          '421': bannister, banister, balustrade, balusters, handrail\n          '422': barbell\n          '423': barber chair\n          '424': barbershop\n          '425': barn\n          '426': barometer\n          '427': barrel, cask\n          '428': barrow, garden cart, lawn cart, wheelbarrow\n          '429': baseball\n          '430': basketball\n          '431': bassinet\n          '432': bassoon\n          '433': bathing cap, swimming cap\n          '434': bath towel\n          '435': bathtub, bathing tub, bath, tub\n          '436': beach wagon, station wagon, wagon, estate car, beach waggon, station\n            waggon, waggon\n          '437': beacon, lighthouse, beacon light, pharos\n          '438': beaker\n          '439': bearskin, busby, shako\n          '440': beer bottle\n          '441': beer glass\n          '442': bell cote, bell cot\n          '443': bib\n          '444': bicycle-built-for-two, tandem bicycle, tandem\n          '445': bikini, two-piece\n          '446': binder, ring-binder\n          '447': binoculars, field glasses, opera glasses\n          '448': birdhouse\n          '449': boathouse\n          '450': bobsled, bobsleigh, bob\n          '451': bolo tie, bolo, bola tie, bola\n          '452': bonnet, poke bonnet\n          '453': bookcase\n          '454': bookshop, bookstore, bookstall\n          '455': bottlecap\n          '456': bow\n          '457': bow tie, bow-tie, bowtie\n          '458': brass, memorial tablet, plaque\n          '459': brassiere, bra, bandeau\n          '460': breakwater, groin, groyne, mole, bulwark, seawall, jetty\n          '461': breastplate, aegis, egis\n          '462': broom\n          '463': bucket, pail\n          '464': buckle\n          '465': bulletproof vest\n          '466': bullet train, bullet\n          '467': butcher shop, meat market\n          '468': cab, hack, taxi, taxicab\n          '469': caldron, cauldron\n          '470': candle, taper, wax light\n          '471': cannon\n          '472': canoe\n          '473': can opener, tin opener\n          '474': cardigan\n          '475': car mirror\n          '476': carousel, carrousel, merry-go-round, roundabout, whirligig\n          '477': carpenter's kit, tool kit\n          '478': carton\n          '479': car wheel\n          '480': cash machine, cash dispenser, automated teller machine, automatic\n            teller machine, automated teller, automatic teller, ATM\n          '481': cassette\n          '482': cassette player\n          '483': castle\n          '484': catamaran\n          '485': CD player\n          '486': cello, violoncello\n          '487': cellular telephone, cellular phone, cellphone, cell, mobile phone\n          '488': chain\n          '489': chainlink fence\n          '490': chain mail, ring mail, mail, chain armor, chain armour, ring armor,\n            ring armour\n          '491': chain saw, chainsaw\n          '492': chest\n          '493': chiffonier, commode\n          '494': chime, bell, gong\n          '495': china cabinet, china closet\n          '496': Christmas stocking\n          '497': church, church building\n          '498': cinema, movie theater, movie theatre, movie house, picture palace\n          '499': cleaver, meat cleaver, chopper\n          '500': cliff dwelling\n          '501': cloak\n          '502': clog, geta, patten, sabot\n          '503': cocktail shaker\n          '504': coffee mug\n          '505': coffeepot\n          '506': coil, spiral, volute, whorl, helix\n          '507': combination lock\n          '508': computer keyboard, keypad\n          '509': confectionery, confectionary, candy store\n          '510': container ship, containership, container vessel\n          '511': convertible\n          '512': corkscrew, bottle screw\n          '513': cornet, horn, trumpet, trump\n          '514': cowboy boot\n          '515': cowboy hat, ten-gallon hat\n          '516': cradle\n          '517': crane2\n          '518': crash helmet\n          '519': crate\n          '520': crib, cot\n          '521': Crock Pot\n          '522': croquet ball\n          '523': crutch\n          '524': cuirass\n          '525': dam, dike, dyke\n          '526': desk\n          '527': desktop computer\n          '528': dial telephone, dial phone\n          '529': diaper, nappy, napkin\n          '530': digital clock\n          '531': digital watch\n          '532': dining table, board\n          '533': dishrag, dishcloth\n          '534': dishwasher, dish washer, dishwashing machine\n          '535': disk brake, disc brake\n          '536': dock, dockage, docking facility\n          '537': dogsled, dog sled, dog sleigh\n          '538': dome\n          '539': doormat, welcome mat\n          '540': drilling platform, offshore rig\n          '541': drum, membranophone, tympan\n          '542': drumstick\n          '543': dumbbell\n          '544': Dutch oven\n          '545': electric fan, blower\n          '546': electric guitar\n          '547': electric locomotive\n          '548': entertainment center\n          '549': envelope\n          '550': espresso maker\n          '551': face powder\n          '552': feather boa, boa\n          '553': file, file cabinet, filing cabinet\n          '554': fireboat\n          '555': fire engine, fire truck\n          '556': fire screen, fireguard\n          '557': flagpole, flagstaff\n          '558': flute, transverse flute\n          '559': folding chair\n          '560': football helmet\n          '561': forklift\n          '562': fountain\n          '563': fountain pen\n          '564': four-poster\n          '565': freight car\n          '566': French horn, horn\n          '567': frying pan, frypan, skillet\n          '568': fur coat\n          '569': garbage truck, dustcart\n          '570': gasmask, respirator, gas helmet\n          '571': gas pump, gasoline pump, petrol pump, island dispenser\n          '572': goblet\n          '573': go-kart\n          '574': golf ball\n          '575': golfcart, golf cart\n          '576': gondola\n          '577': gong, tam-tam\n          '578': gown\n          '579': grand piano, grand\n          '580': greenhouse, nursery, glasshouse\n          '581': grille, radiator grille\n          '582': grocery store, grocery, food market, market\n          '583': guillotine\n          '584': hair slide\n          '585': hair spray\n          '586': half track\n          '587': hammer\n          '588': hamper\n          '589': hand blower, blow dryer, blow drier, hair dryer, hair drier\n          '590': hand-held computer, hand-held microcomputer\n          '591': handkerchief, hankie, hanky, hankey\n          '592': hard disc, hard disk, fixed disk\n          '593': harmonica, mouth organ, harp, mouth harp\n          '594': harp\n          '595': harvester, reaper\n          '596': hatchet\n          '597': holster\n          '598': home theater, home theatre\n          '599': honeycomb\n          '600': hook, claw\n          '601': hoopskirt, crinoline\n          '602': horizontal bar, high bar\n          '603': horse cart, horse-cart\n          '604': hourglass\n          '605': iPod\n          '606': iron, smoothing iron\n          '607': jack-o'-lantern\n          '608': jean, blue jean, denim\n          '609': jeep, landrover\n          '610': jersey, T-shirt, tee shirt\n          '611': jigsaw puzzle\n          '612': jinrikisha, ricksha, rickshaw\n          '613': joystick\n          '614': kimono\n          '615': knee pad\n          '616': knot\n          '617': lab coat, laboratory coat\n          '618': ladle\n          '619': lampshade, lamp shade\n          '620': laptop, laptop computer\n          '621': lawn mower, mower\n          '622': lens cap, lens cover\n          '623': letter opener, paper knife, paperknife\n          '624': library\n          '625': lifeboat\n          '626': lighter, light, igniter, ignitor\n          '627': limousine, limo\n          '628': liner, ocean liner\n          '629': lipstick, lip rouge\n          '630': Loafer\n          '631': lotion\n          '632': loudspeaker, speaker, speaker unit, loudspeaker system, speaker system\n          '633': loupe, jeweler's loupe\n          '634': lumbermill, sawmill\n          '635': magnetic compass\n          '636': mailbag, postbag\n          '637': mailbox, letter box\n          '638': maillot\n          '639': maillot, tank suit\n          '640': manhole cover\n          '641': maraca\n          '642': marimba, xylophone\n          '643': mask\n          '644': matchstick\n          '645': maypole\n          '646': maze, labyrinth\n          '647': measuring cup\n          '648': medicine chest, medicine cabinet\n          '649': megalith, megalithic structure\n          '650': microphone, mike\n          '651': microwave, microwave oven\n          '652': military uniform\n          '653': milk can\n          '654': minibus\n          '655': miniskirt, mini\n          '656': minivan\n          '657': missile\n          '658': mitten\n          '659': mixing bowl\n          '660': mobile home, manufactured home\n          '661': Model T\n          '662': modem\n          '663': monastery\n          '664': monitor\n          '665': moped\n          '666': mortar\n          '667': mortarboard\n          '668': mosque\n          '669': mosquito net\n          '670': motor scooter, scooter\n          '671': mountain bike, all-terrain bike, off-roader\n          '672': mountain tent\n          '673': mouse, computer mouse\n          '674': mousetrap\n          '675': moving van\n          '676': muzzle\n          '677': nail\n          '678': neck brace\n          '679': necklace\n          '680': nipple\n          '681': notebook, notebook computer\n          '682': obelisk\n          '683': oboe, hautboy, hautbois\n          '684': ocarina, sweet potato\n          '685': odometer, hodometer, mileometer, milometer\n          '686': oil filter\n          '687': organ, pipe organ\n          '688': oscilloscope, scope, cathode-ray oscilloscope, CRO\n          '689': overskirt\n          '690': oxcart\n          '691': oxygen mask\n          '692': packet\n          '693': paddle, boat paddle\n          '694': paddlewheel, paddle wheel\n          '695': padlock\n          '696': paintbrush\n          '697': pajama, pyjama, pj's, jammies\n          '698': palace\n          '699': panpipe, pandean pipe, syrinx\n          '700': paper towel\n          '701': parachute, chute\n          '702': parallel bars, bars\n          '703': park bench\n          '704': parking meter\n          '705': passenger car, coach, carriage\n          '706': patio, terrace\n          '707': pay-phone, pay-station\n          '708': pedestal, plinth, footstall\n          '709': pencil box, pencil case\n          '710': pencil sharpener\n          '711': perfume, essence\n          '712': Petri dish\n          '713': photocopier\n          '714': pick, plectrum, plectron\n          '715': pickelhaube\n          '716': picket fence, paling\n          '717': pickup, pickup truck\n          '718': pier\n          '719': piggy bank, penny bank\n          '720': pill bottle\n          '721': pillow\n          '722': ping-pong ball\n          '723': pinwheel\n          '724': pirate, pirate ship\n          '725': pitcher, ewer\n          '726': plane, carpenter's plane, woodworking plane\n          '727': planetarium\n          '728': plastic bag\n          '729': plate rack\n          '730': plow, plough\n          '731': plunger, plumber's helper\n          '732': Polaroid camera, Polaroid Land camera\n          '733': pole\n          '734': police van, police wagon, paddy wagon, patrol wagon, wagon, black\n            Maria\n          '735': poncho\n          '736': pool table, billiard table, snooker table\n          '737': pop bottle, soda bottle\n          '738': pot, flowerpot\n          '739': potter's wheel\n          '740': power drill\n          '741': prayer rug, prayer mat\n          '742': printer\n          '743': prison, prison house\n          '744': projectile, missile\n          '745': projector\n          '746': puck, hockey puck\n          '747': punching bag, punch bag, punching ball, punchball\n          '748': purse\n          '749': quill, quill pen\n          '750': quilt, comforter, comfort, puff\n          '751': racer, race car, racing car\n          '752': racket, racquet\n          '753': radiator\n          '754': radio, wireless\n          '755': radio telescope, radio reflector\n          '756': rain barrel\n          '757': recreational vehicle, RV, R.V.\n          '758': reel\n          '759': reflex camera\n          '760': refrigerator, icebox\n          '761': remote control, remote\n          '762': restaurant, eating house, eating place, eatery\n          '763': revolver, six-gun, six-shooter\n          '764': rifle\n          '765': rocking chair, rocker\n          '766': rotisserie\n          '767': rubber eraser, rubber, pencil eraser\n          '768': rugby ball\n          '769': rule, ruler\n          '770': running shoe\n          '771': safe\n          '772': safety pin\n          '773': saltshaker, salt shaker\n          '774': sandal\n          '775': sarong\n          '776': sax, saxophone\n          '777': scabbard\n          '778': scale, weighing machine\n          '779': school bus\n          '780': schooner\n          '781': scoreboard\n          '782': screen, CRT screen\n          '783': screw\n          '784': screwdriver\n          '785': seat belt, seatbelt\n          '786': sewing machine\n          '787': shield, buckler\n          '788': shoe shop, shoe-shop, shoe store\n          '789': shoji\n          '790': shopping basket\n          '791': shopping cart\n          '792': shovel\n          '793': shower cap\n          '794': shower curtain\n          '795': ski\n          '796': ski mask\n          '797': sleeping bag\n          '798': slide rule, slipstick\n          '799': sliding door\n          '800': slot, one-armed bandit\n          '801': snorkel\n          '802': snowmobile\n          '803': snowplow, snowplough\n          '804': soap dispenser\n          '805': soccer ball\n          '806': sock\n          '807': solar dish, solar collector, solar furnace\n          '808': sombrero\n          '809': soup bowl\n          '810': space bar\n          '811': space heater\n          '812': space shuttle\n          '813': spatula\n          '814': speedboat\n          '815': spider web, spider's web\n          '816': spindle\n          '817': sports car, sport car\n          '818': spotlight, spot\n          '819': stage\n          '820': steam locomotive\n          '821': steel arch bridge\n          '822': steel drum\n          '823': stethoscope\n          '824': stole\n          '825': stone wall\n          '826': stopwatch, stop watch\n          '827': stove\n          '828': strainer\n          '829': streetcar, tram, tramcar, trolley, trolley car\n          '830': stretcher\n          '831': studio couch, day bed\n          '832': stupa, tope\n          '833': submarine, pigboat, sub, U-boat\n          '834': suit, suit of clothes\n          '835': sundial\n          '836': sunglass\n          '837': sunglasses, dark glasses, shades\n          '838': sunscreen, sunblock, sun blocker\n          '839': suspension bridge\n          '840': swab, swob, mop\n          '841': sweatshirt\n          '842': swimming trunks, bathing trunks\n          '843': swing\n          '844': switch, electric switch, electrical switch\n          '845': syringe\n          '846': table lamp\n          '847': tank, army tank, armored combat vehicle, armoured combat vehicle\n          '848': tape player\n          '849': teapot\n          '850': teddy, teddy bear\n          '851': television, television system\n          '852': tennis ball\n          '853': thatch, thatched roof\n          '854': theater curtain, theatre curtain\n          '855': thimble\n          '856': thresher, thrasher, threshing machine\n          '857': throne\n          '858': tile roof\n          '859': toaster\n          '860': tobacco shop, tobacconist shop, tobacconist\n          '861': toilet seat\n          '862': torch\n          '863': totem pole\n          '864': tow truck, tow car, wrecker\n          '865': toyshop\n          '866': tractor\n          '867': trailer truck, tractor trailer, trucking rig, rig, articulated lorry,\n            semi\n          '868': tray\n          '869': trench coat\n          '870': tricycle, trike, velocipede\n          '871': trimaran\n          '872': tripod\n          '873': triumphal arch\n          '874': trolleybus, trolley coach, trackless trolley\n          '875': trombone\n          '876': tub, vat\n          '877': turnstile\n          '878': typewriter keyboard\n          '879': umbrella\n          '880': unicycle, monocycle\n          '881': upright, upright piano\n          '882': vacuum, vacuum cleaner\n          '883': vase\n          '884': vault\n          '885': velvet\n          '886': vending machine\n          '887': vestment\n          '888': viaduct\n          '889': violin, fiddle\n          '890': volleyball\n          '891': waffle iron\n          '892': wall clock\n          '893': wallet, billfold, notecase, pocketbook\n          '894': wardrobe, closet, press\n          '895': warplane, military plane\n          '896': washbasin, handbasin, washbowl, lavabo, wash-hand basin\n          '897': washer, automatic washer, washing machine\n          '898': water bottle\n          '899': water jug\n          '900': water tower\n          '901': whiskey jug\n          '902': whistle\n          '903': wig\n          '904': window screen\n          '905': window shade\n          '906': Windsor tie\n          '907': wine bottle\n          '908': wing\n          '909': wok\n          '910': wooden spoon\n          '911': wool, woolen, woollen\n          '912': worm fence, snake fence, snake-rail fence, Virginia fence\n          '913': wreck\n          '914': yawl\n          '915': yurt\n          '916': web site, website, internet site, site\n          '917': comic book\n          '918': crossword puzzle, crossword\n          '919': street sign\n          '920': traffic light, traffic signal, stoplight\n          '921': book jacket, dust cover, dust jacket, dust wrapper\n          '922': menu\n          '923': plate\n          '924': guacamole\n          '925': consomme\n          '926': hot pot, hotpot\n          '927': trifle\n          '928': ice cream, icecream\n          '929': ice lolly, lolly, lollipop, popsicle\n          '930': French loaf\n          '931': bagel, beigel\n          '932': pretzel\n          '933': cheeseburger\n          '934': hotdog, hot dog, red hot\n          '935': mashed potato\n          '936': head cabbage\n          '937': broccoli\n          '938': cauliflower\n          '939': zucchini, courgette\n          '940': spaghetti squash\n          '941': acorn squash\n          '942': butternut squash\n          '943': cucumber, cuke\n          '944': artichoke, globe artichoke\n          '945': bell pepper\n          '946': cardoon\n          '947': mushroom\n          '948': Granny Smith\n          '949': strawberry\n          '950': orange\n          '951': lemon\n          '952': fig\n          '953': pineapple, ananas\n          '954': banana\n          '955': jackfruit, jak, jack\n          '956': custard apple\n          '957': pomegranate\n          '958': hay\n          '959': carbonara\n          '960': chocolate sauce, chocolate syrup\n          '961': dough\n          '962': meat loaf, meatloaf\n          '963': pizza, pizza pie\n          '964': potpie\n          '965': burrito\n          '966': red wine\n          '967': espresso\n          '968': cup\n          '969': eggnog\n          '970': alp\n          '971': bubble\n          '972': cliff, drop, drop-off\n          '973': coral reef\n          '974': geyser\n          '975': lakeside, lakeshore\n          '976': promontory, headland, head, foreland\n          '977': sandbar, sand bar\n          '978': seashore, coast, seacoast, sea-coast\n          '979': valley, vale\n          '980': volcano\n          '981': ballplayer, baseball player\n          '982': groom, bridegroom\n          '983': scuba diver\n          '984': rapeseed\n          '985': daisy\n          '986': yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus,\n            Cypripedium parviflorum\n          '987': corn\n          '988': acorn\n          '989': hip, rose hip, rosehip\n          '990': buckeye, horse chestnut, conker\n          '991': coral fungus\n          '992': agaric\n          '993': gyromitra\n          '994': stinkhorn, carrion fungus\n          '995': earthstar\n          '996': hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola\n            frondosa\n          '997': bolete\n          '998': ear, spike, capitulum\n          '999': toilet tissue, toilet paper, bathroom tissue\n          '1000': none\n  splits:\n  - name: train\n    num_bytes: 19891500949.711\n    num_examples: 1281167\n  - name: validation\n    num_bytes: 708730650.0\n    num_examples: 50000\n  - name: test\n    num_bytes: 1418148200.0\n    num_examples: 100000\n  download_size: 19759304051\n  dataset_size: 22018379799.711\nconfigs:\n- config_name: default\n  data_files:\n  - split: train\n    path: data/train-*\n  - split: validation\n    path: data/validation-*\n  - split: test\n    path: data/test-*\n---\n\n# Repack Information\n\nThis repository contains a complete repack of [ILSVRC/imagenet-1k](https://huggingface.co/datasets/ILSVRC/imagenet-1k/) in Parquet format with the following data transformations:\n\n1. Images were center-cropped to square to the minimum height/width dimension.\n2. Images were then rescaled to 256x256 using Lanczos resampling.\n\n# Dataset Card for ImageNet\n\n## Table of Contents\n- [Dataset Description](#dataset-description)\n  - [Dataset Summary](#dataset-summary)\n  - [Supported Tasks](#supported-tasks-and-leaderboards)\n  - [Languages](#languages)\n- [Dataset Structure](#dataset-structure)\n  - [Data Instances](#data-instances)\n  - [Data Fields](#data-fields)\n  - [Data Splits](#data-splits)\n- [Dataset Creation](#dataset-creation)\n  - [Curation Rationale](#curation-rationale)\n  - [Source Data](#source-data)\n  - [Annotations](#annotations)\n  - [Personal and Sensitive Information](#personal-and-sensitive-information)\n- [Considerations for Using the Data](#considerations-for-using-the-data)\n  - [Social Impact of Dataset](#social-impact-of-dataset)\n  - [Discussion of Biases](#discussion-of-biases)\n  - [Other Known Limitations](#other-known-limitations)\n- [Additional Information](#additional-information)\n  - [Dataset Curators](#dataset-curators)\n  - [Licensing Information](#licensing-information)\n  - [Citation Information](#citation-information)\n  - [Contributions](#contributions)\n\n## Dataset Description\n\n- **Homepage:** https://image-net.org/index.php\n- **Repository:**\n- **Paper:** https://arxiv.org/abs/1409.0575\n- **Leaderboard:** https://paperswithcode.com/sota/image-classification-on-imagenet?tag_filter=171\n- **Point of Contact:** mailto: imagenet.help.desk@gmail.com  \n\n### Dataset Summary\n\nILSVRC 2012, commonly known as 'ImageNet' is an image dataset organized according to the WordNet hierarchy. Each meaningful concept in WordNet, possibly described by multiple words or word phrases, is called a \"synonym set\" or \"synset\". There are more than 100,000 synsets in WordNet, majority of them are nouns (80,000+). ImageNet aims to provide on average 1000 images to illustrate each synset. Images of each concept are quality-controlled and human-annotated.\n\n💡 This dataset provides access to ImageNet (ILSVRC) 2012 which is the most commonly used **subset** of ImageNet. This dataset spans 1000 object classes and contains 1,281,167 training images, 50,000 validation images and 100,000 test images. The version also has the [patch](https://drive.google.com/file/d/16RYnHpVOW0XKCsn3G3S9GTHUyoV2-4WX/view) which fixes some of the corrupted test set images already applied. For full ImageNet dataset presented in [[2]](https://ieeexplore.ieee.org/abstract/document/5206848), please check the download section of the [main website](https://image-net.org/download-images.php).\n\n### Supported Tasks and Leaderboards\n\n- `image-classification`: The goal of this task is to classify a given image into one of 1000 ImageNet classes. The leaderboard is available [here](https://paperswithcode.com/sota/image-classification-on-imagenet?tag_filter=171).\n\nTo evaluate the `imagenet-classification` accuracy on the test split, one must first create an account at https://image-net.org. This account must be approved by the site administrator. After the account is created, one can submit the results to the test server at https://image-net.org/challenges/LSVRC/eval_server.php The submission consists of several ASCII text files corresponding to multiple tasks. The task of interest is \"Classification submission (top-5 cls error)\". A sample of an exported text file looks like the following:\n\n```\n670 778 794 387 650\n217 691 564 909 364\n737 369 430 531 124\n755 930 755 512 152\n```\n\nThe export format is described in full in \"readme.txt\" within the 2013 development kit available here: https://image-net.org/data/ILSVRC/2013/ILSVRC2013_devkit.tgz. Please see the section entitled \"3.3 CLS-LOC submission format\". Briefly, the format of the text file is 100,000 lines corresponding to each image in the test split. Each line of integers correspond to the rank-ordered, top 5 predictions for each test image. The integers are 1-indexed corresponding to the line number in the corresponding labels file. See `imagenet2012_labels.txt`.\n\n### Languages\n\nThe class labels in the dataset are in English.\n\n## Dataset Structure\n\n### Data Instances\n\nAn example looks like below: \n\n```\n{\n  'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=384x512 at 0x276021C5EB8>,\n  'label': 23\n}\n```\n\n### Data Fields\n\nThe data instances have the following fields:\n\n- `image`: A `PIL.Image.Image` object containing the image. Note that when accessing the image column: `dataset[0][\"image\"]` the image file is automatically decoded. Decoding of a large number of image files might take a significant amount of time. Thus it is important to first query the sample index before the `\"image\"` column, *i.e.* `dataset[0][\"image\"]` should **always** be preferred over `dataset[\"image\"][0]`.\n- `label`: an `int` classification label. -1 for `test` set as the labels are missing.\n\nThe labels are indexed based on a sorted list of synset ids such as `n07565083` which we automatically map to original class names. The original dataset is divided into folders based on these synset ids. To get a mapping from original synset names, use the file [LOC_synset_mapping.txt](https://www.kaggle.com/competitions/imagenet-object-localization-challenge/data?select=LOC_synset_mapping.txt) available on Kaggle challenge page. You can also use `dataset_instance.features[\"labels\"].int2str` function to get the class for a particular label index. Also note that, labels for test set are returned as -1 as they are missing.\n\n<details>\n  <summary>\n  Click here to see the full list of ImageNet class labels mapping:\n  </summary>\n\n  |id|Class|\n  |--|-----|\n  |0 | tench, Tinca tinca|\n  |1 | goldfish, Carassius auratus|\n  |2 | great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias|\n  |3 | tiger shark, Galeocerdo cuvieri|\n  |4 | hammerhead, hammerhead shark|\n  |5 | electric ray, crampfish, numbfish, torpedo|\n  |6 | stingray|\n  |7 | cock|\n  |8 | hen|\n  |9 | ostrich, Struthio camelus|\n  |10 | brambling, Fringilla montifringilla|\n  |11 | goldfinch, Carduelis carduelis|\n  |12 | house finch, linnet, Carpodacus mexicanus|\n  |13 | junco, snowbird|\n  |14 | indigo bunting, indigo finch, indigo bird, Passerina cyanea|\n  |15 | robin, American robin, Turdus migratorius|\n  |16 | bulbul|\n  |17 | jay|\n  |18 | magpie|\n  |19 | chickadee|\n  |20 | water ouzel, dipper|\n  |21 | kite|\n  |22 | bald eagle, American eagle, Haliaeetus leucocephalus|\n  |23 | vulture|\n  |24 | great grey owl, great gray owl, Strix nebulosa|\n  |25 | European fire salamander, Salamandra salamandra|\n  |26 | common newt, Triturus vulgaris|\n  |27 | eft|\n  |28 | spotted salamander, Ambystoma maculatum|\n  |29 | axolotl, mud puppy, Ambystoma mexicanum|\n  |30 | bullfrog, Rana catesbeiana|\n  |31 | tree frog, tree-frog|\n  |32 | tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui|\n  |33 | loggerhead, loggerhead turtle, Caretta caretta|\n  |34 | leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea|\n  |35 | mud turtle|\n  |36 | terrapin|\n  |37 | box turtle, box tortoise|\n  |38 | banded gecko|\n  |39 | common iguana, iguana, Iguana iguana|\n  |40 | American chameleon, anole, Anolis carolinensis|\n  |41 | whiptail, whiptail lizard|\n  |42 | agama|\n  |43 | frilled lizard, Chlamydosaurus kingi|\n  |44 | alligator lizard|\n  |45 | Gila monster, Heloderma suspectum|\n  |46 | green lizard, Lacerta viridis|\n  |47 | African chameleon, Chamaeleo chamaeleon|\n  |48 | Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis|\n  |49 | African crocodile, Nile crocodile, Crocodylus niloticus|\n  |50 | American alligator, Alligator mississipiensis|\n  |51 | triceratops|\n  |52 | thunder snake, worm snake, Carphophis amoenus|\n  |53 | ringneck snake, ring-necked snake, ring snake|\n  |54 | hognose snake, puff adder, sand viper|\n  |55 | green snake, grass snake|\n  |56 | king snake, kingsnake|\n  |57 | garter snake, grass snake|\n  |58 | water snake|\n  |59 | vine snake|\n  |60 | night snake, Hypsiglena torquata|\n  |61 | boa constrictor, Constrictor constrictor|\n  |62 | rock python, rock snake, Python sebae|\n  |63 | Indian cobra, Naja naja|\n  |64 | green mamba|\n  |65 | sea snake|\n  |66 | horned viper, cerastes, sand viper, horned asp, Cerastes cornutus|\n  |67 | diamondback, diamondback rattlesnake, Crotalus adamanteus|\n  |68 | sidewinder, horned rattlesnake, Crotalus cerastes|\n  |69 | trilobite|\n  |70 | harvestman, daddy longlegs, Phalangium opilio|\n  |71 | scorpion|\n  |72 | black and gold garden spider, Argiope aurantia|\n  |73 | barn spider, Araneus cavaticus|\n  |74 | garden spider, Aranea diademata|\n  |75 | black widow, Latrodectus mactans|\n  |76 | tarantula|\n  |77 | wolf spider, hunting spider|\n  |78 | tick|\n  |79 | centipede|\n  |80 | black grouse|\n  |81 | ptarmigan|\n  |82 | ruffed grouse, partridge, Bonasa umbellus|\n  |83 | prairie chicken, prairie grouse, prairie fowl|\n  |84 | peacock|\n  |85 | quail|\n  |86 | partridge|\n  |87 | African grey, African gray, Psittacus erithacus|\n  |88 | macaw|\n  |89 | sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita|\n  |90 | lorikeet|\n  |91 | coucal|\n  |92 | bee eater|\n  |93 | hornbill|\n  |94 | hummingbird|\n  |95 | jacamar|\n  |96 | toucan|\n  |97 | drake|\n  |98 | red-breasted merganser, Mergus serrator|\n  |99 | goose|\n  |100 | black swan, Cygnus atratus|\n  |101 | tusker|\n  |102 | echidna, spiny anteater, anteater|\n  |103 | platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus|\n  |104 | wallaby, brush kangaroo|\n  |105 | koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus|\n  |106 | wombat|\n  |107 | jellyfish|\n  |108 | sea anemone, anemone|\n  |109 | brain coral|\n  |110 | flatworm, platyhelminth|\n  |111 | nematode, nematode worm, roundworm|\n  |112 | conch|\n  |113 | snail|\n  |114 | slug|\n  |115 | sea slug, nudibranch|\n  |116 | chiton, coat-of-mail shell, sea cradle, polyplacophore|\n  |117 | chambered nautilus, pearly nautilus, nautilus|\n  |118 | Dungeness crab, Cancer magister|\n  |119 | rock crab, Cancer irroratus|\n  |120 | fiddler crab|\n  |121 | king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica|\n  |122 | American lobster, Northern lobster, Maine lobster, Homarus americanus|\n  |123 | spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish|\n  |124 | crayfish, crawfish, crawdad, crawdaddy|\n  |125 | hermit crab|\n  |126 | isopod|\n  |127 | white stork, Ciconia ciconia|\n  |128 | black stork, Ciconia nigra|\n  |129 | spoonbill|\n  |130 | flamingo|\n  |131 | little blue heron, Egretta caerulea|\n  |132 | American egret, great white heron, Egretta albus|\n  |133 | bittern|\n  |134 | crane|\n  |135 | limpkin, Aramus pictus|\n  |136 | European gallinule, Porphyrio porphyrio|\n  |137 | American coot, marsh hen, mud hen, water hen, Fulica americana|\n  |138 | bustard|\n  |139 | ruddy turnstone, Arenaria interpres|\n  |140 | red-backed sandpiper, dunlin, Erolia alpina|\n  |141 | redshank, Tringa totanus|\n  |142 | dowitcher|\n  |143 | oystercatcher, oyster catcher|\n  |144 | pelican|\n  |145 | king penguin, Aptenodytes patagonica|\n  |146 | albatross, mollymawk|\n  |147 | grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus|\n  |148 | killer whale, killer, orca, grampus, sea wolf, Orcinus orca|\n  |149 | dugong, Dugong dugon|\n  |150 | sea lion|\n  |151 | Chihuahua|\n  |152 | Japanese spaniel|\n  |153 | Maltese dog, Maltese terrier, Maltese|\n  |154 | Pekinese, Pekingese, Peke|\n  |155 | Shih-Tzu|\n  |156 | Blenheim spaniel|\n  |157 | papillon|\n  |158 | toy terrier|\n  |159 | Rhodesian ridgeback|\n  |160 | Afghan hound, Afghan|\n  |161 | basset, basset hound|\n  |162 | beagle|\n  |163 | bloodhound, sleuthhound|\n  |164 | bluetick|\n  |165 | black-and-tan coonhound|\n  |166 | Walker hound, Walker foxhound|\n  |167 | English foxhound|\n  |168 | redbone|\n  |169 | borzoi, Russian wolfhound|\n  |170 | Irish wolfhound|\n  |171 | Italian greyhound|\n  |172 | whippet|\n  |173 | Ibizan hound, Ibizan Podenco|\n  |174 | Norwegian elkhound, elkhound|\n  |175 | otterhound, otter hound|\n  |176 | Saluki, gazelle hound|\n  |177 | Scottish deerhound, deerhound|\n  |178 | Weimaraner|\n  |179 | Staffordshire bullterrier, Staffordshire bull terrier|\n  |180 | American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier|\n  |181 | Bedlington terrier|\n  |182 | Border terrier|\n  |183 | Kerry blue terrier|\n  |184 | Irish terrier|\n  |185 | Norfolk terrier|\n  |186 | Norwich terrier|\n  |187 | Yorkshire terrier|\n  |188 | wire-haired fox terrier|\n  |189 | Lakeland terrier|\n  |190 | Sealyham terrier, Sealyham|\n  |191 | Airedale, Airedale terrier|\n  |192 | cairn, cairn terrier|\n  |193 | Australian terrier|\n  |194 | Dandie Dinmont, Dandie Dinmont terrier|\n  |195 | Boston bull, Boston terrier|\n  |196 | miniature schnauzer|\n  |197 | giant schnauzer|\n  |198 | standard schnauzer|\n  |199 | Scotch terrier, Scottish terrier, Scottie|\n  |200 | Tibetan terrier, chrysanthemum dog|\n  |201 | silky terrier, Sydney silky|\n  |202 | soft-coated wheaten terrier|\n  |203 | West Highland white terrier|\n  |204 | Lhasa, Lhasa apso|\n  |205 | flat-coated retriever|\n  |206 | curly-coated retriever|\n  |207 | golden retriever|\n  |208 | Labrador retriever|\n  |209 | Chesapeake Bay retriever|\n  |210 | German short-haired pointer|\n  |211 | vizsla, Hungarian pointer|\n  |212 | English setter|\n  |213 | Irish setter, red setter|\n  |214 | Gordon setter|\n  |215 | Brittany spaniel|\n  |216 | clumber, clumber spaniel|\n  |217 | English springer, English springer spaniel|\n  |218 | Welsh springer spaniel|\n  |219 | cocker spaniel, English cocker spaniel, cocker|\n  |220 | Sussex spaniel|\n  |221 | Irish water spaniel|\n  |222 | kuvasz|\n  |223 | schipperke|\n  |224 | groenendael|\n  |225 | malinois|\n  |226 | briard|\n  |227 | kelpie|\n  |228 | komondor|\n  |229 | Old English sheepdog, bobtail|\n  |230 | Shetland sheepdog, Shetland sheep dog, Shetland|\n  |231 | collie|\n  |232 | Border collie|\n  |233 | Bouvier des Flandres, Bouviers des Flandres|\n  |234 | Rottweiler|\n  |235 | German shepherd, German shepherd dog, German police dog, alsatian|\n  |236 | Doberman, Doberman pinscher|\n  |237 | miniature pinscher|\n  |238 | Greater Swiss Mountain dog|\n  |239 | Bernese mountain dog|\n  |240 | Appenzeller|\n  |241 | EntleBucher|\n  |242 | boxer|\n  |243 | bull mastiff|\n  |244 | Tibetan mastiff|\n  |245 | French bulldog|\n  |246 | Great Dane|\n  |247 | Saint Bernard, St Bernard|\n  |248 | Eskimo dog, husky|\n  |249 | malamute, malemute, Alaskan malamute|\n  |250 | Siberian husky|\n  |251 | dalmatian, coach dog, carriage dog|\n  |252 | affenpinscher, monkey pinscher, monkey dog|\n  |253 | basenji|\n  |254 | pug, pug-dog|\n  |255 | Leonberg|\n  |256 | Newfoundland, Newfoundland dog|\n  |257 | Great Pyrenees|\n  |258 | Samoyed, Samoyede|\n  |259 | Pomeranian|\n  |260 | chow, chow chow|\n  |261 | keeshond|\n  |262 | Brabancon griffon|\n  |263 | Pembroke, Pembroke Welsh corgi|\n  |264 | Cardigan, Cardigan Welsh corgi|\n  |265 | toy poodle|\n  |266 | miniature poodle|\n  |267 | standard poodle|\n  |268 | Mexican hairless|\n  |269 | timber wolf, grey wolf, gray wolf, Canis lupus|\n  |270 | white wolf, Arctic wolf, Canis lupus tundrarum|\n  |271 | red wolf, maned wolf, Canis rufus, Canis niger|\n  |272 | coyote, prairie wolf, brush wolf, Canis latrans|\n  |273 | dingo, warrigal, warragal, Canis dingo|\n  |274 | dhole, Cuon alpinus|\n  |275 | African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus|\n  |276 | hyena, hyaena|\n  |277 | red fox, Vulpes vulpes|\n  |278 | kit fox, Vulpes macrotis|\n  |279 | Arctic fox, white fox, Alopex lagopus|\n  |280 | grey fox, gray fox, Urocyon cinereoargenteus|\n  |281 | tabby, tabby cat|\n  |282 | tiger cat|\n  |283 | Persian cat|\n  |284 | Siamese cat, Siamese|\n  |285 | Egyptian cat|\n  |286 | cougar, puma, catamount, mountain lion, painter, panther, Felis concolor|\n  |287 | lynx, catamount|\n  |288 | leopard, Panthera pardus|\n  |289 | snow leopard, ounce, Panthera uncia|\n  |290 | jaguar, panther, Panthera onca, Felis onca|\n  |291 | lion, king of beasts, Panthera leo|\n  |292 | tiger, Panthera tigris|\n  |293 | cheetah, chetah, Acinonyx jubatus|\n  |294 | brown bear, bruin, Ursus arctos|\n  |295 | American black bear, black bear, Ursus americanus, Euarctos americanus|\n  |296 | ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus|\n  |297 | sloth bear, Melursus ursinus, Ursus ursinus|\n  |298 | mongoose|\n  |299 | meerkat, mierkat|\n  |300 | tiger beetle|\n  |301 | ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle|\n  |302 | ground beetle, carabid beetle|\n  |303 | long-horned beetle, longicorn, longicorn beetle|\n  |304 | leaf beetle, chrysomelid|\n  |305 | dung beetle|\n  |306 | rhinoceros beetle|\n  |307 | weevil|\n  |308 | fly|\n  |309 | bee|\n  |310 | ant, emmet, pismire|\n  |311 | grasshopper, hopper|\n  |312 | cricket|\n  |313 | walking stick, walkingstick, stick insect|\n  |314 | cockroach, roach|\n  |315 | mantis, mantid|\n  |316 | cicada, cicala|\n  |317 | leafhopper|\n  |318 | lacewing, lacewing fly|\n  |319 | dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk|\n  |320 | damselfly|\n  |321 | admiral|\n  |322 | ringlet, ringlet butterfly|\n  |323 | monarch, monarch butterfly, milkweed butterfly, Danaus plexippus|\n  |324 | cabbage butterfly|\n  |325 | sulphur butterfly, sulfur butterfly|\n  |326 | lycaenid, lycaenid butterfly|\n  |327 | starfish, sea star|\n  |328 | sea urchin|\n  |329 | sea cucumber, holothurian|\n  |330 | wood rabbit, cottontail, cottontail rabbit|\n  |331 | hare|\n  |332 | Angora, Angora rabbit|\n  |333 | hamster|\n  |334 | porcupine, hedgehog|\n  |335 | fox squirrel, eastern fox squirrel, Sciurus niger|\n  |336 | marmot|\n  |337 | beaver|\n  |338 | guinea pig, Cavia cobaya|\n  |339 | sorrel|\n  |340 | zebra|\n  |341 | hog, pig, grunter, squealer, Sus scrofa|\n  |342 | wild boar, boar, Sus scrofa|\n  |343 | warthog|\n  |344 | hippopotamus, hippo, river horse, Hippopotamus amphibius|\n  |345 | ox|\n  |346 | water buffalo, water ox, Asiatic buffalo, Bubalus bubalis|\n  |347 | bison|\n  |348 | ram, tup|\n  |349 | bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis|\n  |350 | ibex, Capra ibex|\n  |351 | hartebeest|\n  |352 | impala, Aepyceros melampus|\n  |353 | gazelle|\n  |354 | Arabian camel, dromedary, Camelus dromedarius|\n  |355 | llama|\n  |356 | weasel|\n  |357 | mink|\n  |358 | polecat, fitch, foulmart, foumart, Mustela putorius|\n  |359 | black-footed ferret, ferret, Mustela nigripes|\n  |360 | otter|\n  |361 | skunk, polecat, wood pussy|\n  |362 | badger|\n  |363 | armadillo|\n  |364 | three-toed sloth, ai, Bradypus tridactylus|\n  |365 | orangutan, orang, orangutang, Pongo pygmaeus|\n  |366 | gorilla, Gorilla gorilla|\n  |367 | chimpanzee, chimp, Pan troglodytes|\n  |368 | gibbon, Hylobates lar|\n  |369 | siamang, Hylobates syndactylus, Symphalangus syndactylus|\n  |370 | guenon, guenon monkey|\n  |371 | patas, hussar monkey, Erythrocebus patas|\n  |372 | baboon|\n  |373 | macaque|\n  |374 | langur|\n  |375 | colobus, colobus monkey|\n  |376 | proboscis monkey, Nasalis larvatus|\n  |377 | marmoset|\n  |378 | capuchin, ringtail, Cebus capucinus|\n  |379 | howler monkey, howler|\n  |380 | titi, titi monkey|\n  |381 | spider monkey, Ateles geoffroyi|\n  |382 | squirrel monkey, Saimiri sciureus|\n  |383 | Madagascar cat, ring-tailed lemur, Lemur catta|\n  |384 | indri, indris, Indri indri, Indri brevicaudatus|\n  |385 | Indian elephant, Elephas maximus|\n  |386 | African elephant, Loxodonta africana|\n  |387 | lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens|\n  |388 | giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca|\n  |389 | barracouta, snoek|\n  |390 | eel|\n  |391 | coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch|\n  |392 | rock beauty, Holocanthus tricolor|\n  |393 | anemone fish|\n  |394 | sturgeon|\n  |395 | gar, garfish, garpike, billfish, Lepisosteus osseus|\n  |396 | lionfish|\n  |397 | puffer, pufferfish, blowfish, globefish|\n  |398 | abacus|\n  |399 | abaya|\n  |400 | academic gown, academic robe, judge's robe|\n  |401 | accordion, piano accordion, squeeze box|\n  |402 | acoustic guitar|\n  |403 | aircraft carrier, carrier, flattop, attack aircraft carrier|\n  |404 | airliner|\n  |405 | airship, dirigible|\n  |406 | altar|\n  |407 | ambulance|\n  |408 | amphibian, amphibious vehicle|\n  |409 | analog clock|\n  |410 | apiary, bee house|\n  |411 | apron|\n  |412 | ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin|\n  |413 | assault rifle, assault gun|\n  |414 | backpack, back pack, knapsack, packsack, rucksack, haversack|\n  |415 | bakery, bakeshop, bakehouse|\n  |416 | balance beam, beam|\n  |417 | balloon|\n  |418 | ballpoint, ballpoint pen, ballpen, Biro|\n  |419 | Band Aid|\n  |420 | banjo|\n  |421 | bannister, banister, balustrade, balusters, handrail|\n  |422 | barbell|\n  |423 | barber chair|\n  |424 | barbershop|\n  |425 | barn|\n  |426 | barometer|\n  |427 | barrel, cask|\n  |428 | barrow, garden cart, lawn cart, wheelbarrow|\n  |429 | baseball|\n  |430 | basketball|\n  |431 | bassinet|\n  |432 | bassoon|\n  |433 | bathing cap, swimming cap|\n  |434 | bath towel|\n  |435 | bathtub, bathing tub, bath, tub|\n  |436 | beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon|\n  |437 | beacon, lighthouse, beacon light, pharos|\n  |438 | beaker|\n  |439 | bearskin, busby, shako|\n  |440 | beer bottle|\n  |441 | beer glass|\n  |442 | bell cote, bell cot|\n  |443 | bib|\n  |444 | bicycle-built-for-two, tandem bicycle, tandem|\n  |445 | bikini, two-piece|\n  |446 | binder, ring-binder|\n  |447 | binoculars, field glasses, opera glasses|\n  |448 | birdhouse|\n  |449 | boathouse|\n  |450 | bobsled, bobsleigh, bob|\n  |451 | bolo tie, bolo, bola tie, bola|\n  |452 | bonnet, poke bonnet|\n  |453 | bookcase|\n  |454 | bookshop, bookstore, bookstall|\n  |455 | bottlecap|\n  |456 | bow|\n  |457 | bow tie, bow-tie, bowtie|\n  |458 | brass, memorial tablet, plaque|\n  |459 | brassiere, bra, bandeau|\n  |460 | breakwater, groin, groyne, mole, bulwark, seawall, jetty|\n  |461 | breastplate, aegis, egis|\n  |462 | broom|\n  |463 | bucket, pail|\n  |464 | buckle|\n  |465 | bulletproof vest|\n  |466 | bullet train, bullet|\n  |467 | butcher shop, meat market|\n  |468 | cab, hack, taxi, taxicab|\n  |469 | caldron, cauldron|\n  |470 | candle, taper, wax light|\n  |471 | cannon|\n  |472 | canoe|\n  |473 | can opener, tin opener|\n  |474 | cardigan|\n  |475 | car mirror|\n  |476 | carousel, carrousel, merry-go-round, roundabout, whirligig|\n  |477 | carpenter's kit, tool kit|\n  |478 | carton|\n  |479 | car wheel|\n  |480 | cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM|\n  |481 | cassette|\n  |482 | cassette player|\n  |483 | castle|\n  |484 | catamaran|\n  |485 | CD player|\n  |486 | cello, violoncello|\n  |487 | cellular telephone, cellular phone, cellphone, cell, mobile phone|\n  |488 | chain|\n  |489 | chainlink fence|\n  |490 | chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour|\n  |491 | chain saw, chainsaw|\n  |492 | chest|\n  |493 | chiffonier, commode|\n  |494 | chime, bell, gong|\n  |495 | china cabinet, china closet|\n  |496 | Christmas stocking|\n  |497 | church, church building|\n  |498 | cinema, movie theater, movie theatre, movie house, picture palace|\n  |499 | cleaver, meat cleaver, chopper|\n  |500 | cliff dwelling|\n  |501 | cloak|\n  |502 | clog, geta, patten, sabot|\n  |503 | cocktail shaker|\n  |504 | coffee mug|\n  |505 | coffeepot|\n  |506 | coil, spiral, volute, whorl, helix|\n  |507 | combination lock|\n  |508 | computer keyboard, keypad|\n  |509 | confectionery, confectionary, candy store|\n  |510 | container ship, containership, container vessel|\n  |511 | convertible|\n  |512 | corkscrew, bottle screw|\n  |513 | cornet, horn, trumpet, trump|\n  |514 | cowboy boot|\n  |515 | cowboy hat, ten-gallon hat|\n  |516 | cradle|\n  |517 | crane_1|\n  |518 | crash helmet|\n  |519 | crate|\n  |520 | crib, cot|\n  |521 | Crock Pot|\n  |522 | croquet ball|\n  |523 | crutch|\n  |524 | cuirass|\n  |525 | dam, dike, dyke|\n  |526 | desk|\n  |527 | desktop computer|\n  |528 | dial telephone, dial phone|\n  |529 | diaper, nappy, napkin|\n  |530 | digital clock|\n  |531 | digital watch|\n  |532 | dining table, board|\n  |533 | dishrag, dishcloth|\n  |534 | dishwasher, dish washer, dishwashing machine|\n  |535 | disk brake, disc brake|\n  |536 | dock, dockage, docking facility|\n  |537 | dogsled, dog sled, dog sleigh|\n  |538 | dome|\n  |539 | doormat, welcome mat|\n  |540 | drilling platform, offshore rig|\n  |541 | drum, membranophone, tympan|\n  |542 | drumstick|\n  |543 | dumbbell|\n  |544 | Dutch oven|\n  |545 | electric fan, blower|\n  |546 | electric guitar|\n  |547 | electric locomotive|\n  |548 | entertainment center|\n  |549 | envelope|\n  |550 | espresso maker|\n  |551 | face powder|\n  |552 | feather boa, boa|\n  |553 | file, file cabinet, filing cabinet|\n  |554 | fireboat|\n  |555 | fire engine, fire truck|\n  |556 | fire screen, fireguard|\n  |557 | flagpole, flagstaff|\n  |558 | flute, transverse flute|\n  |559 | folding chair|\n  |560 | football helmet|\n  |561 | forklift|\n  |562 | fountain|\n  |563 | fountain pen|\n  |564 | four-poster|\n  |565 | freight car|\n  |566 | French horn, horn|\n  |567 | frying pan, frypan, skillet|\n  |568 | fur coat|\n  |569 | garbage truck, dustcart|\n  |570 | gasmask, respirator, gas helmet|\n  |571 | gas pump, gasoline pump, petrol pump, island dispenser|\n  |572 | goblet|\n  |573 | go-kart|\n  |574 | golf ball|\n  |575 | golfcart, golf cart|\n  |576 | gondola|\n  |577 | gong, tam-tam|\n  |578 | gown|\n  |579 | grand piano, grand|\n  |580 | greenhouse, nursery, glasshouse|\n  |581 | grille, radiator grille|\n  |582 | grocery store, grocery, food market, market|\n  |583 | guillotine|\n  |584 | hair slide|\n  |585 | hair spray|\n  |586 | half track|\n  |587 | hammer|\n  |588 | hamper|\n  |589 | hand blower, blow dryer, blow drier, hair dryer, hair drier|\n  |590 | hand-held computer, hand-held microcomputer|\n  |591 | handkerchief, hankie, hanky, hankey|\n  |592 | hard disc, hard disk, fixed disk|\n  |593 | harmonica, mouth organ, harp, mouth harp|\n  |594 | harp|\n  |595 | harvester, reaper|\n  |596 | hatchet|\n  |597 | holster|\n  |598 | home theater, home theatre|\n  |599 | honeycomb|\n  |600 | hook, claw|\n  |601 | hoopskirt, crinoline|\n  |602 | horizontal bar, high bar|\n  |603 | horse cart, horse-cart|\n  |604 | hourglass|\n  |605 | iPod|\n  |606 | iron, smoothing iron|\n  |607 | jack-o'-lantern|\n  |608 | jean, blue jean, denim|\n  |609 | jeep, landrover|\n  |610 | jersey, T-shirt, tee shirt|\n  |611 | jigsaw puzzle|\n  |612 | jinrikisha, ricksha, rickshaw|\n  |613 | joystick|\n  |614 | kimono|\n  |615 | knee pad|\n  |616 | knot|\n  |617 | lab coat, laboratory coat|\n  |618 | ladle|\n  |619 | lampshade, lamp shade|\n  |620 | laptop, laptop computer|\n  |621 | lawn mower, mower|\n  |622 | lens cap, lens cover|\n  |623 | letter opener, paper knife, paperknife|\n  |624 | library|\n  |625 | lifeboat|\n  |626 | lighter, light, igniter, ignitor|\n  |627 | limousine, limo|\n  |628 | liner, ocean liner|\n  |629 | lipstick, lip rouge|\n  |630 | Loafer|\n  |631 | lotion|\n  |632 | loudspeaker, speaker, speaker unit, loudspeaker system, speaker system|\n  |633 | loupe, jeweler's loupe|\n  |634 | lumbermill, sawmill|\n  |635 | magnetic compass|\n  |636 | mailbag, postbag|\n  |637 | mailbox, letter box|\n  |638 | maillot|\n  |639 | maillot, tank suit|\n  |640 | manhole cover|\n  |641 | maraca|\n  |642 | marimba, xylophone|\n  |643 | mask|\n  |644 | matchstick|\n  |645 | maypole|\n  |646 | maze, labyrinth|\n  |647 | measuring cup|\n  |648 | medicine chest, medicine cabinet|\n  |649 | megalith, megalithic structure|\n  |650 | microphone, mike|\n  |651 | microwave, microwave oven|\n  |652 | military uniform|\n  |653 | milk can|\n  |654 | minibus|\n  |655 | miniskirt, mini|\n  |656 | minivan|\n  |657 | missile|\n  |658 | mitten|\n  |659 | mixing bowl|\n  |660 | mobile home, manufactured home|\n  |661 | Model T|\n  |662 | modem|\n  |663 | monastery|\n  |664 | monitor|\n  |665 | moped|\n  |666 | mortar|\n  |667 | mortarboard|\n  |668 | mosque|\n  |669 | mosquito net|\n  |670 | motor scooter, scooter|\n  |671 | mountain bike, all-terrain bike, off-roader|\n  |672 | mountain tent|\n  |673 | mouse, computer mouse|\n  |674 | mousetrap|\n  |675 | moving van|\n  |676 | muzzle|\n  |677 | nail|\n  |678 | neck brace|\n  |679 | necklace|\n  |680 | nipple|\n  |681 | notebook, notebook computer|\n  |682 | obelisk|\n  |683 | oboe, hautboy, hautbois|\n  |684 | ocarina, sweet potato|\n  |685 | odometer, hodometer, mileometer, milometer|\n  |686 | oil filter|\n  |687 | organ, pipe organ|\n  |688 | oscilloscope, scope, cathode-ray oscilloscope, CRO|\n  |689 | overskirt|\n  |690 | oxcart|\n  |691 | oxygen mask|\n  |692 | packet|\n  |693 | paddle, boat paddle|\n  |694 | paddlewheel, paddle wheel|\n  |695 | padlock|\n  |696 | paintbrush|\n  |697 | pajama, pyjama, pj's, jammies|\n  |698 | palace|\n  |699 | panpipe, pandean pipe, syrinx|\n  |700 | paper towel|\n  |701 | parachute, chute|\n  |702 | parallel bars, bars|\n  |703 | park bench|\n  |704 | parking meter|\n  |705 | passenger car, coach, carriage|\n  |706 | patio, terrace|\n  |707 | pay-phone, pay-station|\n  |708 | pedestal, plinth, footstall|\n  |709 | pencil box, pencil case|\n  |710 | pencil sharpener|\n  |711 | perfume, essence|\n  |712 | Petri dish|\n  |713 | photocopier|\n  |714 | pick, plectrum, plectron|\n  |715 | pickelhaube|\n  |716 | picket fence, paling|\n  |717 | pickup, pickup truck|\n  |718 | pier|\n  |719 | piggy bank, penny bank|\n  |720 | pill bottle|\n  |721 | pillow|\n  |722 | ping-pong ball|\n  |723 | pinwheel|\n  |724 | pirate, pirate ship|\n  |725 | pitcher, ewer|\n  |726 | plane, carpenter's plane, woodworking plane|\n  |727 | planetarium|\n  |728 | plastic bag|\n  |729 | plate rack|\n  |730 | plow, plough|\n  |731 | plunger, plumber's helper|\n  |732 | Polaroid camera, Polaroid Land camera|\n  |733 | pole|\n  |734 | police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria|\n  |735 | poncho|\n  |736 | pool table, billiard table, snooker table|\n  |737 | pop bottle, soda bottle|\n  |738 | pot, flowerpot|\n  |739 | potter's wheel|\n  |740 | power drill|\n  |741 | prayer rug, prayer mat|\n  |742 | printer|\n  |743 | prison, prison house|\n  |744 | projectile, missile|\n  |745 | projector|\n  |746 | puck, hockey puck|\n  |747 | punching bag, punch bag, punching ball, punchball|\n  |748 | purse|\n  |749 | quill, quill pen|\n  |750 | quilt, comforter, comfort, puff|\n  |751 | racer, race car, racing car|\n  |752 | racket, racquet|\n  |753 | radiator|\n  |754 | radio, wireless|\n  |755 | radio telescope, radio reflector|\n  |756 | rain barrel|\n  |757 | recreational vehicle, RV, R.V.|\n  |758 | reel|\n  |759 | reflex camera|\n  |760 | refrigerator, icebox|\n  |761 | remote control, remote|\n  |762 | restaurant, eating house, eating place, eatery|\n  |763 | revolver, six-gun, six-shooter|\n  |764 | rifle|\n  |765 | rocking chair, rocker|\n  |766 | rotisserie|\n  |767 | rubber eraser, rubber, pencil eraser|\n  |768 | rugby ball|\n  |769 | rule, ruler|\n  |770 | running shoe|\n  |771 | safe|\n  |772 | safety pin|\n  |773 | saltshaker, salt shaker|\n  |774 | sandal|\n  |775 | sarong|\n  |776 | sax, saxophone|\n  |777 | scabbard|\n  |778 | scale, weighing machine|\n  |779 | school bus|\n  |780 | schooner|\n  |781 | scoreboard|\n  |782 | screen, CRT screen|\n  |783 | screw|\n  |784 | screwdriver|\n  |785 | seat belt, seatbelt|\n  |786 | sewing machine|\n  |787 | shield, buckler|\n  |788 | shoe shop, shoe-shop, shoe store|\n  |789 | shoji|\n  |790 | shopping basket|\n  |791 | shopping cart|\n  |792 | shovel|\n  |793 | shower cap|\n  |794 | shower curtain|\n  |795 | ski|\n  |796 | ski mask|\n  |797 | sleeping bag|\n  |798 | slide rule, slipstick|\n  |799 | sliding door|\n  |800 | slot, one-armed bandit|\n  |801 | snorkel|\n  |802 | snowmobile|\n  |803 | snowplow, snowplough|\n  |804 | soap dispenser|\n  |805 | soccer ball|\n  |806 | sock|\n  |807 | solar dish, solar collector, solar furnace|\n  |808 | sombrero|\n  |809 | soup bowl|\n  |810 | space bar|\n  |811 | space heater|\n  |812 | space shuttle|\n  |813 | spatula|\n  |814 | speedboat|\n  |815 | spider web, spider's web|\n  |816 | spindle|\n  |817 | sports car, sport car|\n  |818 | spotlight, spot|\n  |819 | stage|\n  |820 | steam locomotive|\n  |821 | steel arch bridge|\n  |822 | steel drum|\n  |823 | stethoscope|\n  |824 | stole|\n  |825 | stone wall|\n  |826 | stopwatch, stop watch|\n  |827 | stove|\n  |828 | strainer|\n  |829 | streetcar, tram, tramcar, trolley, trolley car|\n  |830 | stretcher|\n  |831 | studio couch, day bed|\n  |832 | stupa, tope|\n  |833 | submarine, pigboat, sub, U-boat|\n  |834 | suit, suit of clothes|\n  |835 | sundial|\n  |836 | sunglass|\n  |837 | sunglasses, dark glasses, shades|\n  |838 | sunscreen, sunblock, sun blocker|\n  |839 | suspension bridge|\n  |840 | swab, swob, mop|\n  |841 | sweatshirt|\n  |842 | swimming trunks, bathing trunks|\n  |843 | swing|\n  |844 | switch, electric switch, electrical switch|\n  |845 | syringe|\n  |846 | table lamp|\n  |847 | tank, army tank, armored combat vehicle, armoured combat vehicle|\n  |848 | tape player|\n  |849 | teapot|\n  |850 | teddy, teddy bear|\n  |851 | television, television system|\n  |852 | tennis ball|\n  |853 | thatch, thatched roof|\n  |854 | theater curtain, theatre curtain|\n  |855 | thimble|\n  |856 | thresher, thrasher, threshing machine|\n  |857 | throne|\n  |858 | tile roof|\n  |859 | toaster|\n  |860 | tobacco shop, tobacconist shop, tobacconist|\n  |861 | toilet seat|\n  |862 | torch|\n  |863 | totem pole|\n  |864 | tow truck, tow car, wrecker|\n  |865 | toyshop|\n  |866 | tractor|\n  |867 | trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi|\n  |868 | tray|\n  |869 | trench coat|\n  |870 | tricycle, trike, velocipede|\n  |871 | trimaran|\n  |872 | tripod|\n  |873 | triumphal arch|\n  |874 | trolleybus, trolley coach, trackless trolley|\n  |875 | trombone|\n  |876 | tub, vat|\n  |877 | turnstile|\n  |878 | typewriter keyboard|\n  |879 | umbrella|\n  |880 | unicycle, monocycle|\n  |881 | upright, upright piano|\n  |882 | vacuum, vacuum cleaner|\n  |883 | vase|\n  |884 | vault|\n  |885 | velvet|\n  |886 | vending machine|\n  |887 | vestment|\n  |888 | viaduct|\n  |889 | violin, fiddle|\n  |890 | volleyball|\n  |891 | waffle iron|\n  |892 | wall clock|\n  |893 | wallet, billfold, notecase, pocketbook|\n  |894 | wardrobe, closet, press|\n  |895 | warplane, military plane|\n  |896 | washbasin, handbasin, washbowl, lavabo, wash-hand basin|\n  |897 | washer, automatic washer, washing machine|\n  |898 | water bottle|\n  |899 | water jug|\n  |900 | water tower|\n  |901 | whiskey jug|\n  |902 | whistle|\n  |903 | wig|\n  |904 | window screen|\n  |905 | window shade|\n  |906 | Windsor tie|\n  |907 | wine bottle|\n  |908 | wing|\n  |909 | wok|\n  |910 | wooden spoon|\n  |911 | wool, woolen, woollen|\n  |912 | worm fence, snake fence, snake-rail fence, Virginia fence|\n  |913 | wreck|\n  |914 | yawl|\n  |915 | yurt|\n  |916 | web site, website, internet site, site|\n  |917 | comic book|\n  |918 | crossword puzzle, crossword|\n  |919 | street sign|\n  |920 | traffic light, traffic signal, stoplight|\n  |921 | book jacket, dust cover, dust jacket, dust wrapper|\n  |922 | menu|\n  |923 | plate|\n  |924 | guacamole|\n  |925 | consomme|\n  |926 | hot pot, hotpot|\n  |927 | trifle|\n  |928 | ice cream, icecream|\n  |929 | ice lolly, lolly, lollipop, popsicle|\n  |930 | French loaf|\n  |931 | bagel, beigel|\n  |932 | pretzel|\n  |933 | cheeseburger|\n  |934 | hotdog, hot dog, red hot|\n  |935 | mashed potato|\n  |936 | head cabbage|\n  |937 | broccoli|\n  |938 | cauliflower|\n  |939 | zucchini, courgette|\n  |940 | spaghetti squash|\n  |941 | acorn squash|\n  |942 | butternut squash|\n  |943 | cucumber, cuke|\n  |944 | artichoke, globe artichoke|\n  |945 | bell pepper|\n  |946 | cardoon|\n  |947 | mushroom|\n  |948 | Granny Smith|\n  |949 | strawberry|\n  |950 | orange|\n  |951 | lemon|\n  |952 | fig|\n  |953 | pineapple, ananas|\n  |954 | banana|\n  |955 | jackfruit, jak, jack|\n  |956 | custard apple|\n  |957 | pomegranate|\n  |958 | hay|\n  |959 | carbonara|\n  |960 | chocolate sauce, chocolate syrup|\n  |961 | dough|\n  |962 | meat loaf, meatloaf|\n  |963 | pizza, pizza pie|\n  |964 | potpie|\n  |965 | burrito|\n  |966 | red wine|\n  |967 | espresso|\n  |968 | cup|\n  |969 | eggnog|\n  |970 | alp|\n  |971 | bubble|\n  |972 | cliff, drop, drop-off|\n  |973 | coral reef|\n  |974 | geyser|\n  |975 | lakeside, lakeshore|\n  |976 | promontory, headland, head, foreland|\n  |977 | sandbar, sand bar|\n  |978 | seashore, coast, seacoast, sea-coast|\n  |979 | valley, vale|\n  |980 | volcano|\n  |981 | ballplayer, baseball player|\n  |982 | groom, bridegroom|\n  |983 | scuba diver|\n  |984 | rapeseed|\n  |985 | daisy|\n  |986 | yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum|\n  |987 | corn|\n  |988 | acorn|\n  |989 | hip, rose hip, rosehip|\n  |990 | buckeye, horse chestnut, conker|\n  |991 | coral fungus|\n  |992 | agaric|\n  |993 | gyromitra|\n  |994 | stinkhorn, carrion fungus|\n  |995 | earthstar|\n  |996 | hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa|\n  |997 | bolete|\n  |998 | ear, spike, capitulum|\n  |999 | toilet tissue, toilet paper, bathroom tissue|\n</details>\n\n### Data Splits\n\n|             |train  |validation| test  |\n|-------------|------:|---------:|------:|\n|# of examples|1281167|50000     |100000 |\n\n## Dataset Creation\n\n### Curation Rationale\n\nThe ImageNet project was inspired by two important needs in computer vision research. The first was the need to establish a clear North Star problem in computer vision. While the field enjoyed an abundance of important tasks to work on, from stereo vision to image retrieval, from 3D reconstruction to image segmentation, object categorization was recognized to be one of the most fundamental capabilities of both human and machine vision. Hence there was a growing demand for a high quality object categorization benchmark with clearly established evaluation metrics. Second, there was a critical need for more data to enable more generalizable machine learning methods. Ever since the birth of the digital era and the availability of web-scale data exchanges, researchers in these fields have been working hard to design more and more sophisticated algorithms to index, retrieve, organize and annotate multimedia data. But good research requires good resources. To tackle this problem at scale (think of your growing personal collection of digital images, or videos, or a commercial web search engine’s database), it was critical to provide researchers with a large-scale image database for both training and testing. The convergence of these two intellectual reasons motivated us to build ImageNet.\n\n### Source Data\n\n#### Initial Data Collection and Normalization\n\nInitial data for ImageNet image classification task consists of photographs collected from [Flickr](https://www.flickr.com) and other search engines, manually labeled with the presence of one of 1000 object categories. Constructing ImageNet was an effort to scale up an image classification dataset to cover most nouns in English using tens of millions of manually verified photographs [1](https://ieeexplore.ieee.org/abstract/document/5206848). The image classification task of ILSVRC came as a direct extension of this effort. A subset of categories and images was chosen and fixed to provide a standardized benchmark while the rest of ImageNet continued to grow.\n\n#### Who are the source language producers?\n\nWordNet synsets further quality controlled by human annotators. The images are from Flickr.\n\n### Annotations\n\n#### Annotation process\n\nThe annotation process of collecting ImageNet for image classification task is a three step process.\n\n1. Defining the 1000 object categories for the image classification task. These categories have evolved over the years.\n1. Collecting the candidate image for these object categories using a search engine.\n1. Quality control on the candidate images by using human annotators on Amazon Mechanical Turk (AMT) to make sure the image has the synset it was collected for.\n\nSee the section 3.1 in [1](https://arxiv.org/abs/1409.0575) for more details on data collection procedure and [2](https://ieeexplore.ieee.org/abstract/document/5206848) for general information on ImageNet.\n\n#### Who are the annotators?\n\nImages are automatically fetched from an image search engine based on the synsets and filtered using human annotators on Amazon Mechanical Turk. See [1](https://arxiv.org/abs/1409.0575) for more details.\n\n### Personal and Sensitive Information\n\nThe 1,000 categories selected for this subset contain only 3 people categories (scuba diver, bridegroom, and baseball player) while the full ImageNet contains 2,832 people categories under the person subtree (accounting for roughly 8.3% of the total images). This subset does contain the images of people without their consent. Though, the study in [[1]](https://image-net.org/face-obfuscation/) on obfuscating faces of the people in the ImageNet 2012 subset shows that blurring people's faces causes a very minor decrease in accuracy (~0.6%) suggesting that privacy-aware models can be trained on ImageNet. On larger ImageNet, there has been [an attempt](https://arxiv.org/abs/1912.07726) at filtering and balancing the people subtree in the larger ImageNet.\n\n## Considerations for Using the Data\n\n### Social Impact of Dataset\n\nThe ImageNet dataset has been very crucial in advancement of deep learning technology as being the standard benchmark for the computer vision models. The dataset aims to probe models on their understanding of the objects and has become the de-facto dataset for this purpose. ImageNet is still one of the major datasets on which models are evaluated for their generalization in computer vision capabilities as the field moves towards self-supervised algorithms. Please see the future section in [1](https://arxiv.org/abs/1409.0575) for a discussion on social impact of the dataset.\n\n### Discussion of Biases\n\n1. A [study](https://image-net.org/update-sep-17-2019.php) of the history of the multiple layers (taxonomy, object classes and labeling) of ImageNet and WordNet in 2019 described how bias is deeply embedded in most classification approaches for of all sorts of images.\n1. A [study](https://arxiv.org/abs/1811.12231) has also shown that ImageNet trained models are biased towards texture rather than shapes which in contrast with how humans do object classification. Increasing the shape bias improves the accuracy and robustness.\n1. Another [study](https://arxiv.org/abs/2109.13228) more potential issues and biases with the ImageNet dataset and provides an alternative benchmark for image classification task. The data collected contains humans without their consent.\n1. ImageNet data with face obfuscation is also provided at [this link](https://image-net.org/face-obfuscation/)\n1. A study on genealogy of ImageNet is can be found at [this link](https://journals.sagepub.com/doi/full/10.1177/20539517211035955) about the \"norms, values, and assumptions\" in ImageNet.\n1. See [this study](https://arxiv.org/abs/1912.07726) on filtering and balancing the distribution of people subtree in the larger complete ImageNet.\n\n### Other Known Limitations\n\n1. Since most of the images were collected from internet, keep in mind that some images in ImageNet might be subject to copyrights. See the following papers for more details: [[1]](https://arxiv.org/abs/2109.13228) [[2]](https://arxiv.org/abs/1409.0575) [[3]](https://ieeexplore.ieee.org/abstract/document/5206848).\n\n## Additional Information\n\n### Dataset Curators\n\nAuthors of [[1]](https://arxiv.org/abs/1409.0575) and [[2]](https://ieeexplore.ieee.org/abstract/document/5206848):\n\n- Olga Russakovsky\n- Jia Deng\n- Hao Su\n- Jonathan Krause\n- Sanjeev Satheesh\n- Wei Dong\n- Richard Socher\n- Li-Jia Li\n- Kai Li\n- Sean Ma\n- Zhiheng Huang\n- Andrej Karpathy\n- Aditya Khosla\n- Michael Bernstein\n- Alexander C Berg\n- Li Fei-Fei\n\n### Licensing Information\n\nIn exchange for permission to use the ImageNet database (the \"Database\") at Princeton University and Stanford University, Researcher hereby agrees to the following terms and conditions:\n\n1. Researcher shall use the Database only for non-commercial research and educational purposes.\n1. Princeton University and Stanford University make no representations or warranties regarding the Database, including but not limited to warranties of non-infringement or fitness for a particular purpose.\n1. Researcher accepts full responsibility for his or her use of the Database and shall defend and indemnify the ImageNet team, Princeton University, and Stanford University, including their employees, Trustees, officers and agents, against any and all claims arising from Researcher's use of the Database, including but not limited to Researcher's use of any copies of copyrighted images that he or she may create from the Database.\n1. Researcher may provide research associates and colleagues with access to the Database provided that they first agree to be bound by these terms and conditions.\n1. Princeton University and Stanford University reserve the right to terminate Researcher's access to the Database at any time.\n1. If Researcher is employed by a for-profit, commercial entity, Researcher's employer shall also be bound by these terms and conditions, and Researcher hereby represents that he or she is fully authorized to enter into this agreement on behalf of such employer.\n1. The law of the State of New Jersey shall apply to all disputes under this agreement.\n\n### Citation Information\n\n```bibtex\n@article{imagenet15russakovsky,\n    Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},\n    Title = { {ImageNet Large Scale Visual Recognition Challenge} },\n    Year = {2015},\n    journal   = {International Journal of Computer Vision (IJCV)},\n    doi = {10.1007/s11263-015-0816-y},\n    volume={115},\n    number={3},\n    pages={211-252}\n}\n```\n\n### Contributions\n\nThanks to [@apsdehal](https://github.com/apsdehal) for adding this dataset."
                }
              ]
            }
          }
        },
        "experiment_runs": [
          {
            "run_id": "proposed-iter1-ViT-Base-86-M-ImageNet-1K",
            "method_name": "proposed",
            "model_name": "ViT-Base (86 M)",
            "dataset_name": "ImageNet-1K"
          },
          {
            "run_id": "comparative-1-iter1-ViT-Base-86-M-ImageNet-1K",
            "method_name": "comparative-1",
            "model_name": "ViT-Base (86 M)",
            "dataset_name": "ImageNet-1K"
          }
        ]
      }
    ]
  }
}