{
  "research_topic": "Optimize Transformer training by incorporating biological perspectives",
  "queries": [
    "bio-inspired transformer",
    "Hebbian learning transformer",
    "spiking neural transformer",
    "biologically plausible attention",
    "synaptic plasticity transformer"
  ],
  "research_study_list": [
    {
      "title": "Analogous to Evolutionary Algorithm: Designing a Unified Sequence Model",
      "abstract": "Inspired by biological evolution, we explain the rationality of Vision Transformer by analogy with the proven practical Evolutionary Algorithm (EA) and derive that both of them have consistent mathematical representation. Analogous to the dynamic local population in EA, we improve the existing transformer structure and propose a more efficient EAT model, and design task-related heads to deal with different tasks more flexibly. Moreover, we introduce the spatial-filling curve into the current vision transformer to sequence image data into a uniform sequential format. Thus we can design a unified EAT framework to address multi-modal tasks, separating the network architecture from the data format adaptation. Our approach achieves state-of-the-art results on the ImageNet classification task compared with recent vision transformer works while having smaller parameters and greater throughput. We further conduct multi-modal tasks to demonstrate the superiority of the unified EAT, e.g., Text-Based Image Retrieval, and our approach improves the rank-1 by +3.7 points over the baseline on the CSS dataset.",
      "meta_data": {
        "arxiv_id": "2105.15089v3",
        "authors": [
          "Jiangning Zhang",
          "Chao Xu",
          "Jian Li",
          "Wenzhou Chen",
          "Yabiao Wang",
          "Ying Tai",
          "Shuo Chen",
          "Chengjie Wang",
          "Feiyue Huang",
          "Yong Liu"
        ],
        "published_date": "2021-05-31T16:20:03Z",
        "pdf_url": "https://arxiv.org/pdf/2105.15089v3.pdf"
      }
    },
    {
      "title": "Spikformer: When Spiking Neural Network Meets Transformer ",
      "abstract": "We consider two biologically plausible structures, the Spiking Neural Network (SNN) and the self-attention mechanism. The former offers an energy-efficient and event-driven paradigm for deep learning, while the latter has the ability to capture feature dependencies, enabling Transformer to achieve good performance. It is intuitively promising to explore the marriage between them. In this paper, we consider leveraging both self-attention capability and biological properties of SNNs, and propose a novel Spiking Self Attention (SSA) as well as a powerful framework, named Spiking Transformer (Spikformer). The SSA mechanism in Spikformer models the sparse visual feature by using spike-form Query, Key, and Value without softmax. Since its computation is sparse and avoids multiplication, SSA is efficient and has low computational energy consumption. It is shown that Spikformer with SSA can outperform the state-of-the-art SNNs-like frameworks in image classification on both neuromorphic and static datasets. Spikformer (66.3M parameters) with comparable size to SEW-ResNet-152 (60.2M,69.26%) can achieve 74.81% top1 accuracy on ImageNet using 4 time steps, which is the state-of-the-art in directly trained SNNs models.",
      "meta_data": {
        "arxiv_id": "2209.15425v2",
        "authors": [
          "Zhaokun Zhou",
          "Yuesheng Zhu",
          "Chao He",
          "Yaowei Wang",
          "Shuicheng Yan",
          "Yonghong Tian",
          "Li Yuan"
        ],
        "published_date": "2022-09-29T14:16:49Z",
        "pdf_url": "https://arxiv.org/pdf/2209.15425v2.pdf"
      }
    },
    {
      "title": "BIOT: Biosignal Transformer for Cross-data Learning in the Wild",
      "abstract": "Biological signals, such as electroencephalograms (EEG), play a crucial role in numerous clinical applications, exhibiting diverse data formats and quality profiles. Current deep learning models for biosignals are typically specialized for specific datasets and clinical settings, limiting their broader applicability. Motivated by the success of large language models in text processing, we explore the development of foundational models that are trained from multiple data sources and can be fine-tuned on different downstream biosignal tasks.\n  To overcome the unique challenges associated with biosignals of various formats, such as mismatched channels, variable sample lengths, and prevalent missing values, we propose a Biosignal Transformer (\\method). The proposed \\method model can enable cross-data learning with mismatched channels, variable lengths, and missing values by tokenizing diverse biosignals into unified \"biosignal sentences\". Specifically, we tokenize each channel into fixed-length segments containing local signal features, flattening them to form consistent \"sentences\". Channel embeddings and {\\em relative} position embeddings are added to preserve spatio-temporal features.\n  The \\method model is versatile and applicable to various biosignal learning settings across different datasets, including joint pre-training for larger models. Comprehensive evaluations on EEG, electrocardiogram (ECG), and human activity sensory signals demonstrate that \\method outperforms robust baselines in common settings and facilitates learning across multiple datasets with different formats. Use CHB-MIT seizure detection task as an example, our vanilla \\method model shows 3\\% improvement over baselines in balanced accuracy, and the pre-trained \\method models (optimized from other data sources) can further bring up to 4\\% improvements.",
      "meta_data": {
        "arxiv_id": "2305.10351v1",
        "authors": [
          "Chaoqi Yang",
          "M. Brandon Westover",
          "Jimeng Sun"
        ],
        "published_date": "2023-05-10T19:26:58Z",
        "pdf_url": "https://arxiv.org/pdf/2305.10351v1.pdf"
      }
    },
    {
      "title": "Redesigning the Transformer Architecture with Insights from Multi-particle Dynamical Systems",
      "abstract": "The Transformer and its variants have been proven to be efficient sequence learners in many different domains. Despite their staggering success, a critical issue has been the enormous number of parameters that must be trained (ranging from $10^7$ to $10^{11}$) along with the quadratic complexity of dot-product attention. In this work, we investigate the problem of approximating the two central components of the Transformer -- multi-head self-attention and point-wise feed-forward transformation, with reduced parameter space and computational complexity. We build upon recent developments in analyzing deep neural networks as numerical solvers of ordinary differential equations. Taking advantage of an analogy between Transformer stages and the evolution of a dynamical system of multiple interacting particles, we formulate a temporal evolution scheme, TransEvolve, to bypass costly dot-product attention over multiple stacked layers. We perform exhaustive experiments with TransEvolve on well-known encoder-decoder as well as encoder-only tasks. We observe that the degree of approximation (or inversely, the degree of parameter reduction) has different effects on the performance, depending on the task. While in the encoder-decoder regime, TransEvolve delivers performances comparable to the original Transformer, in encoder-only tasks it consistently outperforms Transformer along with several subsequent variants.",
      "meta_data": {
        "arxiv_id": "2109.15142v3",
        "authors": [
          "Subhabrata Dutta",
          "Tanya Gautam",
          "Soumen Chakrabarti",
          "Tanmoy Chakraborty"
        ],
        "published_date": "2021-09-30T14:01:06Z",
        "pdf_url": "https://arxiv.org/pdf/2109.15142v3.pdf"
      }
    },
    {
      "title": "Redesigning the Transformer Architecture with Insights from Multi-particle Dynamical Systems",
      "abstract": "The Transformer and its variants have been proven to be efficient sequence learners in many different domains. Despite their staggering success, a critical issue has been the enormous number of parameters that must be trained (ranging from $10^7$ to $10^{11}$) along with the quadratic complexity of dot-product attention. In this work, we investigate the problem of approximating the two central components of the Transformer -- multi-head self-attention and point-wise feed-forward transformation, with reduced parameter space and computational complexity. We build upon recent developments in analyzing deep neural networks as numerical solvers of ordinary differential equations. Taking advantage of an analogy between Transformer stages and the evolution of a dynamical system of multiple interacting particles, we formulate a temporal evolution scheme, TransEvolve, to bypass costly dot-product attention over multiple stacked layers. We perform exhaustive experiments with TransEvolve on well-known encoder-decoder as well as encoder-only tasks. We observe that the degree of approximation (or inversely, the degree of parameter reduction) has different effects on the performance, depending on the task. While in the encoder-decoder regime, TransEvolve delivers performances comparable to the original Transformer, in encoder-only tasks it consistently outperforms Transformer along with several subsequent variants.",
      "meta_data": {
        "arxiv_id": "2109.15142v3",
        "authors": [
          "Subhabrata Dutta",
          "Tanya Gautam",
          "Soumen Chakrabarti",
          "Tanmoy Chakraborty"
        ],
        "published_date": "2021-09-30T14:01:06Z",
        "pdf_url": "https://arxiv.org/pdf/2109.15142v3.pdf"
      }
    },
    {
      "title": "Hebbian Deep Learning Without Feedback",
      "abstract": "Recent approximations to backpropagation (BP) have mitigated many of BP's computational inefficiencies and incompatibilities with biology, but important limitations still remain. Moreover, the approximations significantly decrease accuracy in benchmarks, suggesting that an entirely different approach may be more fruitful. Here, grounded on recent theory for Hebbian learning in soft winner-take-all networks, we present multilayer SoftHebb, i.e. an algorithm that trains deep neural networks, without any feedback, target, or error signals. As a result, it achieves efficiency by avoiding weight transport, non-local plasticity, time-locking of layer updates, iterative equilibria, and (self-) supervisory or other feedback signals -- which were necessary in other approaches. Its increased efficiency and biological compatibility do not trade off accuracy compared to state-of-the-art bio-plausible learning, but rather improve it. With up to five hidden layers and an added linear classifier, accuracies on MNIST, CIFAR-10, STL-10, and ImageNet, respectively reach 99.4%, 80.3%, 76.2%, and 27.3%. In conclusion, SoftHebb shows with a radically different approach from BP that Deep Learning over few layers may be plausible in the brain and increases the accuracy of bio-plausible machine learning. Code is available at https://github.com/NeuromorphicComputing/SoftHebb.",
      "meta_data": {
        "arxiv_id": "2209.11883v2",
        "authors": [
          "Adrien Journé",
          "Hector Garcia Rodriguez",
          "Qinghai Guo",
          "Timoleon Moraitis"
        ],
        "published_date": "2022-09-23T23:12:59Z",
        "venue": "The Eleventh International Conference on Learning Representations (2023) Retrieved from https://openreview.net/forum?id=8gd4M-_Rj1",
        "pdf_url": "https://arxiv.org/pdf/2209.11883v2.pdf"
      }
    },
    {
      "title": "Hebbian Deep Learning Without Feedback",
      "abstract": "Recent approximations to backpropagation (BP) have mitigated many of BP's computational inefficiencies and incompatibilities with biology, but important limitations still remain. Moreover, the approximations significantly decrease accuracy in benchmarks, suggesting that an entirely different approach may be more fruitful. Here, grounded on recent theory for Hebbian learning in soft winner-take-all networks, we present multilayer SoftHebb, i.e. an algorithm that trains deep neural networks, without any feedback, target, or error signals. As a result, it achieves efficiency by avoiding weight transport, non-local plasticity, time-locking of layer updates, iterative equilibria, and (self-) supervisory or other feedback signals -- which were necessary in other approaches. Its increased efficiency and biological compatibility do not trade off accuracy compared to state-of-the-art bio-plausible learning, but rather improve it. With up to five hidden layers and an added linear classifier, accuracies on MNIST, CIFAR-10, STL-10, and ImageNet, respectively reach 99.4%, 80.3%, 76.2%, and 27.3%. In conclusion, SoftHebb shows with a radically different approach from BP that Deep Learning over few layers may be plausible in the brain and increases the accuracy of bio-plausible machine learning. Code is available at https://github.com/NeuromorphicComputing/SoftHebb.",
      "meta_data": {
        "arxiv_id": "2209.11883v2",
        "authors": [
          "Adrien Journé",
          "Hector Garcia Rodriguez",
          "Qinghai Guo",
          "Timoleon Moraitis"
        ],
        "published_date": "2022-09-23T23:12:59Z",
        "venue": "The Eleventh International Conference on Learning Representations (2023) Retrieved from https://openreview.net/forum?id=8gd4M-_Rj1",
        "pdf_url": "https://arxiv.org/pdf/2209.11883v2.pdf"
      }
    },
    {
      "title": "Unsupervised 3D Object Learning through Neuron Activity aware Plasticity",
      "abstract": "We present an unsupervised deep learning model for 3D object classification. Conventional Hebbian learning, a well-known unsupervised model, suffers from loss of local features leading to reduced performance for tasks with complex geometric objects. We present a deep network with a novel Neuron Activity Aware (NeAW) Hebbian learning rule that dynamically switches the neurons to be governed by Hebbian learning or anti-Hebbian learning, depending on its activity. We analytically show that NeAW Hebbian learning relieves the bias in neuron activity, allowing more neurons to attend to the representation of the 3D objects. Empirical results show that the NeAW Hebbian learning outperforms other variants of Hebbian learning and shows higher accuracy over fully supervised models when training data is limited.",
      "meta_data": {
        "arxiv_id": "2302.11622v1",
        "authors": [
          "Beomseok Kang",
          "Biswadeep Chakraborty",
          "Saibal Mukhopadhyay"
        ],
        "published_date": "2023-02-22T19:57:12Z",
        "pdf_url": "https://arxiv.org/pdf/2302.11622v1.pdf"
      }
    },
    {
      "title": "Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks",
      "abstract": "Neuromorphic computing with spiking neural networks is promising for energy-efficient artificial intelligence (AI) applications. However, different from humans who continually learn different tasks in a lifetime, neural network models suffer from catastrophic forgetting. How could neuronal operations solve this problem is an important question for AI and neuroscience. Many previous studies draw inspiration from observed neuroscience phenomena and propose episodic replay or synaptic metaplasticity, but they are not guaranteed to explicitly preserve knowledge for neuron populations. Other works focus on machine learning methods with more mathematical grounding, e.g., orthogonal projection on high dimensional spaces, but there is no neural correspondence for neuromorphic computing. In this work, we develop a new method with neuronal operations based on lateral connections and Hebbian learning, which can protect knowledge by projecting activity traces of neurons into an orthogonal subspace so that synaptic weight update will not interfere with old tasks. We show that Hebbian and anti-Hebbian learning on recurrent lateral connections can effectively extract the principal subspace of neural activities and enable orthogonal projection. This provides new insights into how neural circuits and Hebbian learning can help continual learning, and also how the concept of orthogonal projection can be realized in neuronal systems. Our method is also flexible to utilize arbitrary training methods based on presynaptic activities/traces. Experiments show that our method consistently solves forgetting for spiking neural networks with nearly zero forgetting under various supervised training methods with different error propagation approaches, and outperforms previous approaches under various settings. Our method can pave a solid path for building continual neuromorphic computing systems.",
      "meta_data": {
        "arxiv_id": "2402.11984v1",
        "authors": [
          "Mingqing Xiao",
          "Qingyan Meng",
          "Zongpeng Zhang",
          "Di He",
          "Zhouchen Lin"
        ],
        "published_date": "2024-02-19T09:29:37Z",
        "pdf_url": "https://arxiv.org/pdf/2402.11984v1.pdf"
      }
    },
    {
      "title": "Outlier-Efficient Hopfield Layers for Large Transformer-Based Models",
      "abstract": "We introduce an Outlier-Efficient Modern Hopfield Model (termed $\\mathrm{OutEffHop}$) and use it to address the outlier inefficiency problem of {training} gigantic transformer-based models. Our main contribution is a novel associative memory model facilitating \\textit{outlier-efficient} associative memory retrievals. Interestingly, this memory model manifests a model-based interpretation of an outlier-efficient attention mechanism (${\\rm Softmax}_1$): it is an approximation of the memory retrieval process of $\\mathrm{OutEffHop}$. Methodologically, this allows us to introduce novel outlier-efficient Hopfield layers as powerful alternatives to traditional attention mechanisms, with superior post-quantization performance. Theoretically, the Outlier-Efficient Modern Hopfield Model retains and improves the desirable properties of standard modern Hopfield models, including fixed point convergence and exponential storage capacity. Empirically, we demonstrate the efficacy of the proposed model across large-scale transformer-based and Hopfield-based models (including BERT, OPT, ViT, and STanHop-Net), benchmarking against state-of-the-art methods like $\\mathtt{Clipped\\_Softmax}$ and $\\mathtt{Gated\\_Attention}$. Notably, $\\mathrm{OutEffHop}$ achieves an average reduction of 22+\\% in average kurtosis and 26+\\% in the maximum infinity norm of model outputs across four models. Code is available at \\href{https://github.com/MAGICS-LAB/OutEffHop}{GitHub}; models are on \\href{https://huggingface.co/collections/magicslabnu/outeffhop-6610fcede8d2cda23009a98f}{Hugging Face Hub}; future updates are on \\href{https://arxiv.org/abs/2404.03828}{arXiv}.",
      "meta_data": {
        "arxiv_id": "2404.03828v2",
        "authors": [
          "Jerry Yao-Chieh Hu",
          "Pei-Hsuan Chang",
          "Robin Luo",
          "Hong-Yu Chen",
          "Weijian Li",
          "Wei-Po Wang",
          "Han Liu"
        ],
        "published_date": "2024-04-04T23:08:43Z",
        "pdf_url": "https://arxiv.org/pdf/2404.03828v2.pdf"
      }
    },
    {
      "title": "Spikformer: When Spiking Neural Network Meets Transformer ",
      "abstract": "We consider two biologically plausible structures, the Spiking Neural Network (SNN) and the self-attention mechanism. The former offers an energy-efficient and event-driven paradigm for deep learning, while the latter has the ability to capture feature dependencies, enabling Transformer to achieve good performance. It is intuitively promising to explore the marriage between them. In this paper, we consider leveraging both self-attention capability and biological properties of SNNs, and propose a novel Spiking Self Attention (SSA) as well as a powerful framework, named Spiking Transformer (Spikformer). The SSA mechanism in Spikformer models the sparse visual feature by using spike-form Query, Key, and Value without softmax. Since its computation is sparse and avoids multiplication, SSA is efficient and has low computational energy consumption. It is shown that Spikformer with SSA can outperform the state-of-the-art SNNs-like frameworks in image classification on both neuromorphic and static datasets. Spikformer (66.3M parameters) with comparable size to SEW-ResNet-152 (60.2M,69.26%) can achieve 74.81% top1 accuracy on ImageNet using 4 time steps, which is the state-of-the-art in directly trained SNNs models.",
      "meta_data": {
        "arxiv_id": "2209.15425v2",
        "authors": [
          "Zhaokun Zhou",
          "Yuesheng Zhu",
          "Chao He",
          "Yaowei Wang",
          "Shuicheng Yan",
          "Yonghong Tian",
          "Li Yuan"
        ],
        "published_date": "2022-09-29T14:16:49Z",
        "pdf_url": "https://arxiv.org/pdf/2209.15425v2.pdf"
      }
    },
    {
      "title": "SpikedAttention: Training-Free and Fully Spike-Driven Transformer-to-SNN Conversion with Winner-Oriented Spike Shift for Softmax Operation"
    },
    {
      "title": "SpikeZIP-TF: Conversion is All You Need for Transformer-based SNN",
      "abstract": "Spiking neural network (SNN) has attracted great attention due to its characteristic of high efficiency and accuracy. Currently, the ANN-to-SNN conversion methods can obtain ANN on-par accuracy SNN with ultra-low latency (8 time-steps) in CNN structure on computer vision (CV) tasks. However, as Transformer-based networks have achieved prevailing precision on both CV and natural language processing (NLP), the Transformer-based SNNs are still encounting the lower accuracy w.r.t the ANN counterparts. In this work, we introduce a novel ANN-to-SNN conversion method called SpikeZIP-TF, where ANN and SNN are exactly equivalent, thus incurring no accuracy degradation. SpikeZIP-TF achieves 83.82% accuracy on CV dataset (ImageNet) and 93.79% accuracy on NLP dataset (SST-2), which are higher than SOTA Transformer-based SNNs. The code is available in GitHub: https://github.com/Intelligent-Computing-Research-Group/SpikeZIP_transformer",
      "meta_data": {
        "arxiv_id": "2406.03470v1",
        "authors": [
          "Kang You",
          "Zekai Xu",
          "Chen Nie",
          "Zhijie Deng",
          "Qinghai Guo",
          "Xiang Wang",
          "Zhezhi He"
        ],
        "published_date": "2024-06-05T17:24:07Z",
        "venue": "International Conference on Machine Learning 2024",
        "pdf_url": "https://arxiv.org/pdf/2406.03470v1.pdf"
      }
    },
    {
      "title": "Spike-driven Transformer",
      "abstract": "Spiking Neural Networks (SNNs) provide an energy-efficient deep learning option due to their unique spike-based event-driven (i.e., spike-driven) paradigm. In this paper, we incorporate the spike-driven paradigm into Transformer by the proposed Spike-driven Transformer with four unique properties: 1) Event-driven, no calculation is triggered when the input of Transformer is zero; 2) Binary spike communication, all matrix multiplications associated with the spike matrix can be transformed into sparse additions; 3) Self-attention with linear complexity at both token and channel dimensions; 4) The operations between spike-form Query, Key, and Value are mask and addition. Together, there are only sparse addition operations in the Spike-driven Transformer. To this end, we design a novel Spike-Driven Self-Attention (SDSA), which exploits only mask and addition operations without any multiplication, and thus having up to $87.2\\times$ lower computation energy than vanilla self-attention. Especially in SDSA, the matrix multiplication between Query, Key, and Value is designed as the mask operation. In addition, we rearrange all residual connections in the vanilla Transformer before the activation functions to ensure that all neurons transmit binary spike signals. It is shown that the Spike-driven Transformer can achieve 77.1\\% top-1 accuracy on ImageNet-1K, which is the state-of-the-art result in the SNN field. The source code is available at https://github.com/BICLab/Spike-Driven-Transformer.",
      "meta_data": {
        "arxiv_id": "2307.01694v1",
        "authors": [
          "Man Yao",
          "Jiakui Hu",
          "Zhaokun Zhou",
          "Li Yuan",
          "Yonghong Tian",
          "Bo Xu",
          "Guoqi Li"
        ],
        "published_date": "2023-07-04T13:00:18Z",
        "pdf_url": "https://arxiv.org/pdf/2307.01694v1.pdf"
      }
    },
    {
      "title": "Spatio-Temporal Approximation: A Training-Free SNN Conversion for Transformers"
    },
    {
      "title": "Attention as Implicit Structural Inference"
    },
    {
      "title": "Neural encoding with visual attention",
      "abstract": "Visual perception is critically influenced by the focus of attention. Due to limited resources, it is well known that neural representations are biased in favor of attended locations. Using concurrent eye-tracking and functional Magnetic Resonance Imaging (fMRI) recordings from a large cohort of human subjects watching movies, we first demonstrate that leveraging gaze information, in the form of attentional masking, can significantly improve brain response prediction accuracy in a neural encoding model. Next, we propose a novel approach to neural encoding by including a trainable soft-attention module. Using our new approach, we demonstrate that it is possible to learn visual attention policies by end-to-end learning merely on fMRI response data, and without relying on any eye-tracking. Interestingly, we find that attention locations estimated by the model on independent data agree well with the corresponding eye fixation patterns, despite no explicit supervision to do so. Together, these findings suggest that attention modules can be instrumental in neural encoding models of visual stimuli.",
      "meta_data": {
        "arxiv_id": "2010.00516v1",
        "authors": [
          "Meenakshi Khosla",
          "Gia H. Ngo",
          "Keith Jamison",
          "Amy Kuceyeski",
          "Mert R. Sabuncu"
        ],
        "published_date": "2020-10-01T16:04:21Z",
        "pdf_url": "https://arxiv.org/pdf/2010.00516v1.pdf"
      }
    },
    {
      "title": "Neural encoding with visual attention",
      "abstract": "Visual perception is critically influenced by the focus of attention. Due to limited resources, it is well known that neural representations are biased in favor of attended locations. Using concurrent eye-tracking and functional Magnetic Resonance Imaging (fMRI) recordings from a large cohort of human subjects watching movies, we first demonstrate that leveraging gaze information, in the form of attentional masking, can significantly improve brain response prediction accuracy in a neural encoding model. Next, we propose a novel approach to neural encoding by including a trainable soft-attention module. Using our new approach, we demonstrate that it is possible to learn visual attention policies by end-to-end learning merely on fMRI response data, and without relying on any eye-tracking. Interestingly, we find that attention locations estimated by the model on independent data agree well with the corresponding eye fixation patterns, despite no explicit supervision to do so. Together, these findings suggest that attention modules can be instrumental in neural encoding models of visual stimuli.",
      "meta_data": {
        "arxiv_id": "2010.00516v1",
        "authors": [
          "Meenakshi Khosla",
          "Gia H. Ngo",
          "Keith Jamison",
          "Amy Kuceyeski",
          "Mert R. Sabuncu"
        ],
        "published_date": "2020-10-01T16:04:21Z",
        "pdf_url": "https://arxiv.org/pdf/2010.00516v1.pdf"
      }
    },
    {
      "title": "Attention Approximates Sparse Distributed Memory",
      "abstract": "While Attention has come to be an important mechanism in deep learning, there remains limited intuition for why it works so well. Here, we show that Transformer Attention can be closely related under certain data conditions to Kanerva's Sparse Distributed Memory (SDM), a biologically plausible associative memory model. We confirm that these conditions are satisfied in pre-trained GPT2 Transformer models. We discuss the implications of the Attention-SDM map and provide new computational and biological interpretations of Attention.",
      "meta_data": {
        "arxiv_id": "2111.05498v2",
        "authors": [
          "Trenton Bricken",
          "Cengiz Pehlevan"
        ],
        "published_date": "2021-11-10T02:36:32Z",
        "venue": "35th Conference on Neural Information Processing Systems (NeurIPS 2021)",
        "pdf_url": "https://arxiv.org/pdf/2111.05498v2.pdf"
      }
    },
    {
      "title": "Biologically Inspired Learning Model for Instructed Vision",
      "abstract": "As part of understanding how the brain learns, ongoing work seeks to combine biological knowledge and current artificial intelligence (AI) modeling in an attempt to find an efficient biologically plausible learning scheme. Current models of biologically plausible learning often use a cortical-like combination of bottom-up (BU) and top-down (TD) processing, where the TD part carries feedback signals used for learning. However, in the visual cortex, the TD pathway plays a second major role of visual attention, by guiding the visual process to locations and tasks of interest. A biological model should therefore combine the two tasks, and learn to guide the visual process. We introduce a model that uses a cortical-like combination of BU and TD processing that naturally integrates the two major functions of the TD stream. The integrated model is obtained by an appropriate connectivity pattern between the BU and TD streams, a novel processing cycle that uses the TD part twice, and the use of 'Counter-Hebb' learning that operates across the streams. We show that the 'Counter-Hebb' mechanism can provide an exact backpropagation synaptic modification. We further demonstrate the model's ability to guide the visual stream to perform a task of interest, achieving competitive performance compared with AI models on standard multi-task learning benchmarks. The successful combination of learning and visual guidance could provide a new view on combining BU and TD processing in human vision, and suggests possible directions for both biologically plausible models and artificial instructed models, such as vision-language models (VLMs).",
      "meta_data": {
        "arxiv_id": "2306.02415v3",
        "authors": [
          "Roy Abel",
          "Shimon Ullman"
        ],
        "published_date": "2023-06-04T17:38:06Z",
        "pdf_url": "https://arxiv.org/pdf/2306.02415v3.pdf"
      }
    },
    {
      "title": "Short-Term Plasticity Neurons Learning to Learn and Forget",
      "abstract": "Short-term plasticity (STP) is a mechanism that stores decaying memories in synapses of the cerebral cortex. In computing practice, STP has been used, but mostly in the niche of spiking neurons, even though theory predicts that it is the optimal solution to certain dynamic tasks. Here we present a new type of recurrent neural unit, the STP Neuron (STPN), which indeed turns out strikingly powerful. Its key mechanism is that synapses have a state, propagated through time by a self-recurrent connection-within-the-synapse. This formulation enables training the plasticity with backpropagation through time, resulting in a form of learning to learn and forget in the short term. The STPN outperforms all tested alternatives, i.e. RNNs, LSTMs, other models with fast weights, and differentiable plasticity. We confirm this in both supervised and reinforcement learning (RL), and in tasks such as Associative Retrieval, Maze Exploration, Atari video games, and MuJoCo robotics. Moreover, we calculate that, in neuromorphic or biological circuits, the STPN minimizes energy consumption across models, as it depresses individual synapses dynamically. Based on these, biological STP may have been a strong evolutionary attractor that maximizes both efficiency and computational power. The STPN now brings these neuromorphic advantages also to a broad spectrum of machine learning practice. Code is available at https://github.com/NeuromorphicComputing/stpn",
      "meta_data": {
        "arxiv_id": "2206.14048v1",
        "authors": [
          "Hector Garcia Rodriguez",
          "Qinghai Guo",
          "Timoleon Moraitis"
        ],
        "published_date": "2022-06-28T14:47:56Z",
        "venue": "Proceedings of the 39th International Conference on Machine Learning, 162:18704-18722 (2022)",
        "pdf_url": "https://arxiv.org/pdf/2206.14048v1.pdf"
      }
    },
    {
      "title": "Short-Term Plasticity Neurons Learning to Learn and Forget",
      "abstract": "Short-term plasticity (STP) is a mechanism that stores decaying memories in synapses of the cerebral cortex. In computing practice, STP has been used, but mostly in the niche of spiking neurons, even though theory predicts that it is the optimal solution to certain dynamic tasks. Here we present a new type of recurrent neural unit, the STP Neuron (STPN), which indeed turns out strikingly powerful. Its key mechanism is that synapses have a state, propagated through time by a self-recurrent connection-within-the-synapse. This formulation enables training the plasticity with backpropagation through time, resulting in a form of learning to learn and forget in the short term. The STPN outperforms all tested alternatives, i.e. RNNs, LSTMs, other models with fast weights, and differentiable plasticity. We confirm this in both supervised and reinforcement learning (RL), and in tasks such as Associative Retrieval, Maze Exploration, Atari video games, and MuJoCo robotics. Moreover, we calculate that, in neuromorphic or biological circuits, the STPN minimizes energy consumption across models, as it depresses individual synapses dynamically. Based on these, biological STP may have been a strong evolutionary attractor that maximizes both efficiency and computational power. The STPN now brings these neuromorphic advantages also to a broad spectrum of machine learning practice. Code is available at https://github.com/NeuromorphicComputing/stpn",
      "meta_data": {
        "arxiv_id": "2206.14048v1",
        "authors": [
          "Hector Garcia Rodriguez",
          "Qinghai Guo",
          "Timoleon Moraitis"
        ],
        "published_date": "2022-06-28T14:47:56Z",
        "venue": "Proceedings of the 39th International Conference on Machine Learning, 162:18704-18722 (2022)",
        "pdf_url": "https://arxiv.org/pdf/2206.14048v1.pdf"
      }
    },
    {
      "title": "Spatio-Temporal Approximation: A Training-Free SNN Conversion for Transformers"
    },
    {
      "title": "Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data",
      "abstract": "State-of-the-art systems neuroscience experiments yield large-scale multimodal data, and these data sets require new tools for analysis. Inspired by the success of large pretrained models in vision and language domains, we reframe the analysis of large-scale, cellular-resolution neuronal spiking data into an autoregressive spatiotemporal generation problem. Neuroformer is a multimodal, multitask generative pretrained transformer (GPT) model that is specifically designed to handle the intricacies of data in systems neuroscience. It scales linearly with feature size, can process an arbitrary number of modalities, and is adaptable to downstream tasks, such as predicting behavior. We first trained Neuroformer on simulated datasets, and found that it both accurately predicted simulated neuronal circuit activity, and also intrinsically inferred the underlying neural circuit connectivity, including direction. When pretrained to decode neural responses, the model predicted the behavior of a mouse with only few-shot fine-tuning, suggesting that the model begins learning how to do so directly from the neural representations themselves, without any explicit supervision. We used an ablation study to show that joint training on neuronal responses and behavior boosted performance, highlighting the model's ability to associate behavioral and neural representations in an unsupervised manner. These findings show that Neuroformer can analyze neural datasets and their emergent properties, informing the development of models and hypotheses associated with the brain.",
      "meta_data": {
        "arxiv_id": "2311.00136v4",
        "authors": [
          "Antonis Antoniades",
          "Yiyi Yu",
          "Joseph Canzano",
          "William Wang",
          "Spencer LaVere Smith"
        ],
        "published_date": "2023-10-31T20:17:32Z",
        "pdf_url": "https://arxiv.org/pdf/2311.00136v4.pdf"
      }
    },
    {
      "title": "Spikformer: When Spiking Neural Network Meets Transformer "
    }
  ],
  "evaluated_hypothesis_history": [
    {
      "hypothesis": {
        "open_problems": "1. Transformers often develop very large internal activation magnitudes in some layers while others remain under-utilised, leading to unstable training and sub-optimal generalisation.\n2. Biological neurons exhibit homeostatic synaptic scaling: average firing rates are kept in a narrow range by up- or down-regulating all incoming weights. This stabilises activity without changing relative information content.\n3. Current Transformer training does not explicitly enforce such layer-wise homeostasis; adding it only requires a tiny change to the loss function.",
        "method": "Homeostatic Synaptic Scaling Regularisation (HSSR)\n• For every Transformer layer l compute the mini-batch mean pre-activation magnitude  A_l  (e.g. mean(|h_l|)).\n• Add a quadratic penalty that encourages A_l to stay close to a biologically plausible target τ (τ≈1.0).\n  L_total = L_task  +  λ · Σ_l (A_l − τ)^2\n• λ is a small constant (e.g. 1e-3). No other changes to optimiser, architecture, or training loop are needed.\nTheoretical motivation: Keeping layer-wise activity in a narrow band mimics biological homeostasis, avoids saturation/exploding activations, promotes balanced gradient flow, and implicitly regularises the model.",
        "experimental_setup": "Model: 2-layer Transformer encoder (tiny) with 2 attention heads (~1 M parameters).\nDataset: WikiText-2 word-level language modelling.\nTraining: 40 epochs with Adam (lr=1e-3, warm-up 2k steps).\nBaselines: (a) Standard training, (b) Standard + conventional L2 weight decay (0.01).\nProposed: Standard + HSSR (λ=1e-3, τ=1.0).\nEvaluation: validation and test perplexity every epoch.",
        "primary_metric": "perplexity",
        "experimental_code": "import torch, torch.nn as nn\n\nclass HSSR(nn.Module):\n    def __init__(self, model, tau=1.0, lam=1e-3):\n        super().__init__()\n        self.layers = [m for m in model.modules() if isinstance(m, nn.TransformerEncoderLayer)]\n        self.tau, self.lam = tau, lam\n    def forward(self):\n        loss = 0.0\n        for layer in self.layers:\n            # hook retrieves pre-activations stored during forward pass\n            act = layer.last_hidden_state   # shape (seq,batch,dim)\n            A = act.abs().mean()\n            loss = loss + (A - self.tau) ** 2\n        return self.lam * loss\n\n# in training loop\nout = model(src)            # usual forward\nloss_task = criterion(out, tgt)\nloss_hssr = hssr()           # extra regulariser\nloss = loss_task + loss_hssr\nloss.backward()\noptimizer.step()",
        "expected_result": "On WikiText-2 test set after 40 epochs:\n• Baseline perplexity ≈ 72\n• +L2 weight decay perplexity ≈ 70\n• +HSSR perplexity ≈ 66 (≈8% relative reduction)\nTraining is more stable: gradient norm variance reduced by ~15%.",
        "expected_conclusion": "Adding a biologically inspired homeostatic term is trivial to implement yet yields noticeably lower perplexity and stabler training. The improvement arises because balanced layer activations prevent saturation and maintain informative gradients, mirroring how biological neurons regulate firing rates. Thus, minimal biological insight can translate into practical gains for Transformer optimisation."
      },
      "evaluation": {
        "novelty_reason": "The idea of directly regularising the average activation magnitude is related to existing techniques such as (1) activation-regularisation and temporal activation-regularisation used for RNN language models (e.g., AWD-LSTM), (2) squared-L2 penalties on hidden states sometimes called \"Activation L2\" in Transformer implementations, and (3) implicit control of activation statistics through BatchNorm or LayerNorm.  However, those methods are motivated from over-fitting or optimisation convenience, not from the biological concept of homeostatic synaptic scaling, and they are usually applied to specific layers or tokens rather than enforced uniformly across all Transformer layers with a global target τ.  The proposed HSSR frames the constraint explicitly as a biologically-inspired layer-wise homeostasis term, applies it to every Transformer layer regardless of position, and requires only one additional scalar per layer, which to the best of my knowledge has not been systematically studied in Transformer training literature.  Therefore the hypothesis is moderately novel: it repackages a familiar mathematical tool but introduces a new biological motivation and a uniform, very simple implementation that has not yet been evaluated for Transformers.",
        "novelty_score": 6,
        "significance_reason": "If the hypothesised ~8 % perplexity drop and 15 % reduction in gradient-norm variance generalise to larger models and datasets, the method would offer a virtually free improvement to Transformer training—no architectural change, negligible computational overhead, and only one hyper-parameter.  This makes it attractive for both academic research (better understanding of how activation homeostasis affects optimisation and generalisation) and for industry-scale training where stability is critical and training budgets are huge.  In addition, showing that a concrete principle from neurobiology yields measurable gains strengthens the interdisciplinary dialogue between neuroscience and machine learning, a topic of rising academic interest.  Nonetheless, the current evidence is limited to a tiny model on WikiText-2; impact on state-of-the-art, multi-billion-parameter Transformers and other modalities is untested.  Hence the potential significance is promising but not yet proven.",
        "significance_score": 7
      }
    },
    {
      "hypothesis": {
        "open_problems": "1. Transformer training is notoriously sensitive to the scale of internal activations; when a few layers or heads run \"hot\" while others remain \"cold\", gradient flow becomes erratic, convergence slows, and generalisation suffers.\n2. Existing fixes (LayerNorm, activation-L2, careful learning-rate schedules) require hand-tuned hyper-parameters that are re-optimised for every model size, task, and hardware setting – an expensive practice that inflates the carbon footprint of research and industry deployments.\n3. In the brain, firing-rate homeostasis is achieved through an *integral* feedback mechanism: neurons accumulate an error signal (difference between actual and target rate) and use it to multiplicatively scale all incoming synapses, thereby restoring activity without external hyper-parameter tuning.\n4. No current Transformer optimisation scheme replicates this integral control; prior \"activation regularisers\" use a *fixed* penalty weight, meaning the strength of the correction is frozen throughout training and must be guessed by the user.\n5. The open question: Can we translate biological integral homeostatic control into a zero-tuning regulariser that autonomously keeps every Transformer layer in its optimal activation range, thereby improving both performance and sustainability?",
        "method": "Homeostatic Integral Control Regularisation (HICR)\n-----------------------------------------------------\nLet h_l \\in R^{B\\times T\\times d} be the pre-LayerNorm activations of Transformer layer l in a mini-batch of size B, sequence length T.\n1. Per-layer activation statistic:  A_l = mean(|h_l|).\n2. Target firing rate (constant): τ (default 1.0).\n3. Per-layer *homeostatic drive* λ_l  (a persistent scalar, one per layer) initialised to 0 and **not** updated by back-prop.\n4. Extra loss term:   L_homeo = Σ_l  λ_l · (A_l – τ).\n   (linear, not quadratic – mirrors Lagrange multipliers in constrained optimisation.)\n5. After every optimiser step, update each λ_l with an **integral control rule**\n      λ_l ← λ_l + η · (A_l.detach() – τ)\n   where η≈10^{-2} is a small learning rate.  This is gradient ascent on the dual variable and drives the *constraint* A_l=τ in expectation.\n6. No additional hyper-parameters beyond η (shared by all layers) and the biologically motivated τ.  Crucially, *λ_l learns itself*: if a layer is persistently over-active λ_l becomes positive and suppresses it; if under-active λ_l becomes negative and boosts it.\n   The mechanism is therefore plug-and-play, optimiser-agnostic, and scale-free.",
        "experimental_setup": "Language Modelling:\n  • Dataset: WikiText-2.\n  • Model: 2-layer Transformer encoder (≈1 M params).\n  • Training: Adam, lr=1e-3, 40 epochs, identical schedule for all conditions.\nVision:\n  • Dataset: CIFAR-10.\n  • Model: ViT-Tiny (4 heads, 192-dim, 12 layers).\n  • Training: AdamW, lr=3e-4, 100 epochs, cosine decay.\nBaselines:\n  a) Standard training\n  b) Quadratic homeostasis (HSSR from prior hypothesis, λ=1e-3)\nProposed:\n  c) HICR (η=0.01, τ=1.0)\nMetrics collected:\n  • WikiText-2: validation/test perplexity\n  • CIFAR-10: top-1 accuracy\n  • Training stability: variance of gradient norms, incidence of NaNs, wall-clock time until convergence\n  • Hyper-parameter cost: GPU-hours spent on λ/η grid-search (expected to be 0 for HICR)",
        "primary_metric": "(i) WikiText-2 test perplexity; (ii) CIFAR-10 top-1 accuracy.  Secondary: gradient-norm variance.",
        "experimental_code": "class HICR(torch.nn.Module):\n    def __init__(self, model, tau=1.0, eta=1e-2):\n        super().__init__()\n        self.tau, self.eta = tau, eta\n        # pick Transformer layers\n        self.layers = [m for m in model.modules()\n                       if isinstance(m, torch.nn.TransformerEncoderLayer)]\n        # one persistent lambda per layer, registered as buffer\n        for idx, _ in enumerate(self.layers):\n            self.register_buffer(f\"lambda_{idx}\", torch.zeros(1))\n\n    def penalty(self):\n        loss = 0.0\n        for idx, layer in enumerate(self.layers):\n            A = layer.cached_act.abs().mean()  # hook stores activations\n            lam = getattr(self, f\"lambda_{idx}\")\n            loss = loss + lam * (A - self.tau)\n        return loss\n\n    @torch.no_grad()\n    def update_lambdas(self):\n        for idx, layer in enumerate(self.layers):\n            A = layer.cached_act.abs().mean()\n            lam = getattr(self, f\"lambda_{idx}\")\n            lam += self.eta * (A - self.tau)\n\n# --- training loop snippet ---\nout = model(src)\nloss_task = criterion(out, tgt)\nloss_homeo = hicr.penalty()\nloss = loss_task + loss_homeo\nloss.backward()\noptimizer.step()\nhicr.update_lambdas()\noptimizer.zero_grad()",
        "expected_result": "WikiText-2 (40 epochs):\n  • Baseline perplexity ≈ 72\n  • Quadratic HSSR ≈ 66\n  • HICR ≈ 64  (≈11 % relative drop vs baseline)\n  • Gradient-norm variance down by ~20 %\nCIFAR-10 (ViT-Tiny, 100 epochs):\n  • Baseline accuracy 84.5 %\n  • HSSR 85.1 %\n  • HICR 86.0 % (+1.5 pp) with identical training budget\nHyper-parameter search cost:\n  • Baseline+HSSR: 6 grid points × 2 seeds = 12 runs\n  • HICR: zero additional runs (η kept fixed)",
        "expected_conclusion": "Replacing a fixed quadratic activation penalty with biologically inspired *integral* homeostatic control yields consistent gains in language and vision Transformers, while completely eliminating the need to tune a regularisation coefficient.  By continuously adjusting a per-layer drive variable, HICR keeps neural activity in a healthy regime, leading to smoother gradients, faster convergence, and better generalisation.  The work demonstrates that a direct translation of synaptic-scaling feedback control from neuroscience can reduce computational waste and carbon emissions associated with hyper-parameter sweeps, thus offering both academic insight and tangible social benefit."
      },
      "evaluation": {
        "novelty_reason": "1. Existing stabilisation techniques for Transformers (LayerNorm, BatchNorm variants, Fixup/ScaleNorm, ReZero, activation-L2 penalties, adaptive gradient clipping) all rely on coefficients that are fixed throughout training and must be selected by grid search. 2. Prior attempts to mimic biological homeostasis in ANNs generally use static quadratic penalties (e.g. synaptic-scaling regularisers, Sparsity penalties with a fixed beta) rather than a dual-ascent/integral controller that updates its own strength online. 3. The proposed Homeostatic Integral Control Regularisation (HICR) introduces an explicit dual variable λ_l per layer that is updated with gradient-free integral feedback—this mirrors the integral control found in neural firing-rate homeostasis and is not present in mainstream optimisation literature for Transformers. 4. To the best of current knowledge no published Transformer work combines: (i) an activation-level constraint, (ii) a dual-ascent rule external to back-prop, and (iii) zero additional tuned hyper-parameters, making the method conceptually and algorithmically distinct from both fixed penalties and adaptive normalisation layers.",
        "novelty_score": 8,
        "significance_reason": "Academic: The hypothesis provides a biologically grounded control-theoretic lens on Transformer optimisation, potentially opening a new line of work that unifies constrained optimisation (dual ascent), neuroscience (integral homeostatic scaling), and deep-learning practice. If validated, it would enrich our theoretical understanding of why balanced activations matter for gradient flow and generalisation. Societal/Practical: Hyper-parameter sweeps for large Transformers consume thousands of GPU-hours and substantial energy. A plug-and-play mechanism that eliminates regularisation tuning could cut both cost and carbon footprint while simultaneously delivering modest accuracy/perplexity gains. Because Transformers dominate NLP and are rapidly spreading to vision, audio and reinforcement learning, even single-digit-percent improvements and reduced tuning effort translate into large absolute resource savings.",
        "significance_score": 8
      }
    },
    {
      "hypothesis": {
        "open_problems": "1. LayerNorm is the single most-frequent operation inside modern Transformers (two calls per block). At training time it stabilises gradients, yet at inference it contributes 3–8 % of total FLOPs, extra memory reads/writes, and prevents efficient operator fusion.\n2. Existing LayerNorm-free recipes (Fixup, ScaleNorm, ReZero, etc.) achieve partial success but still demand hand-tuned initialisations, tailored learning-rate schedules, or fragile depth-dependent scalings that break when model size, data modality, or hardware changes.\n3. In biological cortex, long-term stability of firing rates is realised through integral homeostatic control: each neuron integrates the deviation between its recent activity and a target set-point and scales all incoming synapses multiplicatively. This mechanism is automatic, local, and does not rely on any external “normalisation layer.”\n4. The open question: can we replace LayerNorm entirely with an online, biologically inspired integral controller that (a) keeps every Transformer block in its optimal activation range, (b) requires **zero** manual re-tuning across depths, widths and tasks, and (c) reduces both training instability and inference cost?",
        "method": "LayerNorm-Free Integral Homeostatic Control (LIHC)\n--------------------------------------------------\nLet h_l ∈ R^{B×T×d} be the pre-activation tensor that would normally be fed into a LayerNorm inside block l.\n1. Per-block statistic   A_l = mean(|h_l|)  (mean over batch, sequence, features).\n2. Persistent *homeostatic drive* λ_l  (scalar, one per block) initialised to 0 and **not** updated by back-prop.\n3. Replace LayerNorm with a simple multiplicative gate  g_l  applied to the block output:\n      ĥ_l = g_l · h_l , with   g_l = exp(−λ_l).\n   (Exponent ensures positivity and keeps gradients well-behaved.)\n4. Extra loss term (dual objective)\n      L_homeo = Σ_l λ_l · (A_l − τ)  (τ≈1.0 fixed).\n5. After every optimiser step perform **integral update** (gradient-free):\n      λ_l ← λ_l + η · (A_l.detach() − τ)  (η≈10⁻² shared by all blocks).\n   This is dual-ascent on the constrained problem “∀l: A_l = τ.”\n6. No LayerNorm, no special weight initialisation, and only two global hyper-parameters (τ, η) that stay constant across models and datasets.\n7. Optional per-head variant: maintain λ_{l,h} to equalise attention heads when desired (swap Σ_l for Σ_{l,h}).",
        "experimental_setup": "1. Language Modelling: WikiText-2 (small) + OpenWebText (large).   Models: 2-layer Tiny-Transformer (1 M params) and GPT-Small (124 M).\n2. Vision: CIFAR-10 + ImageNet-1k.   Models: ViT-Tiny (12 layers) and ViT-Base (86 M).\n3. Speech: LibriSpeech 100 h with Conformer encoder.   Model: Conformer-Small (46 M).\nConditions:\n  a) Baseline: Standard LayerNorm Transformer.\n  b) Fixup initialisation (no norms).\n  c) LIHC (ours, no norms, τ=1, η=0.01).\nAll other hyper-parameters use community defaults; **no extra tuning** for LIHC.\nMetrics:\n  • Task performance – perplexity, top-1 accuracy, word-error-rate.\n  • Training stability – gradient-norm variance, NaN incidence.\n  • Inference cost – FLOPs, latency, SRAM bandwidth measured on NVIDIA A100.\n  • Carbon footprint – Watt-hours per training run and per 1 M inference tokens.\n  • Hyper-parameter search cost – total GPU-hours spent tuning.",
        "primary_metric": "Task-specific metric (perplexity / accuracy / WER) under identical training budgets, plus percentage reduction in inference FLOPs relative to LayerNorm baseline.",
        "experimental_code": "# PyTorch pseudo-code for one Transformer block without LayerNorm\nclass LIHCBlock(nn.Module):\n    def __init__(self, ... , tau=1.0, eta=1e-2):\n        super().__init__()\n        self.attn = MultiHeadAttention(...)\n        self.ff   = FeedForward(...)\n        self.tau, self.eta = tau, eta\n        self.register_buffer('lambda_h', torch.zeros(1))  # persistent\n    def forward(self, x):\n        # usual residual path but NO LayerNorm\n        h = x + self.attn(x)\n        h = h + self.ff(h)\n        A = h.detach().abs().mean()\n        g = torch.exp(-self.lambda_h)         # gating factor\n        h = g * h                             # apply homeostatic scaling\n        # cache A for later lambda update\n        self.cached_A = A\n        return h\n    @torch.no_grad()\n    def update_lambda(self):\n        self.lambda_h += self.eta * (self.cached_A - self.tau)\n\n# after each optimiser step\nfor blk in model.blocks:\n    blk.update_lambda()",
        "expected_result": "WikiText-2 Tiny-Transformer:\n  • Baseline (LayerNorm)   perplexity ≈ 72\n  • Fixup                  perplexity ≈ 75 (diverges occasionally)\n  • LIHC                   perplexity ≈ 66 (−8 %, stable)\nGPT-Small on OpenWebText (300 K steps):\n  • Baseline ppl 34.5 ; LIHC ppl 33.1 ; training 7 % faster (no LN ops).\nViT-Base on ImageNet-1k (300 epochs):\n  • Baseline top-1 83.0 %; Fixup 81.7 %; LIHC 83.4 %.\nInference: removing LayerNorm saves ≈6 % FLOPs and 9 % SRAM bandwidth; end-to-end latency −5 % on A100.\nAcross all runs LIHC required zero hyper-parameter trials beyond the default τ,η.",
        "expected_conclusion": "A biologically inspired integral homeostatic controller can completely substitute LayerNorm in Transformers, yielding equal or better accuracy, smoother optimisation, and measurable reductions in both training and inference cost. By eliminating the need for norm layers and hyper-parameter sweeps, LIHC lowers the carbon footprint of large-scale model development and daily inference, illustrating that neuroscience principles of firing-rate homeostasis can translate into concrete efficiency gains for mainstream machine-learning systems."
      },
      "evaluation": {
        "novelty_reason": "Replacing LayerNorm with a biologically-inspired, gradient-free integral controller is not present in prior Transformer literature. Existing “norm-free” approaches (Fixup, ReZero, DeepNorm, ScaleNorm, μParam, etc.) rely on static initialisation rules or depth-dependent learnt scalars that stay frozen during a forward pass; none maintain a persistent state updated after every optimisation step to enforce a homeostatic set-point. Likewise, neuroscience-inspired synaptic-scaling work has been limited to small recurrent nets and has not been applied to the very high-dimensional, residual-heavy Transformer architecture. LIHC’s use of (i) a single scalar λ per block updated by dual-ascent, (ii) removal of all feature-wise normalisation statistics, and (iii) independence from model depth/width constitutes a new algorithmic family that cannot be reproduced by re-combining existing methods.",
        "novelty_score": 8,
        "significance_reason": "LayerNorm accounts for 3–8 % of inference FLOPs and memory traffic in production-scale Transformers. Eliminating it while matching or slightly surpassing accuracy directly lowers latency, power, and cost for language, vision, and speech models that serve billions of requests daily. Academically, the work tightens the link between control-theoretic formulations of biological homeostasis and optimisation stability in deep networks, opening a new line of inquiry into continuous, local learning rules inside large models. If validated, LIHC would become a drop-in replacement that simplifies hyper-parameter tuning (no depth-specific scalings) and reduces training crashes, benefiting both researchers and industry practitioners and contributing to greener AI.",
        "significance_score": 8
      }
    },
    {
      "hypothesis": {
        "open_problems": "1. LayerNorm is indispensable for stable optimisation of deep Transformers but costs 3–8 % of inference FLOPs and hampers kernel fusion.\n2. Prior ‘norm-free’ techniques (Fixup, ReZero, μParam, DeepNorm, ScaleNorm, LIHC) either demand hand-tuned depth-dependent constants or retain a per-token multiplicative gate that still executes at inference time.\n3. In cortex, firing-rate homeostasis is realised not by a runtime gate but by slowly scaling **synaptic efficacies** (weights) through integral feedback, so that the neuron’s operating point is restored with *zero* per-spike overhead.\n4. The open question: Can we translate integral synaptic scaling into Transformer training so that (a) LayerNorm is removed, (b) all extra computations vanish after training, and (c) no manual re-tuning is required across depths, widths, tasks or hardware?",
        "method": "Fused Integral Synaptic Normalisation (FISN)\n------------------------------------------------\nNotation: for block l let h_l be its post-residual activations and W_l^k each parameter matrix inside the block (Q,K,V,FFN-up,FFN-down).\n\n1. Statistic   A_l = mean(|h_l|)   (averaged over batch, tokens, features).\n2. Persistent dual variable (homeostatic drive)   λ_l   initialised to 0; **not** updated by back-prop.\n3. Dual loss term added to the task loss:\n     L_homeo = Σ_l λ_l · (A_l − τ)              (τ≈1.0 fixed).\n4. After every optimiser step perform two gradient-free updates:\n   a) Integral ascent on λ_l\n        λ_l ← λ_l + η (A_l.detach() − τ)        (η≈10⁻²).\n   b) Weight fusion (compiles the gate away)\n        for every matrix W_l^k:   W_l^k ← e^{−η_w λ_l} · W_l^k     (η_w=1 by default).\n   Because all incoming weights are rescaled, the next forward pass automatically applies the corrective gain; no runtime multiply, no extra tensor.\n5. Optional hierarchical variant: keep λ_{l,h} per attention head; rescale each head’s Q,K,V separately.\n6. No LayerNorm, no special initialisation, two global hyper-parameters (τ,η) that remain constant for any model or modality.",
        "experimental_setup": "Benchmarks\n1. Language: WikiText-103 (medium) & The Pile (large) with 6-layer (44 M) and 24-layer (350 M) GPT-style models.\n2. Vision: CIFAR-100 & ImageNet-1k with ViT-Small (22 M) and ViT-Base (86 M).\n3. Speech: LibriSpeech 100 h with Conformer-Small (46 M).\n\nConditions\nA) LayerNorm baseline.\nB) LIHC (activation gate, stays in graph).\nC) FISN (ours, weights fused, no runtime gate).\nD) Fixup (no norms, tuned as in original paper).\n\nTraining uses identical optimiser schedules; FISN employs fixed τ=1, η=0.01, η_w=1 everywhere.  No other hyper-parameter search is allowed for FISN.\n\nMeasurements\n• Task metrics: perplexity / top-1 accuracy / word-error-rate.\n• Training stability: gradient-norm variance, NaN count.\n• Inference efficiency on NVIDIA A100: total FLOPs, SRAM bandwidth, end-to-end latency for 128-token batch.\n• Carbon footprint: watt-hours per training run + per 1 M inference tokens.\n• Memory footprint of inference graph.",
        "primary_metric": "Task performance under equal training budgets plus percentage reduction in inference FLOPs/latency relative to LayerNorm baseline.",
        "experimental_code": "# simplified PyTorch sketch for one Transformer block without LayerNorm\nclass FISNBlock(nn.Module):\n    def __init__(self, dim, heads, tau=1., eta=1e-2):\n        super().__init__()\n        self.attn  = MultiHeadAttention(dim, heads)\n        self.ff    = FeedForward(dim)\n        self.tau, self.eta = tau, eta\n        self.register_buffer('lambda_h', torch.zeros(1))\n    def forward(self, x):\n        h = x + self.attn(x)\n        h = h + self.ff(h)\n        self.A = h.detach().abs().mean()  # cached for update\n        return h                           # no runtime gate\n    @torch.no_grad()\n    def fuse_weights(self):\n        # integral dual-ascent\n        self.lambda_h += self.eta * (self.A - self.tau)\n        g = torch.exp(-self.lambda_h)      # positive scale\n        # fuse into all parameter matrices of the block\n        for mod in [self.attn.q_proj, self.attn.k_proj, self.attn.v_proj,\n                    self.attn.o_proj, self.ff.w1, self.ff.w2]:\n            mod.weight.mul_(g)",
        "expected_result": "WikiText-103, 6-layer GPT (44 M):\n LayerNorm   ppl 32.8   (baseline)\n Fixup       ppl 34.3   (diverges 2/5 runs)\n LIHC        ppl 32.2   (-1.8 %)\n FISN        ppl 31.6   (-3.7 %), training 6 % faster than LN.\n\nImageNet ViT-Base (86 M, 300 ep):\n LayerNorm   top-1 83.0 %\n LIHC        83.4 %\n FISN        83.6 %, identical hyper-params.\n\nInference on A100, ViT-Base (224×224):\n LayerNorm   17.4 GFLOPs, 5.8 ms latency\n LIHC        16.4 GFLOPs, 5.5 ms (gate multiply remains)\n FISN        16.2 GFLOPs, 5.4 ms (no gate, no LN)  →  -7 % FLOPs, ‑7 % latency.\n\nAcross all experiments FISN required zero hyper-parameter sweeps and showed 20-25 % lower gradient-norm variance than the baseline.",
        "expected_conclusion": "By migrating biological integral homeostatic control from the activation domain to the weight domain, FISN removes the need for both LayerNorm *and* any residual runtime gate, yielding a truly zero-overhead inference path.  The single feedback variable per block self-regulates activation magnitude across depths, obviating manual tuning and improving convergence.  Empirically FISN matches or surpasses LayerNorm accuracy while cutting inference FLOPs and latency by ~7 %, thereby lowering operational cost and carbon footprint.  The results demonstrate that synaptic-scaling principles from neuroscience can be compiled into weight matrices of modern Transformers, offering a new efficiency frontier for large-scale ML systems."
      },
      "evaluation": {
        "novelty_reason": "The proposal introduces a control-theoretic, biologically inspired mechanism—integral synaptic scaling—implemented as an auxiliary dual variable that is (1) updated outside back-prop by gradient-free integral ascent and (2) immediately fused into the block’s weights so that no multiplicative gate or learned scale remains in the inference graph.  Prior “norm-free” or “light-norm” works (Fixup, ReZero, µParam, DeepNorm, ScaleNorm, LIHC) either depend on depth-specific hand-tuned constants, keep a per-token scale parameter that is still evaluated at inference, or both; none remove online normalisation while also eliminating all runtime arithmetic.  Existing biological-inspired studies mainly mimic neuronal firing dynamics or Hebbian plasticity, not slow homeostatic weight scaling translated into Transformer optimisation.  The explicit dual-ascent formulation that treats activation magnitude as a constraint and compiles the Lagrange multiplier into weights appears unprecedented in Transformer literature, giving the hypothesis clear novelty.",
        "novelty_score": 8,
        "significance_reason": "LayerNorm accounts for 3–8 % of inference FLOPs and impedes kernel fusion in production systems; eliminating it without sacrificing accuracy directly lowers latency, energy use, and carbon footprint for the rapidly growing volume of Transformer inference.  If the same two global hyper-parameters work across depths, widths, and modalities, practitioners can save extensive tuning time and cost.  Academically, the work offers a fresh bridge between neuroscience (homeostatic synaptic scaling) and deep learning optimisation, potentially inspiring further biologically grounded control schemes.  Societally, a 7 % efficiency gain at the scale of large-language-model deployment translates into substantial energy savings and CO₂ reduction.  Hence the hypothesis bears high practical and scientific importance.",
        "significance_score": 8
      }
    },
    {
      "hypothesis": {
        "open_problems": "1. Removing LayerNorm with weight-fusion homeostatic control (FISN) eliminates runtime overhead but still requires (a) computing full-batch activation statistics every step and (b) leaves the over-parameterised weights that were continually down-scaled, wasting FLOPs and energy at inference.\n2. Existing pruning pipelines (magnitude or movement pruning) need an extra fine-tuning stage and hyper-parameter sweeps; they are orthogonal to normalisation methods.\n3. Biology solves both stability and metabolic efficiency jointly: after long-term depression, persistently weak synapses are eliminated (synaptic pruning) while the remaining ones are re-scaled to keep firing rates homeostatic.\n4. Open question: can we design a single integral-control rule that (i) stabilises Transformer training without LayerNorm, (ii) identifies chronically under-utilised weights on the fly, and (iii) compiles both normalisation and sparsity into the final static graph—without any extra retraining or tuning?",
        "method": "Homeostatic Integral Scale-AND-Prune (HISP)\n------------------------------------------------\nNotation: For block l let h_l be its post-residual activations, and W_l^k each weight matrix (Q, K, V, FFN-up, FFN-down).\n\n1. Streaming statistic  A_l  = EMA_β(|h_l|)   (|·| averaged over 128 randomly sampled tokens); β≈0.95.\n2. Persistent dual variable  λ_l  (one scalar per block) initialised to 0, **not** in back-prop.\n3. Dual loss added to task loss:   L_homeo = Σ_l λ_l·(A_l−τ)   with τ≈1.\n4. After every optimiser step perform two gradient-free updates:\n   a) Integral ascent   λ_l ← λ_l + η(A_l.detach()−τ)       (η≈10⁻²)\n   b) Weight fusion    W_l^k ← e^{−η_w λ_l} · W_l^k        (η_w=1)\n5. Online pruning flag: maintain an EMA of λ̂_l = EMA_0.99(λ_l).  If λ̂_l > θ (θ≈0.8) this block has been down-scaled for many iterations; mask (set to zero) all weights in W_l^k whose absolute value < γ·median(|W_l^k|) with γ=0.25.  Masking is permanent and executed immediately after step 4b; pruned weights no longer receive gradients.\n6. Optional fine granularity: apply steps 1–5 per attention head (λ_{l,h}) to yield head-wise sparsity.\n7. Hyper-parameters (τ,η,β,θ,γ) are global constants reused for every depth, width, modality.",
        "experimental_setup": "Benchmarks & models (same as FISN to ensure comparability)\n• Language: WikiText-103 (44 M, 350 M GPT).\n• Vision: ImageNet-1k with ViT-Small/ViT-Base.\n• Speech: LibriSpeech 100 h with Conformer-Small.\n\nConditions\nA) LayerNorm baseline.\nB) FISN (norm-free, dense).\nC) HISP (ours, norm-free + online pruning).\n\nAll methods share identical optimiser, schedule, and training budget; HISP uses fixed hyper-parameters listed above.  No separate pruning or fine-tuning phase.\n\nMeasurements\n• Task metric (ppl/accuracy/WER).\n• Training stability (grad-norm variance, NaNs).\n• Final sparsity (% non-zero weights) and structured sparsity (% pruned heads).\n• Inference efficiency on A100: GFLOPs, latency, energy.\n• Total GPU-hours including any pruning or tuning (should be equal across B & C).",
        "primary_metric": "Task performance under equal budget plus percentage reduction in inference GFLOPs after pruning.",
        "experimental_code": "# simplified PyTorch core of HISP for one block\nclass HISPBlock(nn.Module):\n    def __init__(self, dim, heads, tau=1., eta=1e-2, beta=0.95,\n                 prune_th=0.8, prune_gamma=0.25):\n        super().__init__()\n        self.attn = MultiHeadAttention(dim, heads)\n        self.ff   = FeedForward(dim)\n        self.tau, self.eta, self.beta = tau, eta, beta\n        self.pth, self.pgamma = prune_th, prune_gamma\n        self.register_buffer('lambda_h', torch.zeros(1))\n        self.register_buffer('lambda_ema', torch.zeros(1))\n        self.register_buffer('A', torch.zeros(1))\n    def forward(self, x):\n        h = x + self.attn(x)\n        h = h + self.ff(h)\n        # update streaming statistic with 128 random tokens\n        sample = h.flatten(0,1)      # (B*T, d)\n        idx = torch.randint(0, sample.size(0), (128,), device=h.device)\n        a_now = sample[idx].abs().mean()\n        self.A.mul_(self.beta).add_(a_now*(1-self.beta))\n        return h     # scaling already fused into weights\n    @torch.no_grad()\n    def fuse_and_prune(self):\n        # integral ascent and weight fusion\n        self.lambda_h += self.eta * (self.A - self.tau)\n        g = torch.exp(-self.lambda_h)\n        for mod in [self.attn.q_proj, self.attn.k_proj, self.attn.v_proj,\n                    self.attn.o_proj, self.ff.w1, self.ff.w2]:\n            mod.weight.mul_(g)\n        # update long-term EMA of lambda\n        self.lambda_ema.mul_(0.99).add_(0.01*self.lambda_h)\n        # automatic pruning if chronically over-active\n        if self.lambda_ema > self.pth:\n            for mod in [self.attn.q_proj, self.attn.k_proj, self.attn.v_proj,\n                        self.attn.o_proj, self.ff.w1, self.ff.w2]:\n                thr = self.pgamma * mod.weight.abs().median()\n                mask = mod.weight.abs() < thr\n                mod.weight[mask] = 0.\n                mod.weight.requires_grad = False",
        "expected_result": "ImageNet ViT-Base (86 M):\n LayerNorm   83.0 % top-1, 17.4 GFLOPs\n FISN        83.6 %, 16.2 GFLOPs (-7 %)\n HISP        83.5 %, 12.8 GFLOPs (-26 % vs LN, ‑21 % vs FISN); 25 % of attention heads pruned, overall 32 % weight sparsity.\n\nWikiText-103 GPT-350 M:\n LayerNorm   ppl 29.2, 69 GFLOPs/128 tok\n FISN        ppl 28.7, 64 GFLOPs\n HISP        ppl 28.9, 48 GFLOPs (-30 %), identical training stability\n\nAcross tasks HISP keeps accuracy within ±0.2 pp or perplexity within +0.3 relative to FISN while delivering 20–30 % additional inference savings.  No re-training or hyper-parameter search required.",
        "expected_conclusion": "A single biologically inspired integral controller can simultaneously normalise activations and conduct synaptic pruning inside Transformers.  By fusing its actions into weights during training, HISP removes LayerNorm, avoids runtime gates, and automatically excises redundant parameters, yielding up to 30 % extra FLOP and energy savings over prior norm-free methods with negligible accuracy loss.  The study shows that cortical principles of homeostatic scaling and structural plasticity translate into practical, self-contained algorithms for greener and faster large-scale AI systems."
      },
      "evaluation": {
        "novelty_reason": "The proposal unifies two research threads that have so far been treated separately: (1) norm-free / scale-equilibrated Transformer training (e.g., FISN, FixNorm, ScaleNorm) and (2) weight/structural pruning (magnitude, movement, SNIP-it, GMP), which is usually applied after pre-training with an additional fine-tuning budget. HISP is novel in that a single biologically-motivated integral controller simultaneously (a) keeps activations in a target range, (b) fuses the normalisation gain directly into the weights so that no run-time operation is left, and (c) drives an online, once-for-all pruning rule that requires zero extra optimisation steps or hyper-parameter sweeps. No existing paper combines homeostatic scaling with on-the-fly, inference-time-static sparsification inside the same optimisation loop for Transformers. Furthermore, the use of a scalar dual variable per block/head that is excluded from back-prop and updated with gradient-free integrator ascent is uncommon in deep-learning practice and directly mirrors cortical synaptic scaling models, adding conceptual novelty.",
        "novelty_score": 8,
        "significance_reason": "If validated, HISP would permit practitioners to train norm-free, sparsity-aware Transformers that are 20–30 % cheaper at inference without any extra fine-tuning cost, immediately lowering energy consumption and enabling deployment on resource-constrained hardware. Academically, it bridges neuroscience (homeostatic plasticity and synaptic pruning) with optimisation theory (integral control, dual ascent) and deep-learning engineering (normalisation and pruning), providing a concrete demonstration that biologically plausible control rules can out-perform hand-engineered pipelines. Because compute and energy are the dominant bottlenecks for scaling large models, a method that jointly improves training stability and inference efficiency while simplifying the pipeline has high relevance to both researchers and industry. The ability to generalise across language, vision and speech models with fixed hyper-parameters further increases its practical impact.",
        "significance_score": 9
      }
    },
    {
      "hypothesis": {
        "open_problems": "1. Current norm-free integral controllers such as FISN/HISP stabilise activations and fuse the scaling gain into the weights, but their optimisation objective is agnostic to the *energetic* cost of the model.  In consequence they may prune weights that contribute little to the loss yet leave expensive dense sub-tensors untouched, limiting real-world efficiency gains.\n2. Magnitude-based on-the-fly pruning produces unstructured sparsity that most inference accelerators cannot exploit.  Hardware-friendly patterns (head-, block-, or row-level sparsity) require separate structured-pruning passes and extra tuning.\n3. Biology addresses both problems through multi-timescale plasticity: fast homeostatic scaling keeps firing rates in range, while slow *structural plasticity* removes metabolically costly synapses until neural activity meets an energy budget.\n4. Open research question: can we endow Transformers with a *dual-objective* integral controller that simultaneously enforces (i) activation homeostasis *and* (ii) a target energy/FLOP budget, yielding hardware-aware structured sparsity with no post-hoc fine-tuning?",
        "method": "Multiscale Integral Plasticity for Energy-aware Transformers (MIPET)\n---------------------------------------------------------------------\nNotation: for block l let h_l be post-residual activations, W_l^k each weight matrix (Q,K,V,FFN-up,FFN-down), and C_l the theoretical dense FLOPs of the block.\n\nDual variables (no back-prop):\n  λ_l  – fast homeostatic drive (activation target τ≈1).\n  μ_l  – slow metabolic drive (energy target ρ, e.g. 50% of dense FLOPs).\n\nStep-wise procedure (executed every optimiser step):\n1. Streaming statistics (over 128 random tokens)\n      A_l  ←  EMA_β(|h_l|)                     # activation magnitude\n      F_l  ←  Σ_k nnz(W_l^k) / |W_l^k|         # current density per matrix\n      E_l  =  F_l · C_l                        # estimated FLOPs this block\n2. Dual loss added to task loss\n      L_dual = Σ_l [ λ_l·(A_l−τ)  +  μ_l·(E_l−ρ·C_l) ]\n3. Optimiser step on total loss.\n4. Gradient-free dual updates (β≈0.95, η_fast≈10⁻², η_slow≈10⁻³)\n      λ_l ← λ_l + η_fast · (A_l.detach()−τ)\n      μ_l ← μ_l + η_slow · (E_l.detach()−ρ·C_l)\n5. Weight fusion (as in FISN)\n      scale = exp(−λ_l)\n      for W_l^k:  W_l^k ← scale · W_l^k\n6. Structured pruning triggered by sustained positive μ̂_l = EMA_0.99(μ_l):\n      if μ̂_l > θ (θ≈0.5) then\n          a) prune entire attention heads whose total |W| < γ·median(|W|_heads)\n          b) prune FFN rows whose ℓ₂ norm < γ·median(‖row‖)\n      Masked structures are permanently zeroed and excluded from further gradient updates, instantly reducing E_l.\n7. Regrowth safety net: if μ̂_l becomes < −θ for 500 steps, randomly resurrect 10% of previously removed heads/rows to avoid capacity collapse (mirrors synaptic regrowth).  Regrown parameters are re-initialised with small Gaussian noise.\n\nGlobal hyper-parameters (τ,ρ,β,η_fast,η_slow,θ,γ) are fixed once and reused for all depths, widths, and modalities.",
        "experimental_setup": "Datasets & Models (all match prior FISN benchmarks for comparability)\n• Language: WikiText-103 (44 M GPT) and C4-subset (350 M GPT).\n• Vision: ImageNet-1k with ViT-Base (86 M).\n• Speech: LibriSpeech 100 h with Conformer-Small (46 M).\n\nConditions\nA) LayerNorm baseline.\nB) FISN (norm-free, dense).\nC) HISP (norm-free, unstructured sparsity).\nD) MIPET (ours, norm-free + structured sparsity + energy constraint).\n\nAll methods use identical optimiser and training budget; MIPET employs a *single* energy target ρ = 0.6 (i.e. aim for 40 % FLOP reduction) and default hyper-parameters above—no per-model tuning.\n\nMetrics\n• Task: perplexity / top-1 accuracy / WER.\n• Training stability: gradient-norm variance, NaNs.\n• Energy efficiency: actual inference GFLOPs, latency, and joules measured with NVIDIA Nsight on A100.\n• Sparsity pattern: % pruned heads, % pruned FFN rows, overall non-zero count.\n• Total GPU-hours including any pruning or tuning.",
        "primary_metric": "Task performance under equal training budget *and* percentage reduction in real measured inference energy (joules) relative to LayerNorm baseline.",
        "experimental_code": "# core PyTorch fragment for one Transformer block\nclass MIPETBlock(nn.Module):\n    def __init__(self, dim, heads, C_flops, tau=1., rho=0.6,\n                 eta_fast=1e-2, eta_slow=1e-3, beta=0.95,\n                 th=0.5, gamma=0.25):\n        super().__init__()\n        self.attn = MultiHeadAttention(dim, heads)\n        self.ff   = FeedForward(dim)\n        self.tau, self.rho = tau, rho\n        self.eta_f, self.eta_s = eta_fast, eta_slow\n        self.beta, self.th, self.gamma = beta, th, gamma\n        self.C = C_flops\n        self.register_buffer('lam', torch.zeros(1))\n        self.register_buffer('mu',  torch.zeros(1))\n        self.register_buffer('A',   torch.zeros(1))\n        self.register_buffer('E',   torch.tensor(self.C))\n    def forward(self, x):\n        h = x + self.attn(x)\n        h = h + self.ff(h)\n        # streaming activation statistic\n        sample = h.flatten(0,1)\n        idx = torch.randint(0, sample.size(0), (128,), device=h.device)\n        a_now = sample[idx].abs().mean()\n        self.A.mul_(self.beta).add_(a_now*(1-self.beta))\n        return h  # scaling already fused into weights\n    def dual_loss(self):\n        # density estimate\n        nz = 0\n        tot = 0\n        for mod in [self.attn.q_proj, self.attn.k_proj, self.attn.v_proj,\n                    self.attn.o_proj, self.ff.w1, self.ff.w2]:\n            nz += mod.weight.count_nonzero()\n            tot += mod.weight.numel()\n        density = nz / tot\n        self.E = density * self.C\n        return self.lam*(self.A - self.tau) + self.mu*(self.E - self.rho*self.C)\n    @torch.no_grad()\n    def fuse_prune_update(self):\n        # integral updates\n        self.lam += self.eta_f * (self.A - self.tau)\n        self.mu  += self.eta_s * (self.E - self.rho*self.C)\n        # fuse scaling into weights\n        g = torch.exp(-self.lam)\n        for mod in [self.attn.q_proj, self.attn.k_proj, self.attn.v_proj,\n                    self.attn.o_proj, self.ff.w1, self.ff.w2]:\n            mod.weight.mul_(g)\n        # structured pruning when sustained high mu\n        if self.mu > self.th:\n            # prune complete attention heads\n            head_scores = self.attn.q_proj.weight.view(self.attn.h, -1).abs().mean(dim=1)\n            head_mask = head_scores < self.gamma * head_scores.median()\n            for name in ['q_proj','k_proj','v_proj','o_proj']:\n                w = getattr(self.attn, name).weight.view(self.attn.h, -1)\n                w[head_mask] = 0.\n                getattr(self.attn, name).weight.requires_grad = False\n            # prune FFN rows\n            row_scores = self.ff.w1.weight.norm(dim=1)\n            row_mask = row_scores < self.gamma * row_scores.median()\n            self.ff.w1.weight[row_mask] = 0.\n            self.ff.w2.weight[:,row_mask] = 0.\n            self.ff.w1.weight.requires_grad = False\n            self.ff.w2.weight.requires_grad = False",
        "expected_result": "ImageNet ViT-Base:\n LayerNorm   83.0 % top-1, 17.4 GFLOPs, 30 J / image batch\n FISN        83.6 %, 16.2 GFLOPs, 28 J\n HISP        83.5 %, 12.8 GFLOPs, 23 J\n MIPET       83.4 %, 10.1 GFLOPs (-42 % vs LN), 18 J (-40 %) ; 38 % heads and 45 % FFN rows pruned.\n\nWikiText-103 GPT-350 M:\n LayerNorm   ppl 29.2, 69 GFLOPs/128 tok, 26 J\n FISN        ppl 28.7, 64 GFLOPs, 24 J\n MIPET       ppl 29.0, 41 GFLOPs (-41 %), 16 J (-38 %) ; training equally stable (no extra NaNs).\n\nAcross all tasks MIPET hits the predefined energy budget (ρ=0.6) within ±3 %, retains accuracy within ±0.2 pp or +0.3 ppl of dense norm-free baselines, and needs no additional fine-tuning.",
        "expected_conclusion": "A two-timescale, biologically inspired integral controller can turn Transformers into self-optimising systems that respect both neuronal (activation) and metabolic (energy) homeostasis.  By jointly regulating activation magnitude and FLOP budget, MIPET removes LayerNorm, fuses gains into weights, and yields hardware-friendly structured sparsity without any post-training phase or hyper-parameter search.  Empirical results on language, vision, and speech show up to 40 % real energy savings at equal accuracy, demonstrating a practical path toward greener large-scale AI driven by principles of cortical plasticity."
      },
      "evaluation": {
        "novelty_reason": "The proposal unifies two research lines that have so far remained separate: (i) norm-free integral controllers (e.g. FISN, HISP) that stabilise activations and (ii) structured-sparsity or lottery-ticket style pruning/regrowth methods that target efficiency.  No prior work endows a transformer with a dual set of biologically-motivated fast/slow Lagrange multipliers that (a) keeps activations in a homeostatic range and (b) simultaneously meters the block-level FLOP budget during training.  The slow μ_l variable drives head- and row-level pruning on-the-fly, eliminating the need for an extra pruning-and-fine-tune stage that all existing structured-sparsity papers (e.g. movement-pruning, head-pruning, block-pruning, RigL) still require.  Moreover, the controller is gradient-free and fuses the gain into weights at every step, a mechanism borrowed from cortical structural plasticity that is absent from previous ML sparsifiers.  Finally, using a *single global energy hyper-parameter* that transfers across modalities is new; existing methods either hand-tune sparsity ratios per model or ignore hardware-realistic energy metrics altogether.",
        "novelty_score": 8,
        "significance_reason": "Academically, the hypothesis offers a principled optimisation framework that treats energy/FLOPs as a first-class constraint and marries it with biologically plausible dual-timescale plasticity.  This could open a new family of norm-free, resource-aware training algorithms and stimulate cross-disciplinary work between neuroscience and efficient ML.  Practically, the expected 40 % reduction in real inference energy at equal accuracy and without post-hoc tuning directly addresses the mounting environmental and economic costs of deploying large transformer models.  Producing hardware-friendly structured sparsity in a single training run simplifies deployment on existing accelerators, increasing the likelihood of industry adoption and large-scale societal energy savings.  Because the method is model-agnostic and uses fixed hyper-parameters, it scales to diverse tasks, further amplifying impact.",
        "significance_score": 8
      }
    }
  ],
  "research_session": {
    "hypothesis": {
      "open_problems": "1. Current norm-free integral controllers such as FISN/HISP stabilise activations and fuse the scaling gain into the weights, but their optimisation objective is agnostic to the *energetic* cost of the model.  In consequence they may prune weights that contribute little to the loss yet leave expensive dense sub-tensors untouched, limiting real-world efficiency gains.\n2. Magnitude-based on-the-fly pruning produces unstructured sparsity that most inference accelerators cannot exploit.  Hardware-friendly patterns (head-, block-, or row-level sparsity) require separate structured-pruning passes and extra tuning.\n3. Biology addresses both problems through multi-timescale plasticity: fast homeostatic scaling keeps firing rates in range, while slow *structural plasticity* removes metabolically costly synapses until neural activity meets an energy budget.\n4. Open research question: can we endow Transformers with a *dual-objective* integral controller that simultaneously enforces (i) activation homeostasis *and* (ii) a target energy/FLOP budget, yielding hardware-aware structured sparsity with no post-hoc fine-tuning?",
      "method": "Multiscale Integral Plasticity for Energy-aware Transformers (MIPET)\n---------------------------------------------------------------------\nNotation: for block l let h_l be post-residual activations, W_l^k each weight matrix (Q,K,V,FFN-up,FFN-down), and C_l the theoretical dense FLOPs of the block.\n\nDual variables (no back-prop):\n  λ_l  – fast homeostatic drive (activation target τ≈1).\n  μ_l  – slow metabolic drive (energy target ρ, e.g. 50% of dense FLOPs).\n\nStep-wise procedure (executed every optimiser step):\n1. Streaming statistics (over 128 random tokens)\n      A_l  ←  EMA_β(|h_l|)                     # activation magnitude\n      F_l  ←  Σ_k nnz(W_l^k) / |W_l^k|         # current density per matrix\n      E_l  =  F_l · C_l                        # estimated FLOPs this block\n2. Dual loss added to task loss\n      L_dual = Σ_l [ λ_l·(A_l−τ)  +  μ_l·(E_l−ρ·C_l) ]\n3. Optimiser step on total loss.\n4. Gradient-free dual updates (β≈0.95, η_fast≈10⁻², η_slow≈10⁻³)\n      λ_l ← λ_l + η_fast · (A_l.detach()−τ)\n      μ_l ← μ_l + η_slow · (E_l.detach()−ρ·C_l)\n5. Weight fusion (as in FISN)\n      scale = exp(−λ_l)\n      for W_l^k:  W_l^k ← scale · W_l^k\n6. Structured pruning triggered by sustained positive μ̂_l = EMA_0.99(μ_l):\n      if μ̂_l > θ (θ≈0.5) then\n          a) prune entire attention heads whose total |W| < γ·median(|W|_heads)\n          b) prune FFN rows whose ℓ₂ norm < γ·median(‖row‖)\n      Masked structures are permanently zeroed and excluded from further gradient updates, instantly reducing E_l.\n7. Regrowth safety net: if μ̂_l becomes < −θ for 500 steps, randomly resurrect 10% of previously removed heads/rows to avoid capacity collapse (mirrors synaptic regrowth).  Regrown parameters are re-initialised with small Gaussian noise.\n\nGlobal hyper-parameters (τ,ρ,β,η_fast,η_slow,θ,γ) are fixed once and reused for all depths, widths, and modalities.",
      "experimental_setup": "Datasets & Models (all match prior FISN benchmarks for comparability)\n• Language: WikiText-103 (44 M GPT) and C4-subset (350 M GPT).\n• Vision: ImageNet-1k with ViT-Base (86 M).\n• Speech: LibriSpeech 100 h with Conformer-Small (46 M).\n\nConditions\nA) LayerNorm baseline.\nB) FISN (norm-free, dense).\nC) HISP (norm-free, unstructured sparsity).\nD) MIPET (ours, norm-free + structured sparsity + energy constraint).\n\nAll methods use identical optimiser and training budget; MIPET employs a *single* energy target ρ = 0.6 (i.e. aim for 40 % FLOP reduction) and default hyper-parameters above—no per-model tuning.\n\nMetrics\n• Task: perplexity / top-1 accuracy / WER.\n• Training stability: gradient-norm variance, NaNs.\n• Energy efficiency: actual inference GFLOPs, latency, and joules measured with NVIDIA Nsight on A100.\n• Sparsity pattern: % pruned heads, % pruned FFN rows, overall non-zero count.\n• Total GPU-hours including any pruning or tuning.",
      "primary_metric": "Task performance under equal training budget *and* percentage reduction in real measured inference energy (joules) relative to LayerNorm baseline.",
      "experimental_code": "# core PyTorch fragment for one Transformer block\nclass MIPETBlock(nn.Module):\n    def __init__(self, dim, heads, C_flops, tau=1., rho=0.6,\n                 eta_fast=1e-2, eta_slow=1e-3, beta=0.95,\n                 th=0.5, gamma=0.25):\n        super().__init__()\n        self.attn = MultiHeadAttention(dim, heads)\n        self.ff   = FeedForward(dim)\n        self.tau, self.rho = tau, rho\n        self.eta_f, self.eta_s = eta_fast, eta_slow\n        self.beta, self.th, self.gamma = beta, th, gamma\n        self.C = C_flops\n        self.register_buffer('lam', torch.zeros(1))\n        self.register_buffer('mu',  torch.zeros(1))\n        self.register_buffer('A',   torch.zeros(1))\n        self.register_buffer('E',   torch.tensor(self.C))\n    def forward(self, x):\n        h = x + self.attn(x)\n        h = h + self.ff(h)\n        # streaming activation statistic\n        sample = h.flatten(0,1)\n        idx = torch.randint(0, sample.size(0), (128,), device=h.device)\n        a_now = sample[idx].abs().mean()\n        self.A.mul_(self.beta).add_(a_now*(1-self.beta))\n        return h  # scaling already fused into weights\n    def dual_loss(self):\n        # density estimate\n        nz = 0\n        tot = 0\n        for mod in [self.attn.q_proj, self.attn.k_proj, self.attn.v_proj,\n                    self.attn.o_proj, self.ff.w1, self.ff.w2]:\n            nz += mod.weight.count_nonzero()\n            tot += mod.weight.numel()\n        density = nz / tot\n        self.E = density * self.C\n        return self.lam*(self.A - self.tau) + self.mu*(self.E - self.rho*self.C)\n    @torch.no_grad()\n    def fuse_prune_update(self):\n        # integral updates\n        self.lam += self.eta_f * (self.A - self.tau)\n        self.mu  += self.eta_s * (self.E - self.rho*self.C)\n        # fuse scaling into weights\n        g = torch.exp(-self.lam)\n        for mod in [self.attn.q_proj, self.attn.k_proj, self.attn.v_proj,\n                    self.attn.o_proj, self.ff.w1, self.ff.w2]:\n            mod.weight.mul_(g)\n        # structured pruning when sustained high mu\n        if self.mu > self.th:\n            # prune complete attention heads\n            head_scores = self.attn.q_proj.weight.view(self.attn.h, -1).abs().mean(dim=1)\n            head_mask = head_scores < self.gamma * head_scores.median()\n            for name in ['q_proj','k_proj','v_proj','o_proj']:\n                w = getattr(self.attn, name).weight.view(self.attn.h, -1)\n                w[head_mask] = 0.\n                getattr(self.attn, name).weight.requires_grad = False\n            # prune FFN rows\n            row_scores = self.ff.w1.weight.norm(dim=1)\n            row_mask = row_scores < self.gamma * row_scores.median()\n            self.ff.w1.weight[row_mask] = 0.\n            self.ff.w2.weight[:,row_mask] = 0.\n            self.ff.w1.weight.requires_grad = False\n            self.ff.w2.weight.requires_grad = False",
      "expected_result": "ImageNet ViT-Base:\n LayerNorm   83.0 % top-1, 17.4 GFLOPs, 30 J / image batch\n FISN        83.6 %, 16.2 GFLOPs, 28 J\n HISP        83.5 %, 12.8 GFLOPs, 23 J\n MIPET       83.4 %, 10.1 GFLOPs (-42 % vs LN), 18 J (-40 %) ; 38 % heads and 45 % FFN rows pruned.\n\nWikiText-103 GPT-350 M:\n LayerNorm   ppl 29.2, 69 GFLOPs/128 tok, 26 J\n FISN        ppl 28.7, 64 GFLOPs, 24 J\n MIPET       ppl 29.0, 41 GFLOPs (-41 %), 16 J (-38 %) ; training equally stable (no extra NaNs).\n\nAcross all tasks MIPET hits the predefined energy budget (ρ=0.6) within ±3 %, retains accuracy within ±0.2 pp or +0.3 ppl of dense norm-free baselines, and needs no additional fine-tuning.",
      "expected_conclusion": "A two-timescale, biologically inspired integral controller can turn Transformers into self-optimising systems that respect both neuronal (activation) and metabolic (energy) homeostasis.  By jointly regulating activation magnitude and FLOP budget, MIPET removes LayerNorm, fuses gains into weights, and yields hardware-friendly structured sparsity without any post-training phase or hyper-parameter search.  Empirical results on language, vision, and speech show up to 40 % real energy savings at equal accuracy, demonstrating a practical path toward greener large-scale AI driven by principles of cortical plasticity."
    },
    "iterations": [
      {
        "iteration_id": 1,
        "method": "Multiscale Integral Plasticity for Energy-aware Transformers (MIPET)\n---------------------------------------------------------------------\nNotation: for block l let h_l be post-residual activations, W_l^k each weight matrix (Q,K,V,FFN-up,FFN-down), and C_l the theoretical dense FLOPs of the block.\n\nDual variables (no back-prop):\n  λ_l  – fast homeostatic drive (activation target τ≈1).\n  μ_l  – slow metabolic drive (energy target ρ, e.g. 50% of dense FLOPs).\n\nStep-wise procedure (executed every optimiser step):\n1. Streaming statistics (over 128 random tokens)\n      A_l  ←  EMA_β(|h_l|)                     # activation magnitude\n      F_l  ←  Σ_k nnz(W_l^k) / |W_l^k|         # current density per matrix\n      E_l  =  F_l · C_l                        # estimated FLOPs this block\n2. Dual loss added to task loss\n      L_dual = Σ_l [ λ_l·(A_l−τ)  +  μ_l·(E_l−ρ·C_l) ]\n3. Optimiser step on total loss.\n4. Gradient-free dual updates (β≈0.95, η_fast≈10⁻², η_slow≈10⁻³)\n      λ_l ← λ_l + η_fast · (A_l.detach()−τ)\n      μ_l ← μ_l + η_slow · (E_l.detach()−ρ·C_l)\n5. Weight fusion (as in FISN)\n      scale = exp(−λ_l)\n      for W_l^k:  W_l^k ← scale · W_l^k\n6. Structured pruning triggered by sustained positive μ̂_l = EMA_0.99(μ_l):\n      if μ̂_l > θ (θ≈0.5) then\n          a) prune entire attention heads whose total |W| < γ·median(|W|_heads)\n          b) prune FFN rows whose ℓ₂ norm < γ·median(‖row‖)\n      Masked structures are permanently zeroed and excluded from further gradient updates, instantly reducing E_l.\n7. Regrowth safety net: if μ̂_l becomes < −θ for 500 steps, randomly resurrect 10% of previously removed heads/rows to avoid capacity collapse (mirrors synaptic regrowth).  Regrown parameters are re-initialised with small Gaussian noise.\n\nGlobal hyper-parameters (τ,ρ,β,η_fast,η_slow,θ,γ) are fixed once and reused for all depths, widths, and modalities.",
        "experimental_design": {
          "experiment_summary": "This experiment evaluates the newly-proposed Multiscale Integral Plasticity for Energy-aware Transformers (MIPET) on an image-classification task.  A standard ViT-Base (86 M parameters) is trained from scratch on ImageNet-1K under four conditions: (A) conventional LayerNorm, (B) FISN (norm-free dense), (C) HISP (norm-free unstructured sparsity) and (D) our MIPET controller.  All configurations share the same training budget (300 epochs, AdamW, cosine LR decay, RandAugment, etc.).\n\nMIPET augments each Transformer block with two integral control signals: a fast homeostatic drive (λ) that keeps post-residual activation magnitude close to τ≈1, and a slow metabolic drive (μ) that keeps per-block FLOPs near a global target ρ·C (ρ=0.6).  At every optimiser step the block computes streaming activation and density statistics, adds a dual loss to the task loss, updates dual variables, fuses the λ gain directly into the weights, and—when μ stays positive—executes structured head- and row-level pruning.  A regrowth protocol reactivates 10 % of previously pruned structures if the energy budget is over-satisfied for 500 steps, preventing capacity collapse without manual tuning.\n\nWe measure task performance (top-1 accuracy), training stability, sparsity pattern, theoretical and real inference FLOPs, latency, and wall-power energy with NVIDIA Nsight on A100 GPUs.  The primary figure of merit is accuracy at equal training cost together with the percentage reduction in measured inference energy relative to the LayerNorm baseline.\n\nA small hyper-parameter sweep (learning-rate, weight-decay, pruning aggressiveness γ, and energy target ρ) is performed with random search (20 trials) on a held-out 50-class ImageNet subset.  The best setting is retrained on the full dataset for the final comparison.",
          "evaluation_metrics": [
            "Top-1 Accuracy",
            "Gradient-Norm Variance",
            "NaN Count",
            "Inference GFLOPs",
            "Inference Latency",
            "Inference Energy (Joules)",
            "Task performance under equal training budget and percentage reduction in real measured inference energy (joules) relative to LayerNorm baseline",
            "Percentage Pruned Heads",
            "Percentage Pruned FFN Rows",
            "Total Non-Zero Parameter Count",
            "GPU Training Hours",
            "Task performance under equal training budget *and* percentage reduction in real measured inference energy (joules) relative to LayerNorm baseline."
          ],
          "proposed_method": "Multiscale Integral Plasticity for Energy-aware Transformers (MIPET)\n1. Every Transformer block l maintains two scalar dual variables: λ_l (fast) and μ_l (slow).\n2. Streaming statistics are updated over 128 randomly sampled activations per forward pass:\n   • A_l ← EMA_β(|h_l|) (β=0.95)\n   • F_l ← Σ_k nnz(W_l^k)/|W_l^k| (current weight density)\n   • E_l = F_l·C_l (estimated FLOPs)\n3. A dual loss L_dual = Σ_l [λ_l(A_l−τ)+μ_l(E_l−ρC_l)] is added to the task loss and back-propagated; λ_l and μ_l themselves do NOT receive gradients.\n4. After the optimiser step, λ and μ are updated by integral control:\n      λ_l ← λ_l + η_fast(A_l−τ)   (η_fast=1e-2)\n      μ_l ← μ_l + η_slow(E_l−ρC_l) (η_slow=1e-3)\n5. All block weights are rescaled in-place by exp(−λ_l), fusing the homeostatic gain (no LayerNorm required).\n6. If the long-term average μ̂_l (EMA 0.99) exceeds θ=0.5, the block prunes:\n      a. Whole attention heads with total |W| below γ·median(|W|_heads) (γ=0.25).\n      b. FFN rows whose ℓ₂ norm is below γ·median(row norms).\n   Masks are permanent and weights are excluded from further gradient updates.\n7. If μ̂_l ≤ −θ for 500 consecutive steps, 10 % of previously pruned heads/rows are randomly re-initialised (Gaussian σ=0.02) and unlocked, providing capacity regrowth.\n8. All global hyper-parameters (τ,ρ,β,η_fast,η_slow,θ,γ) are fixed once and reused across datasets and model sizes, demonstrating robustness and eliminating per-model tuning.",
          "comparative_methods": [
            "FISN (Feature-wise linear Integral SN)"
          ],
          "models_to_use": [
            "ViT-Base (86 M)"
          ],
          "datasets_to_use": [
            "ImageNet-1K"
          ],
          "hyperparameters_to_search": {
            "learning_rate": "1e-4-5e-4",
            "weight_decay": "0.02-0.2",
            "gamma_prune": "0.15-0.35",
            "rho_energy_target": "0.5-0.7"
          }
        },
        "experiment_runs": [
          {
            "run_id": "proposed-iter1-ViT-Base-86-M-ImageNet-1K",
            "method_name": "proposed",
            "model_name": "ViT-Base (86 M)",
            "dataset_name": "ImageNet-1K"
          },
          {
            "run_id": "comparative-1-iter1-ViT-Base-86-M-ImageNet-1K",
            "method_name": "comparative-1",
            "model_name": "ViT-Base (86 M)",
            "dataset_name": "ImageNet-1K"
          }
        ]
      }
    ]
  }
}